{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a56747d",
   "metadata": {},
   "source": [
    "\n",
    "## Binary Animal Sound Classifier using EfficientAT (mn40\\_as\\_ext)\n",
    "\n",
    "This model was developed to significantly extend Project Echo’s ability to classify **animal sounds**, building a binary classifier that distinguishes between animal vocalisations and non-animal environmental noise. The model is built using the EfficientAT (mn40_as_ext) audio-specialised architecture. The earlier EfficientNetV2 system  was replaced to test a more complex architecture and see if it could achieve higher accuracy. This goal was achieved, the updated classifier reached 98.57% accuracy and significantly outperformed the earlier EfficientNetV2-based version. \n",
    "\n",
    "---\n",
    "\n",
    "### Architecture and Model Design\n",
    "The EfficientAT (mn40_as_ext) model is a compact, high-performing convolutional audio classifier pre-trained on the large-scale AudioSet dataset. It leverages knowledge distillation—a technique where a smaller model is trained to replicate the behaviour of a larger, more complex model—to retain strong accuracy while remaining computationally efficient. In our setup, we froze the pre-trained base layers to preserve general audio feature representations and added a new classification head, which was fine-tuned to distinguish between animal sounds and environmental noise.\n",
    "\n",
    "### EfficientAT vs. EfficientNetV2\n",
    "\n",
    "The original system used EfficientNetV2 with 7.1 million parameters, trained on 224×224 mel spectrograms. It achieved 90% classification accuracy across 121 species but was originally designed for image tasks, making it less suited for audio-based classification. In contrast, the updated model adopts EfficientAT (mn40_as_ext)—an audio-specialised network with 120 million parameters. Despite the increased parameter count, EfficientAT is more efficient for sound classification due to its distillation-based training and audio-centric design. We kept the input resolution and augmentation setup consistent to ensure a fair comparison. With expanded species coverage from 121 to 264, the updated model achieved 98.57% accuracy and delivered more consistent predictions across all classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Construction\n",
    "\n",
    "To build the binary classifier, the animal recordings were converted to spectograms in the `animal_mels_224` directory, paired with environmental noise samples sourced from **ESC-50** and **UrbanSound8K** datasets. All files were converted into 224×224 mel spectrograms and saved as .pt tensors for efficient loading and saved as `.pt` tensors. Crucially, all environmental samples from ESC-50 and UrbanSound8K that contained animal-like sounds (e.g., dog barks, bird chirps, insect buzzing) were manually removed to ensure clean binary separation between animal and non-animal classes\n",
    "\n",
    "---\n",
    "\n",
    "### Augmentation Strategy\n",
    "\n",
    "We applied pectrogram-level augmentations to the training in order to improve generalisation and simulate real-world recording conditions,  These included:\n",
    "\n",
    "* **Frequency masking**\n",
    "* **Time masking**\n",
    "* **Gaussian noise injection** (30% probability)\n",
    "\n",
    "These augmentations helped the model stay accurate under situations where there are noise, interference, and recording variability. They were deliberately kept minimal to align with the original model setup, while the validation data remained unaugmented to ensure both fair and consistent evaluation.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Performance and Improvements\n",
    "\n",
    "Compared to the original EfficientNetV2-based pipeline (90% accuracy), the new EfficientAT-based model is both more accurate and better suited for audio. It maintains the same input format (224×224 mel spectrograms) while reducing training complexity and improving classification performance (98.57% accuracy). This improvement is largely due to two key factors: a more specialised architecture for audio classification, and a much richer dataset that includes over 260 animal species. With more diverse sounds and a model built for sound recognition, the system delivers more accurate and consistent results\n",
    "\n",
    "### Improvements\n",
    "\n",
    "During this sprint, the EfficientAT (mn40_as_ext) model was independently developed to improve classification accuracy and generalisation. It outperformed the previous EfficientNetV2-based approach by increasing species recognition accuracy from 90% to 98.57% and extending support from 121 to 264 species. This included 224 bird species, 23 mammals, 16 amphibians, and 2 reptiles. Despite this progress, Project Echo still lacks a standardised training pipeline—currently offering only raw, unbalanced audio buckets with no reusable model or setup guide for future teams. We recommend formally upgrading to EfficientAT or another strong audio baseline, and providing a pre-trained model with preprocessing tools and evaluation benchmarks. Expanding species coverage and standardising workflows will ensure better onboarding, reproducibility, and long-term scalability of project echo.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84ac6f",
   "metadata": {},
   "source": [
    "### Section 1: Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6715910",
   "metadata": {},
   "source": [
    "\n",
    "## Download and Integrate New Animal Sound Folders from Google Drive\n",
    "\n",
    "This Python script automates the process of:\n",
    "\n",
    "1. **Downloading a public folder** containing new animal sound recordings from a shared Google Drive link using the `gdown` library.\n",
    "2. **Moving the downloaded folders** (each representing a species) into the `audio_root` directory used by the Echo Engine model.\n",
    "3. **Avoiding duplicates** by skipping folders that already exist.\n",
    "4. **Cleaning up** the temporary download directory after the files are transferred.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b96c71",
   "metadata": {},
   "source": [
    "You can find the updated sound recordings at the following link:  \n",
    "[https://drive.google.com/drive/folders/1VHutT83YhaUzPw6wKI_hjFeF1GRUb8hO?usp=sharing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1052722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\users\\riley\\anaconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\riley\\anaconda3\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\riley\\anaconda3\\lib\\site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\riley\\anaconda3\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\riley\\anaconda3\\lib\\site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\riley\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\riley\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\riley\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\riley\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\riley\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\riley\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\riley\\anaconda3\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b10819a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgdown\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gdown'"
     ]
    }
   ],
   "source": [
    "\n",
    "import gdown\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "#Google Drive folder URL\n",
    "url = \"https://drive.google.com/drive/folders/1MP1j_oiMGL6hWWMrPcuJYsKLSUH8gjp_\"\n",
    "download_dir = \"downloaded_sounds\"\n",
    "\n",
    "# Download the folder from Google Drive\n",
    "gdown.download_folder(\n",
    "    url=url,\n",
    "    output=download_dir,\n",
    "    quiet=False,\n",
    "    use_cookies=False\n",
    ")\n",
    "\n",
    "# Define target audio directory\n",
    "audio_root = r\"C:\\Users\\riley\\Documents\\Project-Echo\\src\\Prototypes\\data\\data_files\"\n",
    "\n",
    "# Move downloaded folders into the audio_root directory\n",
    "for folder_name in os.listdir(download_dir):\n",
    "    src = os.path.join(download_dir, folder_name)\n",
    "    dst = os.path.join(audio_root, folder_name)\n",
    "\n",
    "    if os.path.isdir(src):\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "            print(f\"Moved: {folder_name}\")\n",
    "        else:\n",
    "            print(f\"Skipped (already exists): {folder_name}\")\n",
    "\n",
    "# Clean up temporary download directory\n",
    "if os.path.exists(download_dir):\n",
    "    shutil.rmtree(download_dir)\n",
    "    print(\" Cleaned up temporary folder:\", download_dir)\n",
    "\n",
    "print(\"New species folders have been integrated into:\", audio_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Alternative: Script to Import New Animal Sound Folders from Google Drive\n",
    "\n",
    "This script downloads a shared Google Drive folder using `gdown` and moves the species subfolders into: \n",
    "Note: please replace with your name \n",
    "\n",
    "```\n",
    "C:\\Users\\riley\\Documents\\Project-Echo\\src\\Prototypes\\data\\data_files\n",
    "```\n",
    "**Note: please replace with your name**\n",
    "- Skips folders that already exist  \n",
    "- Cleans up temporary files after moving  \n",
    "- Requires the Drive folder to be set to “Anyone with the link”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e58f1",
   "metadata": {},
   "source": [
    "This is a complete script that converts .wav or .mp3 files into 224×224 mel spectrogram tensors and saves them as .pt files. It works with both formats as long as you have FFmpeg installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "\n",
    "# Settings\n",
    "input_dir = r\"C:\\Users\\riley\\Documents\\Project-Echo\\src\\Prototypes\\data\\data_files\"\n",
    "output_dir = r\"C:\\Users\\riley\\Documents\\Project-Echo\\src\\Prototypes\\data\\mel_224\"\n",
    "sample_rate = 16000\n",
    "\n",
    "# Mel spectrogram transformer\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=1024,\n",
    "    hop_length=320,\n",
    "    n_mels=224\n",
    ")\n",
    "resize = T.Resize((224, 224))  \n",
    "\n",
    "# Process all audio files\n",
    "for class_folder in Path(input_dir).iterdir():\n",
    "    if class_folder.is_dir():\n",
    "        output_class_dir = Path(output_dir) / class_folder.name\n",
    "        output_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for audio_file in class_folder.glob(\"*\"):\n",
    "            if not audio_file.suffix.lower() in [\".wav\", \".mp3\"]:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Load and resample\n",
    "                waveform, sr = torchaudio.load(audio_file)\n",
    "                if sr != sample_rate:\n",
    "                    resampler = T.Resample(sr, sample_rate)\n",
    "                    waveform = resampler(waveform)\n",
    "\n",
    "                # Convert to mono\n",
    "                if waveform.shape[0] > 1:\n",
    "                    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "                # Generate and resize mel spectrogram\n",
    "                mel_spec = mel_transform(waveform)\n",
    "                mel_spec = torch.log1p(mel_spec)  # Log-scale\n",
    "                mel_spec = resize(mel_spec)\n",
    "\n",
    "                # Save .pt tensor\n",
    "                out_path = output_class_dir / f\"{audio_file.stem}.pt\"\n",
    "                torch.save(mel_spec, out_path)\n",
    "\n",
    "                print(f\"Saved: {out_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed: {audio_file} — {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd630c5",
   "metadata": {},
   "source": [
    "official download links for the **ESC-50** and **UrbanSound8K** datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e54b22",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  ESC-50: Environmental Sound Classification\n",
    "\n",
    "* **Dataset Website:** [https://github.com/karoldvl/ESC-50](https://github.com/karoldvl/ESC-50)\n",
    "* **Direct Download (ZIP):**\n",
    "[https://github.com/karoldvl/ESC-50/archive/master.zip](https://github.com/karoldvl/ESC-50/archive/master.zip)\n",
    "* **Audio files only:**\n",
    "[https://github.com/karoldvl/ESC-50/blob/master/audio/](https://github.com/karoldvl/ESC-50/blob/master/audio/)\n",
    "  (You’ll need to download from GitHub or clone the repo)\n",
    "\n",
    "---\n",
    "\n",
    "### UrbanSound8K\n",
    "\n",
    "* **Dataset Website (with registration):**\n",
    "[https://urbansounddataset.weebly.com/urbansound8k.html](https://urbansounddataset.weebly.com/urbansound8k.html)\n",
    "* **Direct Download (ZIP, 6.2 GB):**\n",
    "[https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz](https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz)\n",
    "  (You may need a Zenodo account for large downloads)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9989eb6",
   "metadata": {},
   "source": [
    "Here’s a Python script that removes audio files from ESC-50 and UrbanSound8K if their class labels indicate animal-like sounds (e.g., dog, bird, insect). It assumes you're working with the original directory structure and accompanying metadata (esc50.csv for ESC-50 and UrbanSound8K.csv for UrbanSound8K):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# unwanted animal-related categories\n",
    "animal_classes_esc50 = {'dog', 'rooster', 'pig', 'cow', 'frog', 'cat', 'hen', 'insects', 'sheep'}\n",
    "animal_classes_us8k = {'dog_bark'}\n",
    "\n",
    "# ESC-50 \n",
    "def clean_esc50(audio_dir, csv_path):\n",
    "    print(\"Cleaning ESC-50...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    animal_files = df[df['category'].isin(animal_classes_esc50)]['filename'].tolist()\n",
    "\n",
    "    for fname in animal_files:\n",
    "        file_path = os.path.join(audio_dir, fname)\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed ESC-50 animal sound: {fname}\")\n",
    "\n",
    "# UrbanSound8K \n",
    "def clean_us8k(audio_root_dir, csv_path):\n",
    "    print(\"Cleaning UrbanSound8K\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for _, row in df.iterrows():\n",
    "        if row['class'] in animal_classes_us8k:\n",
    "            fold = f\"fold{row['fold']}\"\n",
    "            file_path = os.path.join(audio_root_dir, fold, row['slice_file_name'])\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed UrbanSound8K animal sound: {file_path}\")\n",
    "\n",
    "# (update these to your machine's actual locations) \n",
    "esc_audio_dir = r\"C:\\path\\to\\ESC-50-master\\audio\"\n",
    "esc_csv_path = r\"C:\\path\\to\\ESC-50-master\\meta\\esc50.csv\"\n",
    "\n",
    "us8k_audio_root = r\"C:\\path\\to\\UrbanSound8K\\audio\"\n",
    "us8k_csv_path = r\"C:\\path\\to\\UrbanSound8K\\metadata\\UrbanSound8K.csv\"\n",
    "\n",
    "#Run cleaning \n",
    "clean_esc50(esc_audio_dir, esc_csv_path)\n",
    "clean_us8k(us8k_audio_root, us8k_csv_path)\n",
    "\n",
    "print(\"Cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d9e62",
   "metadata": {},
   "source": [
    "EfficientAT download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fschmid56/EfficientAT (download the model from here)\n",
    "\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\riley\\Documents\\Project-Echo\\src\\Prototypes\\EfficientAT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba32f06",
   "metadata": {},
   "source": [
    "### Section 2: Model Code Implementation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4591d45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# Binary Classification\n",
    "class BinaryAnimalDataset(Dataset):\n",
    "    def __init__(self, animal_dir, esc_dir, augment=False):\n",
    "        self.samples = []\n",
    "        self.augment = augment\n",
    "\n",
    "      \n",
    "        for path in Path(animal_dir).glob(\"*.pt\"):\n",
    "            self.samples.append((path, 1))\n",
    "\n",
    "     \n",
    "        for path in Path(esc_dir).glob(\"*.pt\"):\n",
    "            self.samples.append((path, 0))\n",
    "\n",
    "        # augmentations\n",
    "        if self.augment:\n",
    "            self.freq_mask = T.FrequencyMasking(freq_mask_param=12)\n",
    "            self.time_mask = T.TimeMasking(time_mask_param=20)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        mel = torch.load(path)\n",
    "\n",
    "        ## Ensure spectrogram has the shape [1, 224, 224]\n",
    "        if mel.dim() == 2:\n",
    "            mel = mel.unsqueeze(0)\n",
    "        elif mel.shape[0] != 1:\n",
    "            mel = mel.mean(dim=0, keepdim=True)\n",
    "\n",
    "        if mel.shape != (1, 224, 224):\n",
    "            raise ValueError(f\"Bad shape {mel.shape} in file: {path}\")\n",
    "\n",
    "        #Apply augmentation only if enabled\n",
    "        if self.augment:\n",
    "            mel = self.freq_mask(mel)\n",
    "            mel = self.time_mask(mel)\n",
    "            if torch.rand(1).item() < 0.3:  # 30% chance of adding Gaussian noise\n",
    "                mel += 0.005 * torch.randn_like(mel)\n",
    "\n",
    "        return mel, label\n",
    "\n",
    "# Load EfficientAT (mn40_as_ext) Model with Frozen Base\n",
    "def load_mn40_as_ext_model(device):\n",
    "    project_root = r\"C:\\Users\\riley\\Documents\\Project-Echo\\src\\Prototypes\"\n",
    "    efficientat_root = os.path.join(project_root, \"EfficientAT\")\n",
    "    if efficientat_root not in sys.path:\n",
    "        sys.path.append(efficientat_root)\n",
    "\n",
    "    from models.mn.model import get_model as get_mn\n",
    "    model = get_mn(pretrained_name=\"mn40_as_ext\", width_mult=4.0)\n",
    "    model.to(device)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = \"classifier\" in name\n",
    "\n",
    "    return model\n",
    "\n",
    "#Training and Evaluation Pipeline\n",
    "def train_binary_animal_model():\n",
    "    \n",
    "   # Paths to mel spectrogram \n",
    "    animal_dir = r\"C:\\Users\\riley\\Documents\\Deakin 2025\\SIT378\\Sprint 1\\animal_mels_224\"\n",
    "    esc_dir = r\"C:\\Users\\riley\\Documents\\Deakin 2025\\SIT378\\Sprint 1\\other_sounds\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "     # Load and split dataset (80% train, 20% val)\n",
    "    full_dataset = BinaryAnimalDataset(animal_dir, esc_dir)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "      # Enable augmentation for training only\n",
    "    train_dataset.dataset.augment = True\n",
    "    val_dataset.dataset.augment = False\n",
    "\n",
    "     # Prepare data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "      # Load model and replace classifier\n",
    "    model = load_mn40_as_ext_model(device)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(model.classifier[2].in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 2)\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
    "\n",
    "    # store results \n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "\n",
    "     # Training loop\n",
    "    for epoch in range(1, 6):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "         # Computing the training metrics\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "         # Validation\n",
    "        model.eval()\n",
    "        val_loss_total, correct, total = 0.0, 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss_total += loss.item() * x.size(0)\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "         # the validation metrics\n",
    "        val_loss = val_loss_total / total\n",
    "        val_acc = 100 * correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "          # Displays the metrics\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.2f}%\")\n",
    "        print(f\"Epoch {epoch}: Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.2f}%\")\n",
    "\n",
    "         # Final evaluation\n",
    "    print(f\"\\nFinal Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "    print(f\"Final Val Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    print(\"\\nFinal Classification Report\")\n",
    "    print(classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        target_names=[\"non-animal\", \"animal\"],\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"\\nConfusion Matrix\")\n",
    "    print(confusion_matrix(all_labels, all_preds, labels=[0, 1]))\n",
    "\n",
    "     # Plot accuracy\n",
    "    epochs = list(range(1, 6))\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Acc\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run Training\n",
    "train_binary_animal_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Echo)",
   "language": "python",
   "name": "echo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
