{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f46b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbb9d6d2-d47e-4f43-8f5c-104203381c67",
   "metadata": {},
   "source": [
    "## üö® IMPORTANT üö®\n",
    "\n",
    "### 1Ô∏è‚É£ Save a copy of this file with **your initials** at the end:\n",
    "   - **Delete** the current ending of the filename.\n",
    "   - **Replace** it with your initials.\n",
    "\n",
    "### 2Ô∏è‚É£ Once your branch is merged:\n",
    "   - Your personalized file will be added to the folder.\n",
    "\n",
    "\n",
    "\n",
    "itate to ask for help. Let's make this a smooth start! üòä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973de20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\" style=\"font-size: 2em;\">\n",
    "    <strong>Onboarding Task for Project Echo:</strong>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb3824a",
   "metadata": {},
   "source": [
    "\n",
    "      \n",
    "      \n",
    "### **Introduction**\n",
    "\n",
    "Welcome to the onboarding task for **Project Echo**! The onboarding task provides essential hands-on experience with the project's tools, datasets, and workflows, and ensures that you are prepared to contribute effectively to Project Echo.\n",
    "\n",
    "### **Project Echo Overview**\n",
    "\n",
    "Project Echo utlilizes **AI/ML** to classify and assess the density of noise-producing animals in rainforests. This tool helps **conservationists** track endangered species and supports ecosystem health. By analyzing animal sounds, the project aims to monitor populations, identify threatened species, and mitigate the impact of predatory and destructive fauna.\n",
    "\n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5407291-aa6b-40ba-bd37-c05a910657a2",
   "metadata": {},
   "source": [
    "## Project Echo File Hierarcy\n",
    "\n",
    "Take the time to explore the structure of the files thoroughly, paying close attention to where everything is located. This understanding will be invaluable for navigating the project efficiently\n",
    "\n",
    "<img src=\"Files.png\" alt=\"This is an image\" title=\"My Image\" width=\"800\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd91fd-f925-4653-8c22-9c90b393f9e8",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "### **Getting Started**\n",
    "\n",
    "This task will familiarize you with Project Echo's workflow and tools, organized as follows:\n",
    "\n",
    "1. **Set Up Your Development Environment**: Follow the setup instructions for **Anaconda**, **Jupyter**, **TensorFlow** and other required packages.\n",
    "2. **Explore the Dataset**: Download and explore the **Project Otways Dataset**.\n",
    "3. **Data Preprocessing Task**: Work through example tasks to familiarize yourself with the dataset and project.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad2ed6",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"font-size: 1.5em;\">\n",
    "    <strong>Part 1: Set Up Your Development Environment:</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830942bd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Install Anaconda</h1>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63231e23-139f-45fc-afd4-1700dfe59728",
   "metadata": {},
   "source": [
    "\n",
    "      \n",
    "Anaconda is a powerful tool for managing environments and dependencies, making it easier to maintain the required configurations for Project Echo.\n",
    "\n",
    "### General Instructions:\n",
    "\n",
    "1. **Visit the Anaconda Download Page**.\n",
    "\n",
    "2. **Download the appropriate installer** for your operating system:\n",
    "   - **macOS**: Choose either the Intel version or the Apple Silicon version based on your hardware.\n",
    "   - **Windows/Linux**: Select the 64-bit installer.\n",
    "\n",
    "3. **Follow the installation instructions**:\n",
    "   - **macOS**: Double-click the `.pkg` file and complete the installation.\n",
    "   - **Windows**: Run the `.exe` file and complete the installation.\n",
    "   - **Linux**: Open a terminal and run the following command:\n",
    "     ```bash\n",
    "     ~/Downloads/Anaconda3-<version>-Linux-x86_64.sh\n",
    "     ```\n",
    "     Replace `<version>` with the downloaded version. Follow the prompts to complete the initialization of Anaconda.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2530b50-115e-4727-a732-7134a05c182a",
   "metadata": {},
   "source": [
    "Verify Anaconda installation by running the following command in the CLI:\n",
    "``` shell\n",
    "conda --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8e53d-0e24-42e0-8bb4-f2c2af0a08d3",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Create a Virtual Environment</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba884386-1821-4a19-bbf4-adce022b6fea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Virtual environments isolate project-specific dependencies, ensuring compatibility and preventing conflicts with other projects.\n",
    "\n",
    "1. Open a terminal (macOS/Linux) or Anaconda Prompt (Windows).\n",
    "2. Create a virtual environment for Project Echo with Python 3.9:\n",
    "``` shell\n",
    "    conda create --name projectecho python=3.9\n",
    "```\n",
    "3. Activate the environment:\n",
    "```shell\n",
    "    conda activate projectecho\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ddf3e-6d0c-44d3-9c9b-b77ff26c78e2",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Install Jupyter</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd6477-6bcf-4d35-93ca-167e99ae60ec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You will use IPython Notebook (aka Jupyter Notebook) for this project. \n",
    "\n",
    "To install Jupyter Notebook, you run the following command after activating your environment:\n",
    "```shell\n",
    "pip install notebook\n",
    "```\n",
    "To start Jupyter, change to the directory that contains your notebook.\n",
    "```shell\n",
    "cd <directory>\n",
    "```\n",
    "and run the following command:\n",
    "```shell\n",
    "jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbcf360-7fcb-4aa0-86c8-e77b588cf3c6",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Create a Kernel (Optional)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5228a-317f-492a-b91c-a5d545336b55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "For this project, we need to use the environment that we have created as the chosen kernel in Jupyter Notebook. If the environment does not show up as a kernel in the notebook, please run the following commands.\n",
    "\n",
    "* Install the IPython kernel:\n",
    "```\n",
    "pip install ipykernel\n",
    "```\n",
    "\n",
    "* Then, register the environment as a kernel:\n",
    "```\n",
    "python -m ipykernel install --user --name=projectecho --display-name \"Python (projectecho)\"\n",
    "```\n",
    "\n",
    "This will allow you to select your environment directly in Jupyter Notebook.\n",
    "\n",
    "![My Image](image2.png \"This is an image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00091b24",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Install Google Cloud CLI</h1>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba033c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The Google Cloud CLI is used to interact with the Google Cloud Platform (GCP) and manage resources like Google Cloud Storage (GCS) buckets. For the Project Otways Dataset, we will use it to facilitate seamless interaction with the dataset and manage it's resources efficiently. The specific use cases and details of how the CLI will be used for Project Otways will be elaborated further below.\n",
    "      \n",
    "\n",
    "- Securely logs in users and generates credentials for accessing GCS, ensuring authorized access to the dataset.\n",
    "-  Enables users to create, list, upload, and download files from GCS buckets, allowing for efficient data handling.\n",
    "\n",
    "By using the CLI, you can easily authenticate and access the datasets stored in Google Cloud, which is essential for managing and processing the large audio files in the Project Otways Dataset.\n",
    "\n",
    "\n",
    "Follow the steps below to download and install the Google Cloud CLI for Windows and macOS. Refer to the [installation guide](https://cloud.google.com/sdk/docs/install) for additional details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86277a9c-7ecf-498c-8ca4-976abf0d02d2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <strong>For Windows:</strong>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9431cb-9bd2-4091-a859-f39759f8d477",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Download the Installer**:\n",
    "   - [Download the installer here](https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe).\n",
    "   - Or use PowerShell:\n",
    "     ```powershell\n",
    "     (New-Object Net.WebClient).DownloadFile(\"https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe\", \"GoogleCloudSDKInstaller.exe\")\n",
    "     ```\n",
    "\n",
    "- **Run the Installer**: - Double-click the installer and follow the prompts.\n",
    "\n",
    " \n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d6db4-2dee-4a30-99ab-bcbfa610479c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <strong>For macOS:</strong>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9c2f5-9b56-45cb-809c-459a521d4453",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " **Download the Package**:\n",
    "   - For **64-bit (x86_64)**:  \n",
    "     [Download here](https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-x86_64.tar.gz)\n",
    "   - For **ARM64 (M1/M2)**:  \n",
    "     [Download here](https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-arm.tar.gz)\n",
    "\n",
    "\n",
    "\n",
    "- **Run the Installer**: - Double-click the installer and follow the prompts.\n",
    "\n",
    "\n",
    "   ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc924d-50da-409e-91a7-e801fa49c0e0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>For Linux:</strong>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afdc42f-db33-4684-9ef3-31c40576379c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Download the Installer**:\n",
    "   - [Download the installer for Linux (64-bit)](https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-linux-x86_64.tar.gz)\n",
    "   - Or use the terminal:\n",
    "     ```bash\n",
    "     curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-linux-x86_64.tar.gz\n",
    "     ```\n",
    "\n",
    "\n",
    "\n",
    "- **Run the Installer**: - Double-click the installer and follow the prompts.\n",
    "\n",
    "   ---------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122bdd32",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Authenticate with Google Cloud</h1>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30433638",
   "metadata": {},
   "source": [
    "\n",
    "Google Cloud requires authentication to verify your identity and ensure only users with proper permissions are able to access its resources. Temporary credentials are generated and linked to your **Google Cloud Project**  Authentication grants you permission to access project-specific buckets \n",
    "\n",
    "---\n",
    "\n",
    "### **Run the Authentication Command**\n",
    "\n",
    "1. Open your terminal and execute the following command:\n",
    "   ```\n",
    "   gcloud auth application-default login\n",
    "   ```\n",
    "\n",
    "### **Browser Login**\n",
    "\n",
    "This command will open a browser window and redirect you to `accounts.google.com`.\n",
    "Use your institutional credentials to log in.  Example: For **Deakin users**, log in with your **Deakin Outlook account**.\n",
    "\n",
    "### **Deakin Authentication**\n",
    "\n",
    "After entering your Deakin account details, you will be redirected to the `signon.deakin.edu` page.\n",
    "Complete the **Two-Factor Authentication (2FA)** process as instructed.\n",
    "\n",
    "### **Confirmation**\n",
    "\n",
    "Once authentication is successful, return to the terminal. You should see a confirmation message indicating that authentication has been completed.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b3735-82bd-45a2-bab2-4a106e8c74a7",
   "metadata": {},
   "source": [
    "\n",
    "      \n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Install Required Packages</h1>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Windows/Linux\n",
    "To enable GPU support for TensorFlow on Windows, it is essential to install the correct versions of the CUDA Toolkit and cuDNN, which are libraries that facilitate efficient computations on NVIDIA GPUs. The CUDA Toolkit provides the necessary tools for TensorFlow to interact with the GPU, while cuDNN optimizes deep learning operations such as convolutions. For TensorFlow version 2.10.0, the compatible versions are CUDA 11.2 and cuDNN 8.1. Installing these libraries using Conda ensures they are configured correctly within your environment, avoiding manual setup complexities and ensuring compatibility. Before running the following command, make sure your Conda environment is activated.\n",
    "\n",
    "```shell\n",
    "conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n",
    "```\n",
    "\n",
    "Install the required packages by running the following commands:\n",
    "```shell\n",
    "pip install notebook\n",
    "pip install jupyterlab\n",
    "pip install tensorflow==2.10.0 tensorflow-hub==0.12.0\n",
    "pip install \"numpy<2\"\n",
    "pip install librosa\n",
    "pip install audiomentations\n",
    "```\n",
    "\n",
    "### MacOS (Silicon Chip)\n",
    "1. Install the required packages by running the following commands:\n",
    "```shell\n",
    "pip install tensorflow-macos==2.10.0 tensorflow-hub==0.12.0\n",
    "```\n",
    "2. Install the Metal plugin for GPU acceleration:\n",
    "```shell\n",
    "pip install tensorflow-metal=0.6.0\n",
    "```\n",
    "3. Install the rest of the necessary packages:\n",
    "```shell\n",
    "pip install librosa audiomentations\n",
    "pip install \"numpy<2\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0406f9",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Protobuf Installation</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b1db9",
   "metadata": {},
   "source": [
    "Protobuf is a key dependency that will be needed Google Cloud Storage and other Google Cloud services but has specific version constraints. Versions below `3.20`  lack the `builder.py` file, causing errors like:\n",
    "\n",
    "```\n",
    "ImportError: cannot import name 'builder' from 'google.protobuf.internal'\n",
    "```\n",
    "\n",
    "\n",
    "To resolve this issue, follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc3acaf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Step 1: Install the Latest Protobuf Version**\n",
    "We need to temporally upgrade protobuf to the latest version to access the required `builder.py` file:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade protobuf\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Locate and Backup `builder.py`**\n",
    "Find the `builder.py` file in your Python environment, typically located at:  \n",
    "```\n",
    "<your_python_directory>/Lib/site-packages/google/protobuf/internal/builder.py\n",
    "```\n",
    "\n",
    "1. Navigate to this directory.\n",
    "2. Copy the `builder.py` file to a safe location, such as your `Documents` folder.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Install the Required Protobuf Version**\n",
    "Install the protobuf version required by your project (e.g., `3.19.4`):\n",
    "\n",
    "```bash\n",
    "pip install protobuf==3.19.4\n",
    "```\n",
    "\n",
    "--------------\n",
    "\n",
    "#### **Step 4: Restore `builder.py`**\n",
    "\n",
    "Copy the previously saved `builder.py` file \n",
    "\n",
    "```\n",
    "<your_python_directory>/Lib/site-packages/google/protobuf/internal/builder.py\n",
    "```\n",
    "\n",
    "This helps ensure that the required ``builder.py`` file is available for older Protobuf versions to avoid import errors.\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd796e23",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Google Cloud Storage SDK</h1>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25794e6",
   "metadata": {},
   "source": [
    "\n",
    "The **Google Cloud Storage SDK** is a powerful Python library that enables efficient interaction with Google Cloud Storage (GCS), It provides several key features:\n",
    "\n",
    "The Google Cloud Storage SDK offers several key benefits:\n",
    "\n",
    "- It Enables downloading, uploading, and managing files in GCS buckets \n",
    "- Automates of tasks such as listing files, deleting objects, and retrieving metadata\n",
    "- Easily integrates with Python-based data pipelines\n",
    "- Optimized for processing large datasets\n",
    "\n",
    "This SDK is particularly useful in managing and processing the Project Otways Dataset that we will use, making the process of working with large audio files efficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6131d4",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "\n",
    "### **Install Google Cloud Storage SDK**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86ab24",
   "metadata": {},
   "source": [
    "To install `google-cloud-storage` without altering the existing dependencies (like Protobuf), you can use the `--no-deps` option with `pip`. This ensures that only the `google-cloud-storage` package itself is installed, without modifying other dependencies in your environment.\n",
    "ons manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195cd849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install google-cloud-storage --no-deps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f638da-082b-4250-9b77-7211fdf3cdb7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\" style=\"font-size: 1.5em;\">\n",
    "    <strong>Fixing Dependency Conflicts in TensorFlow and Related Packages:</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b0573",
   "metadata": {},
   "source": [
    "You will likely encounter errors related to incompatible versions of TensorFlow, Protobuf, Keras, and TensorBoard. \n",
    "\n",
    "```\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "tensorflow-intel 2.18.0 requires keras>=3.5.0, but you have keras 2.10.0 which is incompatible.\n",
    "tensorflow-intel 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.19.4 which is incompatible.\n",
    "tensorflow-intel 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.10.1 which is incompatible.\n",
    "\n",
    "```\n",
    "\n",
    "To resolve, follow these steps to align the versions properly:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Check the Current Package Versions**\n",
    "Run the following command to display the current versions of the key packages:\n",
    "\n",
    "```bash\n",
    "pip show tensorflow keras tensorboard protobuf\n",
    "```\n",
    "\n",
    "\n",
    "#### **Step 2: Uninstall the Incompatible Packages**\n",
    "First, uninstall the existing versions of the conflicting packages:\n",
    "\n",
    "```bash\n",
    "pip uninstall -y tensorflow keras tensorboard protobuf\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Install the Compatible Versions**\n",
    "Install the specific versions of the required packages to resolve conflicts:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow==2.10.0\n",
    "pip install keras==2.10.0\n",
    "pip install tensorboard==2.10.1\n",
    "pip install protobuf==3.19.4\n",
    "pip install streamlit==1.13.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9261e7d-5394-48e8-9694-cd4801fc8444",
   "metadata": {},
   "source": [
    "## Verify Package Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a61a7da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import librosa\n",
    "import audiomentations\n",
    "import google.protobuf as protobuf \n",
    "\n",
    "# Print the versions of each package\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"TensorFlow Hub Version:\", hub.__version__)\n",
    "print(\"Librosa Version:\", librosa.__version__)\n",
    "print(\"Audiomentations Version:\", audiomentations.__version__)\n",
    "print(\"Protobuf Version:\", protobuf.__version__)\n",
    "\n",
    "# Check GPU Configuration\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ea2d6",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"font-size: 1.5em;\">\n",
    "    <strong>Part 2: Explore The Dataset:</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd345c09-5167-406b-9453-73a2a67e3e8d",
   "metadata": {},
   "source": [
    "----------------\n",
    " \n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Project Otways Dataset</h1>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2d632-707c-4307-880b-6e6267cf6b0c",
   "metadata": {},
   "source": [
    "\n",
    "The **Project Otways Dataset** contains audio data from 118 animal species found in the Otways region. This is crucial for bioacoustic research, machine learning model training, and species identification based on sound. The data is Hosted on Google Cloud Storage and can be accessed and processed through the Google Cloud platform.\n",
    "\n",
    "The **Project Otways Dataset** is valuable for **Project Echo**. It provides a large collection of audio recordings for training machine learning models to classify animal sounds. By using this data, **Project Echo** can support non-invasive monitoring of animal populations and track biodiversity in ecosystems like rainforests. The dataset enables efficient and scalable processing, making it a useful resource for conservation efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53d455-f917-4474-b3fe-10eb41bf9993",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: rgba(0, 0, 0, 0.7); color: #fff; padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2); width: fit-content;\">\n",
    "  <h1 style=\"font-size: 24px; margin: 0;\">Dataset Overview</h1>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3dd12-762f-42e3-b085-cc4ede7a42ee",
   "metadata": {},
   "source": [
    "\n",
    "The dataset consists of three versions stored in separate GCS buckets, each containing varying amounts of data for the same 118 species.\n",
    "\n",
    "| Bucket Name              | Description                                                                                 | Total Files | Total Size |\n",
    "|--------------------------|---------------------------------------------------------------------------------------------|-------------|------------|\n",
    "| **`project_echo_bucket_1`** | Contains 3 audio files per species (118 species).                                         | 353         | 73 MB      |\n",
    "| **`project_echo_bucket_2`** | Training data with significant overlap (88%) with `project_echo_bucket_3`.                | 7,161       | 168 MB     |\n",
    "| **`project_echo_bucket_3`** | Unique training clips for 118 species with no overlap.                                     | 7,536       | 349 MB     |\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7890a99",
   "metadata": {},
   "source": [
    "**The tqdm package is used to add progress bars to loops in your code. It helps provide a visual representation of the progress of long-running operations like file downloads. This will be helpful when downloading the buckets**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d7c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9102ac13-ce91-4371-b70a-e13dee0833eb",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "#### Download code for `project_echo_bucket_1`, `project_echo_bucket_2,`  `project_echo_bucket_3` \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca008cb3-eb26-4c52-9ba5-b58d40096de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# project and base directory\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"sit-23t1-project-echo-25288b9\"\n",
    "\n",
    "#enter your desired directory \n",
    "base_dir = r\"C:\\Users\\deanf\\OneDrive\\CTMC\\MAppAI\\SIT764 Team Project A\\Project Echo\"\n",
    "\n",
    "# buckets \n",
    "buckets = [\n",
    "    'project_echo_bucket_1',\n",
    "    'project_echo_bucket_2',\n",
    "    'project_echo_bucket_3'#,\n",
    "    #'project_echo_birdclef'\n",
    "]\n",
    "\n",
    "# Initializes storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "\n",
    "#Downloads the files from a given GCS bucket into specified directory.\n",
    "def processing_bucket(bucket_name, storage_client, base_dir):\n",
    "    \n",
    "    print(f\"\\nProcessing bucket: {bucket_name}\")\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs())  # Get list of files\n",
    "    \n",
    "    with tqdm(total=len(blobs), desc=f\"Downloading {bucket_name}\") as pbar:\n",
    "        for blob in blobs:\n",
    "            # Construct the paths\n",
    "            folder_name = blob.name.split('/')[0]\n",
    "            file_name = '/'.join(blob.name.split('/')[1:])  # nested folders\n",
    "            local_dir = os.path.join(base_dir, folder_name)\n",
    "            local_path = os.path.join(local_dir, file_name)\n",
    "            \n",
    "            #  if they don't exist create directories\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            \n",
    "            # Download the file\n",
    "            blob.download_to_filename(local_path)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Main\n",
    "for bucket_name in buckets:\n",
    "    processing_bucket(bucket_name, storage_client, base_dir)\n",
    "\n",
    "print(\"All files have been downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d09492",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"font-size: 1.5em;\">\n",
    "    <strong>Part 3: Data Preprocessing Task üîß: Identify Outlier Audio Durations</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f993835-72cd-46fb-a551-01422f695590",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this task, you will:\n",
    "- Use the `durations` list (pre-calculated) to find audio files with unusual durations.\n",
    "- Specifically, identify files shorter than 1 second or longer than 10 seconds.\n",
    "- Print the file names of these outliers.\n",
    "\n",
    "üí° **Why this is important**:\n",
    "- Outlier durations can indicate noisy or problematic data.\n",
    "- Cleaning such files ensures better model performance and reduces errors.\n",
    "\n",
    "Once you've identified the outliers, the notebook will validate your results. Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f8a73-1e30-47e5-9232-69e7d37e34be",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "### Import Required Libraries\n",
    "We start by importing the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a25e45f-eae5-4d12-800a-17eb85d2674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import random  \n",
    "import librosa  \n",
    "import librosa.display  \n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np  \n",
    "from PIL import Image  \n",
    "from IPython.display import display  \n",
    "import pandas as pd  \n",
    "from sklearn.model_selection import train_test_split  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce58aff-79d0-4f54-b3b9-9443c690efe7",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## üìÇ Insert Your Dataset\n",
    "\n",
    "Before starting the tasks, ensure that the dataset is included in the correct location. Follow these steps:\n",
    "\n",
    "1. **Dataset Location**:\n",
    "   - The dataset required for this notebook can be found in the **repository**.\n",
    "   - Explore the repository to locate the dataset and understand its structure.\n",
    "\n",
    "2. **Verify the Path**:\n",
    "   - Ensure that the `dataset_path` variable in the first code cell points to the folder containing your dataset:\n",
    "     ```python\n",
    "     dataset_path = r'path_to_your_dataset'\n",
    "     ```\n",
    "\n",
    "3. **Start the Notebook**:\n",
    "   - Once the dataset is in place, you can begin running the notebook and completing the tasks.\n",
    "\n",
    "üí° **Note**: Familiarize yourself with the dataset structure by exploring the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3ed30-b538-47ba-ad33-e1589bad4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset path\n",
    "dataset_path = r'C:\\Users\\deanf\\OneDrive\\CTMC\\MAppAI\\SIT764 Team Project A\\Project Echo'#please paste your directory for your dataset here\n",
    "\n",
    "# Define the output directory for spectrograms\n",
    "output_dir = r'C:\\Users\\deanf\\OneDrive\\CTMC\\MAppAI\\SIT764 Team Project A\\Project Echo\\Output_spectrograms'#please paste your directory here\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39313922-13f6-4f15-bf96-aba4c47788f9",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "### Gather Audio Files\n",
    "1. We list all subdirectories (each representing an animal sound category).\n",
    "2. Traverse each folder and collect the `.wav` `mp3` files into a single list.\n",
    "3. Shuffle the list to ensure randomness in file processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c572dc8c-4d93-41b2-b8a9-0a8597e1b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all subdirectories (animal sound categories)\n",
    "animal_folders = [os.path.join(dataset_path, folder) for folder in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, folder))]\n",
    "\n",
    "# Initialize an empty list to store file paths\n",
    "all_audio_files = []\n",
    "\n",
    "# Traverse through each folder to gather all audio file paths\n",
    "for folder in animal_folders:\n",
    "    audio_files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.wav') or file.endswith('.mp3')]\n",
    "    all_audio_files.extend(audio_files)\n",
    "\n",
    "# Shuffle the entire list of audio files to enhance randomness\n",
    "random.shuffle(all_audio_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f49ec-8e87-46cb-a916-8be67b4bd2a3",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "### Select Random Audio Files\n",
    "We randomly sample 50 audio files from the shuffled list. If there are fewer than 50 files, all files are selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91a1f22f-11fd-4f2f-bb20-61f4bff437e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Randomly sample 50 audio files from the shuffled list\n",
    "random_samples = random.sample(all_audio_files, min(50, len(all_audio_files)))\n",
    "\n",
    "print(f\"Number of audio files found: {len(all_audio_files)}\")\n",
    "print(f\"Randomly selected {len(random_samples)} audio files for processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe7a974-444e-4031-9679-f227c0fcf67f",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### Define Spectrogram Function\n",
    "The `save_spectrogram` function:\n",
    "1. Loads an audio file using `librosa`.\n",
    "2. Converts the audio into a spectrogram and scales it to decibels.\n",
    "3. Plots the spectrogram using `matplotlib` and saves it as a PNG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63656d22-a543-4d67-8913-a057e31a3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save a spectrogram\n",
    "def save_spectrogram(audio_file, output_file):\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(audio_file, sr=None)\n",
    "        \n",
    "        # Generate the spectrogram\n",
    "        spectrogram = librosa.stft(y)\n",
    "        spectrogram_db = librosa.amplitude_to_db(np.abs(spectrogram))\n",
    "        \n",
    "        # Plot the spectrogram\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(spectrogram_db, sr=sr, x_axis='time', y_axis='log', cmap='magma')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Spectrogram')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the spectrogram as a PNG file\n",
    "        plt.savefig(output_file, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {audio_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3187676-e1f0-40c3-8b2f-9e879608e2ef",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### Generate and Save Spectrograms\n",
    "For each randomly sampled audio file:\n",
    "1. Extract the file name without its extension.\n",
    "2. Save the spectrogram with a file name based on the audio file name in the `output_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86f1754d-16af-4d90-9a4f-617bb0f67352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Process and save spectrograms for the sampled audio files\n",
    "existing_spectrograms = [\n",
    "    file for file in os.listdir(output_dir) if file.endswith('.png')\n",
    "]\n",
    "\n",
    "# If there are already 50 spectrograms, skip creating new ones\n",
    "if len(existing_spectrograms) >= 50:\n",
    "    print(f\"There are already 50 spectrograms in the folder. No new spectrograms will be created.\")\n",
    "else:\n",
    "    for audio_file in random_samples:\n",
    "        # Recheck the folder size during each iteration to ensure the limit is respected\n",
    "        existing_spectrograms = [\n",
    "            file for file in os.listdir(output_dir) if file.endswith('.png')\n",
    "        ]\n",
    "        if len(existing_spectrograms) >= 50:\n",
    "            print(f\"Reached 50 spectrograms. Stopping further processing.\")\n",
    "            break\n",
    "\n",
    "        # Extract the base name of the audio file without extension\n",
    "        base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "\n",
    "        # Define the output file path with the same base name\n",
    "        output_file = os.path.join(output_dir, f'{base_name}_spectrogram.png')\n",
    "\n",
    "        # Generate and save the spectrogram\n",
    "        save_spectrogram(audio_file, output_file)\n",
    "\n",
    "    print(f\"Spectrograms saved in: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c95e3f-ce1b-45bb-9ce1-d96cd943036f",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### Summary\n",
    "- We loaded audio files from the dataset, randomly selected 50 of them, and converted them into spectrograms.\n",
    "- The spectrograms were saved in the `spectrograms` folder with filenames correlating to the original audio files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cbaca0-a8e9-4674-a8b2-c2f6d5f51a01",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "### Display 10 Spectrograms\n",
    "In this step, we visualize the spectrograms generated and saved in the `spectrograms` folder:\n",
    "\n",
    "1. **Retrieve Spectrogram Files**:\n",
    "   - Gather all `.png` files from the `spectrograms` folder.\n",
    "\n",
    "2. **Select Files for Display**:\n",
    "   - Limit the display to the first 10 spectrograms to make the visualization manageable.\n",
    "\n",
    "3. **Display Spectrograms**:\n",
    "   - Use `matplotlib` to create a 2x5 grid of images.\n",
    "   - Load each image using `Pillow` (`PIL`) and display it without axis labels for clarity.\n",
    "\n",
    "The visualization provides a quick overview of the generated spectrograms, allowing us to confirm their quality and consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db6014c-f1c1-43a5-95a9-d1487b9c6ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get all spectrogram PNG file paths\n",
    "spectrogram_files = [os.path.join(output_dir, file) for file in os.listdir(output_dir) if file.endswith('.png')]\n",
    "\n",
    "# Ensure we only display up to 5 files\n",
    "spectrograms_to_display = spectrogram_files[:5]\n",
    "\n",
    "# Display the spectrograms\n",
    "plt.figure(figsize=(50, 30))  #figure size for spectrograms\n",
    "for i, spectrogram_file in enumerate(spectrograms_to_display):\n",
    "    # Load the image\n",
    "    img = Image.open(spectrogram_file)\n",
    "    \n",
    "    # Extract the file name without the path\n",
    "    file_name = os.path.basename(spectrogram_file)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.subplot(1, 5, i + 1)  # Arrange in a 1x5 grid for 5 images\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Turn off axis labels for better visualization\n",
    "    plt.title(file_name, fontsize=10)  # Use the file name as the title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ffb374-e714-43e4-8aef-27bebe801cd6",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "## üêæ Dataset Exploration Task: Top 5 Categories with Most Sounds\n",
    "\n",
    "Your task is to:\n",
    "1. Count the number of `.wav` `.mp3` files in each category.\n",
    "2. Find the **top 5 categories** with the most sounds.\n",
    "3. Display the results in a simple bar chart.\n",
    "\n",
    "üí° **Why this is important**:\n",
    "- Helps identify which animal categories are most represented in the dataset.\n",
    "- Provides insight into potential imbalances in the dataset.\n",
    "\n",
    "Follow the template below to complete this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256e088-ef53-4628-849b-cf04fc15c696",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# üêæ Task: Top 5 Categories with Most Sounds (Including .mp3 Files)\n",
    "# Fill in the missing parts of the code to complete the task.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 0: Reset the category counts dictionary to ensure no previous data\n",
    "category_counts_task = {}\n",
    "\n",
    "# Step 1: Count the number of .wav and .mp3 files in each category\n",
    "for folder in animal_folders:\n",
    "    category_name = os.path.basename(folder)\n",
    "    \n",
    "    # Ensure file extensions are provided\n",
    "    required_extensions = ['', '']  # Participants must fill these in\n",
    "    if '.wav' and '.mp3' in required_extensions:\n",
    "        raise ValueError(\"üö® Error: File extensions ('.wav' and '.mp3') must be specified in the code.\")\n",
    "\n",
    "    # Count both .wav and .mp3 files\n",
    "    category_counts_task[category_name] = len([\n",
    "        file for file in os.listdir(folder)\n",
    "        if os.path.isfile(os.path.join(folder, file)) and (file.endswith('.wav') or file.endswith('.mp3'))\n",
    "    ])\n",
    "\n",
    "# Step 2: Sort the categories by the number of files in descending order\n",
    "sorted_categories = sorted(category_counts_task.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 3: Extract the top 5 categories\n",
    "top_5_categories = sorted_categories[:5]\n",
    "\n",
    "# Step 4: Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot the bar chart\n",
    "plt.bar(\n",
    "    [category[0] for category in top_5_categories],  # Category names\n",
    "    [category[1] for category in top_5_categories]   # Number of files\n",
    ")\n",
    "plt.title('Top 5 Categories with Most Sounds (Including .mp3 Files)')\n",
    "plt.xlabel('Animal Categories')\n",
    "plt.ylabel('Number of Sounds')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the top 5 categories (for reference)\n",
    "print(\"Top 5 Categories with Most Sounds (Including .mp3 Files):\")\n",
    "for category, count in top_5_categories:\n",
    "    print(f\"{category}: {count} sounds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2d7cf-49ea-4da3-9e0b-383e83637fc5",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "## üîß Data Preprocessing Task: Identify Outlier Audio Durations\n",
    "\n",
    "In this task, you will:\n",
    "- Use the `durations` list (pre-calculated) to find audio files with unusual durations.\n",
    "- Specifically, identify files shorter than 1 second or longer than 5 seconds.\n",
    "- Print the file names of these outliers.\n",
    "\n",
    "üí° **Why this is important**:\n",
    "- Outlier durations can indicate noisy or problematic data.\n",
    "- Cleaning such files ensures better model performance and reduces errors.\n",
    "\n",
    "Once you've identified the outliers, the notebook will validate your results. Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "813d6139-58db-4d49-955f-b9de302b87cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Calculate durations of all audio files\n",
    "durations = []\n",
    "for file in all_audio_files:\n",
    "    y, sr = librosa.load(file, sr=None)  # Load the audio file\n",
    "    durations.append(len(y) / sr)  # Calculate duration in seconds\n",
    "\n",
    "# Print a summary of durations for validation\n",
    "print(f\"Total files: {len(durations)}\")\n",
    "print(f\"Sample durations (in seconds): {durations[:5]}\")  # Display the first 10 durations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0a649-9b30-4e8a-bfee-8685b91649cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# üîß Task: Identify Outlier Audio Files\n",
    "# Fill in the missing part of the code to complete the task.\n",
    "\n",
    "# Initialize a list to store outlier files\n",
    "outliers = []\n",
    "\n",
    "# Loop through the durations and identify outliers\n",
    "for i, duration in enumerate(durations):\n",
    "    # Add a condition to check for files shorter than 1 second or longer than 5 seconds\n",
    "    if duration < 1 or duration > 5:  # <- This is the condition you need focus on (Please fill this in)\n",
    "        outliers.append(all_audio_files[i])\n",
    "\n",
    "# Print the outliers\n",
    "print(\"Your outliers:\", outliers)\n",
    "\n",
    "# Validation (this is pre-written and does not require editing)\n",
    "correct_outliers = [\n",
    "    all_audio_files[i] for i, duration in enumerate(durations) if duration < 1 or duration > 5\n",
    "]\n",
    "if outliers == correct_outliers:\n",
    "    print(\"‚úÖ Correct! Your outliers match the expected list.\")\n",
    "else:\n",
    "    print(\"‚ùå Incorrect. Double-check your condition for duration checks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c61867-bede-426d-8a04-170492cb5b04",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "## üéØ Feature Extraction Task: Generate and Visualize MFCCs\n",
    "\n",
    "In this task, you will:\n",
    "- Pick any audio file from the dataset (or use the random selection provided).\n",
    "- Extract its Mel-Frequency Cepstral Coefficients (MFCCs).\n",
    "- Plot a heatmap of the MFCCs to visualize the feature representation.\n",
    "\n",
    "üí° **Why this is important**:\n",
    "- MFCCs are widely used in audio machine learning tasks as key features.\n",
    "- Visualizing them helps you understand the transformation from raw audio to numerical features.\n",
    "\n",
    "Use the cell below to generate your MFCC heatmap. Then, compare your visualization with other team members!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df980e8c-3339-42e2-a284-000061c9cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "\n",
    "# üéØ Task: Generate and Visualize MFCCs\n",
    "# Fill in the missing part of the code to complete the task.\n",
    "\n",
    "# Select a random audio file (or replace with a specific file path)\n",
    "sample_file_task = random.choice(all_audio_files)\n",
    "\n",
    "# Load the audio file\n",
    "y_task, sr_task = librosa.load(sample_file_task, sr=16000)\n",
    "\n",
    "# Generate the MFCCs\n",
    "mfccs_task = librosa.feature.mfcc(y=y_task, sr=sr_task, n_mfcc=13)  # <- Focus on understanding this line\n",
    "\n",
    "# Visualize the MFCCs\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs_task, sr=sr_task, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title(f\"MFCC for {os.path.basename(sample_file_task)}\")\n",
    "plt.show()\n",
    "\n",
    "# Validation: Compare with other team members to verify the visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3a76c-8498-403e-ae02-52da7a6cd9bd",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## insert your teammate's screenshot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02c22b3c-ae4b-4b99-b8c7-a6f168adb793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display the image\n",
    "image_path = r\"C:\\Users\\deanf\\OneDrive\\CTMC\\MAppAI\\SIT764 Team Project A\\Assignments\\Week 03\\Onboarding pic.png\"  \n",
    "display(Image(filename=image_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0afb4c-fa2f-4f8f-b28e-13c3162c773d",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"font-size: 1.5em;\">\n",
    "    <strong>Congratulations!</strong>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35ed15-1ce6-4c6d-bb58-554287531cf5",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "Well done! You‚Äôve completed the onboarding task for **Project Echo**. By familiarizing yourself with the project's structure, setting up your environment, and completing the practical tasks, you are now prepared to contribute effectively to the development of this exciting AI/ML solution for wildlife conservation. \n",
    "\n",
    "----------\n",
    "\n",
    "### üö®Next Steps:üö®\n",
    "\n",
    "1. Open the Terminal in the Current Folder:\n",
    "\n",
    "If you are not already in the terminal, open it in the folder Tutorials/Engine team first Git Commit/.\n",
    "\n",
    "2. **Create a New Git Branch**:\n",
    "   - Go back to the terminal.\n",
    "   - Create a branch using the naming convention:\n",
    "     ```\n",
    "     git checkout -b EE/<your_name>/<task_name>\n",
    "     ```\n",
    "     Example:\n",
    "     ```\n",
    "     git checkout -b EE/Riley/UpdateOnboardingTask\n",
    "     ```\n",
    "\n",
    "3. **Move the Notebook**:\n",
    "   - Move the renamed notebook (`Onboarding_Task_[Initials].ipynb`) back into the same directory:  \n",
    "     `Tutorials/Engine team first Git Commit/`.\n",
    "\n",
    "4. **Stage and Commit Changes**:\n",
    "   - Stage your changes:\n",
    "     ```\n",
    "     git add Onboarding_Task_[Initials].ipynb\n",
    "     ```\n",
    "   - Commit your changes with a meaningful message:\n",
    "     ```\n",
    "     git commit -m \"Updated Onboarding Task with <your_name>\"\n",
    "     ```\n",
    "\n",
    "5. **Push Changes to GitHub**:\n",
    "   - Push your changes to the new branch:\n",
    "     ```\n",
    "     git push origin EE/<your_name>/<task_name>\n",
    "     ```\n",
    "     Example:\n",
    "     ```\n",
    "     git push origin EE/Riley/UpdateOnboardingTask\n",
    "     ```\n",
    "\n",
    "6. **Create a Pull Request**:\n",
    "   - Open GitHub in your browser and navigate to the Project Echo repository.\n",
    "   - You should see a notification for your new branch.\n",
    "   - Click on **Create Pull Request**, provide a brief description of your changes, and submit it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc245bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectecho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
