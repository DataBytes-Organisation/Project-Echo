# -*- coding: utf-8 -*-
"""Kunal_anand_s224704431_Benchmarking_framework_sprint2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ONyEQTluuUMwp7T2at2K-rDKGpoS1zqp
"""

# Install deps if needed
# !pip install librosa soundfile --quiet

import os, math, glob, itertools, shutil
from dataclasses import dataclass
from typing import Callable, Dict, List, Tuple
import numpy as np
import matplotlib.pyplot as plt

import librosa, soundfile as sf
import tensorflow as tf
from tensorflow.keras import layers as L, Model
from sklearn.metrics import classification_report, confusion_matrix

# Make TF errors easier to debug
tf.config.run_functions_eagerly(True)

print("TensorFlow:", tf.__version__)

# ====== OPTION A: USE REAL DATASET ======
# Expected structure:
# data/
#   train/
#     class1/*.wav
#     class2/*.wav
#   val/
#     class1/*.wav
#     class2/*.wav
#
# Example (Windows):
# TRAIN_DIR = r"C:\Users\Kunal\Desktop\audio_data\train"
# VAL_DIR   = r"C:\Users\Kunal\Desktop\audio_data\val"

# ====== OPTION B: AUTO-GENERATE DUMMY DATASET ======
def generate_dummy_audio_dataset(root="demo_audio", n_classes=3, n_train=20, n_val=5,
                                 sr=16000, dur=1.0):
    if os.path.exists(root): shutil.rmtree(root)
    for split, n_samples in [("train", n_train), ("val", n_val)]:
        for c in range(n_classes):
            class_dir = os.path.join(root, split, f"class{c}")
            os.makedirs(class_dir, exist_ok=True)
            for i in range(n_samples):
                y = np.random.randn(int(sr*dur)) * 0.01
                sf.write(os.path.join(class_dir, f"sample{i}.wav"), y, sr)
    return os.path.join(root, "train"), os.path.join(root, "val")

# Use dummy data by default
TRAIN_DIR, VAL_DIR = generate_dummy_audio_dataset(root="demo_audio", n_classes=3)

# ====== AUDIO SETTINGS ======
SR        = 16000
N_MELS    = 64
HOP_LEN   = 256
N_FFT     = 1024
TIME_LEN  = 128

# ====== MODEL SETTINGS ======
N_CLASSES = None
ARCH_NAME = "SafeCNN"  # "SafeCNN" or "PatchTransformerTiny"
BATCH     = 16
EPOCHS    = 3
LR        = 3e-4
SEED      = 42

np.random.seed(SEED)
tf.random.set_seed(SEED)

print("Config ready. Training dir:", TRAIN_DIR)
print("Validation dir:", VAL_DIR)

def discover_classes(root_dir: str) -> List[str]:
    classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])
    if not classes:
        raise RuntimeError(f"No class folders found in {root_dir}")
    return classes

def list_wavs(root_dir: str, classes: List[str]):
    files, labels = [], []
    for idx, c in enumerate(classes):
        for f in glob.glob(os.path.join(root_dir, c, "*.wav")):
            files.append(f)
            labels.append(idx)
    return files, np.array(labels, dtype=np.int32)

classes = discover_classes(TRAIN_DIR)
if N_CLASSES is None: N_CLASSES = len(classes)

train_files, train_labels = list_wavs(TRAIN_DIR, classes)
val_files,   val_labels   = list_wavs(VAL_DIR,   classes)

print(f"Classes ({len(classes)}): {classes}")
print(f"Train files: {len(train_files)}, Val files: {len(val_files)}")

def wav_to_logmel(path: str,
                  sr: int = SR,
                  n_mels: int = N_MELS,
                  n_fft: int = N_FFT,
                  hop_length: int = HOP_LEN,
                  time_len: int = TIME_LEN) -> np.ndarray:
    y, _ = librosa.load(path, sr=sr, mono=True)
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft,
                                       hop_length=hop_length, n_mels=n_mels)
    logmel = librosa.power_to_db(S, ref=np.max)

    if logmel.shape[1] < time_len:
        logmel = np.pad(logmel, ((0,0),(0,time_len-logmel.shape[1])), mode="constant")
    else:
        logmel = logmel[:, :time_len]

    logmel = (logmel - logmel.mean()) / (logmel.std() + 1e-6)
    return logmel[..., np.newaxis].astype(np.float32)

class AudioMelSequence(tf.keras.utils.Sequence):
    def __init__(self, files: List[str], labels: np.ndarray, batch_size: int, shuffle: bool = True):
        self.files  = np.array(files)
        self.labels = np.array(labels)
        self.batch_size = max(1, int(batch_size))
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        n = len(self.files)
        return 0 if n == 0 else int(np.ceil(n / self.batch_size))

    def on_epoch_end(self):
        if self.shuffle and len(self.files) > 0:
            idx = np.arange(len(self.files))
            np.random.shuffle(idx)
            self.files  = self.files[idx]
            self.labels = self.labels[idx]

    def __getitem__(self, i):
        if len(self.files) == 0:
            raise IndexError("Empty dataset")
        start = i * self.batch_size
        end   = min((i+1) * self.batch_size, len(self.files))
        if start >= end:
            raise IndexError("Empty slice for this index")

        batch_files  = self.files[start:end]
        batch_labels = self.labels[start:end]
        # Build only from available files
        X_list = [wav_to_logmel(p) for p in batch_files]
        if len(X_list) == 0:
            raise IndexError("No files in this batch")
        X = np.stack(X_list, axis=0)
        y = batch_labels
        return X, y

# Rebuild datasets after replacing the class
train_ds = AudioMelSequence(train_files, train_labels, batch_size=BATCH, shuffle=True)
val_ds   = AudioMelSequence(val_files,   val_labels,   batch_size=BATCH, shuffle=False)

print("Len train:", len(train_ds), "Len val:", len(val_ds))
if len(val_ds) == 0:
    print("⚠️ Validation set is empty — metrics based on val_ds will be skipped.")

def gelu(x): return tf.nn.gelu(x)

class SafeResBlock(L.Layer):
    def __init__(self, in_ch, out_ch, stride=(1,1)):
        super().__init__()
        self.conv1 = L.Conv2D(out_ch, 3, strides=stride, padding="same", use_bias=False)
        self.bn1, self.act1 = L.BatchNormalization(), L.Activation("relu")
        self.conv2 = L.Conv2D(out_ch, 3, padding="same", use_bias=False)
        self.bn2 = L.BatchNormalization()
        self.proj = None
        if (in_ch != out_ch) or (stride != (1,1)):
            self.proj, self.pbn = L.Conv2D(out_ch, 1, strides=stride, padding="same", use_bias=False), L.BatchNormalization()
        self.act_out = L.Activation("relu")

    def call(self, x, training=None):
        idt = x
        y = self.act1(self.bn1(self.conv1(x), training=training))
        y = self.bn2(self.conv2(y), training=training)
        if self.proj is not None: idt = self.pbn(self.proj(idt), training=training)
        return self.act_out(y + idt)

class SafeCNN(Model):
    def __init__(self, n_mels, n_classes, width=32):
        super().__init__()
        self.stem = L.Conv2D(width, 3, strides=(2,2), padding="same", use_bias=False)
        self.sbn, self.sact = L.BatchNormalization(), L.Activation("relu")
        self.blocks = [
            SafeResBlock(width, width, (1,2)), SafeResBlock(width, width),
            SafeResBlock(width, width*2, (2,2)), SafeResBlock(width*2, width*2),
            SafeResBlock(width*2, width*4, (2,2)), SafeResBlock(width*4, width*4)
        ]
        self.gap, self.drop, self.fc = L.GlobalAveragePooling2D(), L.Dropout(0.2), L.Dense(n_classes)

    def call(self, x, training=None):
        x = self.sact(self.sbn(self.stem(x), training=training))
        for b in self.blocks: x = b(x, training=training)
        return self.fc(self.drop(self.gap(x), training=training))

# PatchTransformerTiny omitted for brevity here; same as before...
# (you can keep it if you want to switch ARCH_NAME)
@dataclass
class NetArgs:
    n_mels: int
    n_classes: int
    time_len: int

ARCH_REGISTRY: Dict[str, Callable[[NetArgs], tf.keras.Model]] = {}

def register_arch(name):
    def deco(fn): ARCH_REGISTRY[name] = fn; return fn
    return deco

@register_arch("SafeCNN")
def make_safe_cnn(cfg: NetArgs):
    inp = L.Input(shape=(cfg.n_mels, cfg.time_len, 1))
    out = SafeCNN(cfg.n_mels, cfg.n_classes)(inp)
    return Model(inp, out, name="SafeCNN")

def compile_model(model, lr=LR):
    opt = getattr(tf.keras.optimizers, "AdamW", tf.keras.optimizers.Adam)(learning_rate=lr)
    model.compile(optimizer=opt,
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name="acc")])
    return model

def load_model(arch, n_mels, n_classes, time_len):
    cfg = NetArgs(n_mels=n_mels, n_classes=n_classes, time_len=time_len)
    model = ARCH_REGISTRY[arch](cfg)
    return compile_model(model)

xb, yb = train_ds[0]
fig, axes = plt.subplots(1, min(5, xb.shape[0]), figsize=(15,3))
for i, ax in enumerate(axes):
    ax.imshow(xb[i,:,:,0], aspect="auto", origin="lower", cmap="magma")
    ax.set_title(classes[yb[i]])
    ax.axis("off")
plt.suptitle("Sample Log-Mel Spectrograms")
plt.show()

model = load_model(ARCH_NAME, n_mels=N_MELS, n_classes=N_CLASSES, time_len=TIME_LEN)
history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=2)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history["acc"], label="train")
plt.plot(history.history["val_acc"], label="val")
plt.title("Accuracy"); plt.xlabel("epoch"); plt.ylabel("acc"); plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history["loss"], label="train")
plt.plot(history.history["val_loss"], label="val")
plt.title("Loss"); plt.xlabel("epoch"); plt.ylabel("loss"); plt.legend()
plt.show()

val_loss, val_acc = model.evaluate(val_ds, verbose=0)
print(f"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc*100:.2f}%")

from sklearn.metrics import classification_report, confusion_matrix

if len(val_ds) == 0:
    print("⚠️ No validation batches available. Skipping per-class report & confusion matrix.")
else:
    y_true, y_pred = [], []
    for xb, yb in val_ds:
        if xb.shape[0] == 0:  # just in case
            continue
        logits = model.predict(xb, verbose=0)
        y_true.extend(yb.tolist())
        y_pred.extend(np.argmax(logits, axis=1).tolist())

    if len(y_true) == 0:
        print("⚠️ No validation examples collected. Skipping report.")
    else:
        print("\nClassification Report:")
        try:
            print(classification_report(y_true, y_pred, target_names=classes, digits=4))
        except Exception:
            # Fallback if target_names length mismatches
            print(classification_report(y_true, y_pred, digits=4))

        cm = confusion_matrix(y_true, y_pred, labels=list(range(N_CLASSES)))
        import itertools, matplotlib.pyplot as plt
        plt.figure(figsize=(6 + N_CLASSES*0.2, 5 + N_CLASSES*0.2))
        plt.imshow(cm, cmap="Blues")
        plt.title("Confusion Matrix"); plt.colorbar()
        plt.xticks(range(len(classes)), classes, rotation=45, ha="right")
        plt.yticks(range(len(classes)), classes)
        thresh = cm.max() / 2. if cm.max() > 0 else 0.5
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, str(cm[i, j]), ha="center", va="center",
                     color="white" if cm[i, j] > thresh else "black")
        plt.xlabel("Predicted"); plt.ylabel("True"); plt.tight_layout(); plt.show()

