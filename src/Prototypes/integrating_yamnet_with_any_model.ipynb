{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated YAMNet Sound Event Detection with any model\n",
    "\n",
    "This code demonstrates the integration of the YAMNet sound event detection model with a custom, optimized audio classification model. By combining the capabilities of both, this integration forms the foundation of advanced sound event detection, paving the way for more precise audio classification tasks.\n",
    "\n",
    "## **How it Works**:\n",
    "\n",
    "1. **Load YAMNet Model**: Initializes the YAMNet model, which is a pre-trained deep net that predicts 521 audio event classes based on the AudioSet-YouTube corpus.\n",
    "\n",
    "2. **Process Audio**: The provided audio file (`test.m4a`) is read and segmented into 1-second chunks.\n",
    "\n",
    "3. **YAMNet Predictions**: Each chunk of audio data is passed through the YAMNet model. If the highest probability class from YAMNet is 'Animal' and the confidence is above 0.3, this chunk is further processed.\n",
    "\n",
    "4. **Integrated Custom Model Prediction**: If YAMNet detects the presence of an 'Animal' sound, the chunk is sent to the custom model for a more detailed classification.\n",
    "\n",
    "5. **Dataframe for Results**: A dataframe is maintained to store the results, capturing start time, end time, YAMNet prediction label and probability, and the custom model's prediction label and probability.\n",
    "\n",
    "6. **Visualization**: The Mel spectrogram of the processed audio segment is displayed in real-time.\n",
    "\n",
    "## **Usage**:\n",
    "\n",
    "1. Ensure that you have all the necessary dependencies and modules imported.\n",
    "2. Place your audio file (`test.m4a`) in the appropriate directory.\n",
    "3. Run the script.\n",
    "4. Inspect the dataframe `df` for the results. This dataframe captures the segment's start and end times, the label predicted by YAMNet, its confidence, and the label and confidence from the custom optimized model (if YAMNet detected an 'Animal').\n",
    "\n",
    "## **Author**:\n",
    "Rohit Dhanda\n",
    "\n",
    "## **Note**:\n",
    "The `predict_on_audio(binary_file.read())` function in the code should be defined elsewhere in your project. It's responsible for obtaining predictions from your custom model.\n",
    "\n",
    "---\n",
    "\n",
    "I hope this documentation helps explain the function and use of your integrated code. If any modifications are needed, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import yamnet.params as params\n",
    "import yamnet.yamnet as yamnet_model\n",
    "import librosa\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Load YAMNet model\n",
    "yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet.load_weights('yamnet/yamnet.h5')\n",
    "yamnet_classes = yamnet_model.class_names('yamnet/yamnet_class_map.csv')\n",
    "\n",
    "frame_len = int(params.SAMPLE_RATE * 1)  # 1sec\n",
    "\n",
    "# Read the whole audio file\n",
    "filename = 'test.m4a'\n",
    "data, sr = librosa.load(filename, sr=params.SAMPLE_RATE)\n",
    "\n",
    "# Split the audio data into 1 second chunks\n",
    "chunks = np.array_split(data, len(data) // frame_len)\n",
    "\n",
    "# Dataframe to store the results\n",
    "df = pd.DataFrame(columns=['start_time', 'end_time', 'yamnet_label', 'yamnet_probability', 'your_model_label', 'your_model_probability'])\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "for cnt, frame_data in enumerate(chunks):\n",
    "    print(len(frame_data))\n",
    "    start_time = cnt\n",
    "    end_time = cnt + 1\n",
    "\n",
    "    # model prediction\n",
    "    scores, melspec = yamnet.predict(np.reshape(frame_data, [1, -1]), steps=1)\n",
    "    yamnet_prediction = np.mean(scores, axis=0)\n",
    "\n",
    "    # visualize input audio\n",
    "    plt.imshow(melspec.T, cmap='jet', aspect='auto', origin='lower')\n",
    "    plt.pause(0.001)\n",
    "    plt.show()\n",
    "\n",
    "    top5_i = np.argsort(yamnet_prediction)[::-1][:5]\n",
    "\n",
    "    # If the top prediction is 'Animal', save the audio segment and send it to your model\n",
    "    if yamnet_classes[top5_i[0]] == 'Animal' and yamnet_prediction[top5_i[0]] > 0.3:\n",
    "        # Pad the audio if it's shorter than 1 second\n",
    "        if len(frame_data) < frame_len:\n",
    "            padding = frame_len - len(frame_data)\n",
    "            frame_data = np.pad(frame_data, (0, padding), 'constant')\n",
    "\n",
    "        # Create a temporary file to store the frame data\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_audio_file:\n",
    "            sf.write(temp_audio_file.name, frame_data, params.SAMPLE_RATE)\n",
    "            # Reload the audio file as a binary file\n",
    "            with open(temp_audio_file.name, 'rb') as binary_file:\n",
    "                # Get prediction from your model\n",
    "                your_model_prediction, your_model_probability = predict_on_audio(binary_file.read())\n",
    "\n",
    "            # Add the results to the DataFrame\n",
    "            df = df.append({\n",
    "                'start_time': start_time, \n",
    "                'end_time': end_time,\n",
    "                'yamnet_label': 'Animal',\n",
    "                'yamnet_probability': yamnet_prediction[top5_i[0]],\n",
    "                'your_model_label': your_model_prediction,\n",
    "                'your_model_probability': your_model_probability\n",
    "            }, ignore_index=True)\n",
    "# ...\n",
    "\n",
    "    else:\n",
    "        # Add the results to the DataFrame\n",
    "        df = df.append({\n",
    "            'start_time': start_time, \n",
    "            'end_time': end_time,\n",
    "            'yamnet_label': yamnet_classes[top5_i[0]],\n",
    "            'yamnet_probability': yamnet_prediction[top5_i[0]],\n",
    "            'your_model_label': None,\n",
    "            'your_model_probability': None\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimised model \n",
    "for this we have changed ramdom_subslection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import base64\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "# Make sure to use the correct configuration\n",
    "config = {\n",
    "    'AUDIO_SAMPLE_RATE': 48000,\n",
    "    'AUDIO_CLIP_DURATION': 1,\n",
    "    'AUDIO_NFFT': 2048,\n",
    "    'AUDIO_STRIDE': 200,\n",
    "    'AUDIO_MELS': 260,\n",
    "    'AUDIO_FMIN': 20,\n",
    "    'AUDIO_FMAX': 13000,\n",
    "    'AUDIO_WINDOW': None,\n",
    "    'AUDIO_TOP_DB': 80,\n",
    "    'MODEL_INPUT_IMAGE_CHANNELS': 3,\n",
    "    'MODEL_INPUT_IMAGE_WIDTH': 260,\n",
    "    'MODEL_INPUT_IMAGE_HEIGHT': 260\n",
    "}\n",
    "\n",
    "\n",
    "class_names= ['Aegotheles Cristatus', 'Alauda Arvensis', 'Caligavis Chrysops', 'Capra Hircus', 'Cervus Unicolour', 'Colluricincla Harmonica', 'Corvus Coronoides',\n",
    "              'Dama Dama', 'Eopsaltria Australis', 'Felis Catus', 'Pachycephala Rufiventris', 'Ptilotula Penicillata', 'Rattus Norvegicus', 'Strepera Graculina', 'Sus Scrofa']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('models/echo_model/2/')\n",
    "\n",
    "# Define the preprocessing steps as functions.\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "    # this function is adapted from generic_engine_pipeline.ipynb\n",
    "    # TODO: need to create a pipeline library and link same code into engine\n",
    "    ########################################################################################\n",
    "def combined_pipeline(config, audio_clip):\n",
    "\n",
    "    # Load the audio data with librosa(works only while give direct audio to it)\n",
    "    #audio_clip, sample_rate = librosa.load(audio_clip, sr=config['AUDIO_SAMPLE_RATE'])\n",
    "    \n",
    "    #to use it with yamnet\n",
    "    file = io.BytesIO(audio_clip)\n",
    "    audio_clip, sample_rate = librosa.load(file, sr=config['AUDIO_SAMPLE_RATE'])\n",
    "        \n",
    "    # keep right channel only\n",
    "    if audio_clip.ndim == 2 and audio_clip.shape[0] == 2:\n",
    "        audio_clip = audio_clip[1, :]\n",
    "        \n",
    "    # cast to float32 type\n",
    "    audio_clip = audio_clip.astype(np.float32)\n",
    "        \n",
    "    # analyse a random 5 second subsection\n",
    "    audio_clip = load_random_subsection(audio_clip, duration_secs=config['AUDIO_CLIP_DURATION'])\n",
    "\n",
    "    # Compute the mel-spectrogram\n",
    "    image = librosa.feature.melspectrogram(\n",
    "        y=audio_clip, \n",
    "        sr=config['AUDIO_SAMPLE_RATE'], \n",
    "        n_fft=config['AUDIO_NFFT'], \n",
    "        hop_length=config['AUDIO_STRIDE'], \n",
    "        n_mels=config['AUDIO_MELS'],\n",
    "        fmin=config['AUDIO_FMIN'],\n",
    "        fmax=config['AUDIO_FMAX'],\n",
    "        win_length=config['AUDIO_WINDOW'])\n",
    "\n",
    "    # Optionally convert the mel-spectrogram to decibel scale\n",
    "    image = librosa.power_to_db(\n",
    "        image, \n",
    "        top_db=config['AUDIO_TOP_DB'], \n",
    "        ref=1.0)\n",
    "        \n",
    "    # Calculate the expected number of samples in a clip\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "        \n",
    "    # swap axis and clip to expected samples to avoid rounding errors\n",
    "    image = np.moveaxis(image, 1, 0)\n",
    "    image = image[0:expected_clip_samples,:]\n",
    "        \n",
    "    # reshape into standard 3 channels to add the color channel\n",
    "    image = tf.expand_dims(image, -1)\n",
    "        \n",
    "    # most pre-trained model classifer model expects 3 color channels\n",
    "    image = tf.repeat(image, config['MODEL_INPUT_IMAGE_CHANNELS'], axis=2)\n",
    "        \n",
    "    # calculate the image shape and ensure it is correct\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "    image = tf.ensure_shape(image, [expected_clip_samples, config['AUDIO_MELS'], config['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "        \n",
    "    # note here a high quality LANCZOS5 is applied to resize the image to match model image input size\n",
    "    image = tf.image.resize(image, (config['MODEL_INPUT_IMAGE_WIDTH'], config['MODEL_INPUT_IMAGE_HEIGHT']), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "\n",
    "\n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.0000001)\n",
    "        \n",
    "    return image, audio_clip, sample_rate\n",
    "\n",
    "\n",
    "\n",
    " ########################################################################################\n",
    "    # Function to predict class and probability given a prediction\n",
    "    ########################################################################################\n",
    "def predict_class( predictions):\n",
    "    # Get the index of the class with the highest predicted probability\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    print(predicted_index, type(predicted_index))\n",
    "\n",
    "    # Get the class name using the predicted index\n",
    "    predicted_class = self.class_names[predicted_index]\n",
    "    # Calculate the predicted probability for the selected class\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[predicted_index].numpy()\n",
    "    # Round the probability to 2 decimal places\n",
    "    predicted_probability = round(predicted_probability, 2)\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "# this method takes in binary audio data and encodes to string\n",
    "def audio_to_string( audio_binary):\n",
    "    base64_encoded_data = base64.b64encode(audio_binary)\n",
    "    base64_message = base64_encoded_data.decode('utf-8')\n",
    "    return base64_message    \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "    # this method takes in string and ecodes to audio binary data\n",
    "    ########################################################################################\n",
    "def string_to_audio( audio_string):\n",
    "    base64_img_bytes = audio_string.encode('utf-8')\n",
    "    decoded_data = base64.decodebytes(base64_img_bytes)\n",
    "    return decoded_data\n",
    "    \n",
    "def predict_class(predictions):\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    predicted_class = class_names[predicted_index]\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[0, predicted_index].numpy()\n",
    "    predicted_probability = round(predicted_probability, 2)\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_random_subsection(sample, duration_secs):\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Determine the audio file's duration in samples\n",
    "    audio_duration_samples = tf.shape(sample)[0]\n",
    "        \n",
    "        # Determine the required padding length to reach 1-second duration\n",
    "    padding_length = tf.maximum(0, duration_secs * config['AUDIO_SAMPLE_RATE'] - audio_duration_samples)\n",
    "        \n",
    "        # If padding_length is zero or negative, clip the audio to the desired length\n",
    "    if padding_length <= 0:\n",
    "        sample = sample[:duration_secs * config['AUDIO_SAMPLE_RATE']]\n",
    "    else:\n",
    "            # Apply padding if necessary (if padding_length is zero, this will not affect the sample)\n",
    "        padding = tf.zeros([padding_length], dtype=sample.dtype)\n",
    "        sample = tf.concat([sample, padding], axis=0)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_on_audio(audio_binary):\n",
    "    # Preprocess the audio to be suitable for your model\n",
    "    image, audio_clip, sample_rate = combined_pipeline(config, audio_binary)\n",
    "    \n",
    "    # Add a dimension to match the model's input shape\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    # Make the prediction\n",
    "    predictions_array = model.predict(image)[0]  # Assuming the model returns 2D array, take the first element\n",
    "    \n",
    "    # Pair the class names with the predictions\n",
    "    paired_predictions = list(zip(class_names, predictions_array))\n",
    "    \n",
    "    # Sort the paired predictions based on probability\n",
    "    sorted_predictions = sorted(paired_predictions, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_predictions[:2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
