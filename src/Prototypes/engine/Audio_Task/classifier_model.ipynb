{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136210-16d3-4266-b5e5-4050123633bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created (or already exists): C:\\Users\\riley\\Documents\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Animal Sounds\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Environment\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\Animal Sounds\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\Environment\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\Mixed Sounds\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\train\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\validation\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\train\\Animal Sounds\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\train\\Environment\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\validation\\Animal Sounds\n",
      "Directory created (or already exists): C:\\Users\\riley\\Documents\\Spectrograms\\validation\\Environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files: 100%|██████████████████████████████████████████████████████| 32/32 [00:00<00:00, 3210.57file/s]\n",
      "Processing audio files: 100%|██████████████████████████████████████████████████████| 33/33 [00:00<00:00, 3001.26file/s]\n",
      "Generating mixed sounds:  28%|███████████████▊                                        | 9/32 [00:05<00:13,  1.71file/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, ReLU\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Input, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Directory structure for storing audio data, spectrograms, and training/validation datasets.\n",
    "def setup_directories(base_dir):\n",
    "    subdirs = {\n",
    "        # Serves as Base directory for all data\n",
    "        \"base_dir\": base_dir,\n",
    "\n",
    "        # These are the Raw audio directories\n",
    "        \"animal_dir\": os.path.join(base_dir, \"Animal Sounds\"),\n",
    "        \"env_dir\": os.path.join(base_dir, \"Environment\"),\n",
    "\n",
    "        # Spectrogram storage\n",
    "        \"spectrogram_dir\": os.path.join(base_dir, \"Spectrograms\"),\n",
    "        \"animal_spectrogram_dir\": os.path.join(base_dir, \"Spectrograms\", \"Animal Sounds\"),\n",
    "        \"env_spectrogram_dir\": os.path.join(base_dir, \"Spectrograms\", \"Environment\"),\n",
    "        \"mixed_spectrogram_dir\": os.path.join(base_dir, \"Spectrograms\", \"Mixed Sounds\"),\n",
    "\n",
    "        # The Train and validation directories\n",
    "        \"train_dir\": os.path.join(base_dir, \"Spectrograms\", \"train\"),\n",
    "        \"val_dir\": os.path.join(base_dir, \"Spectrograms\", \"validation\"),\n",
    "        \"train_animal_dir\": os.path.join(base_dir, \"Spectrograms\", \"train\", \"Animal Sounds\"),\n",
    "        \"train_env_dir\": os.path.join(base_dir, \"Spectrograms\", \"train\", \"Environment\"),\n",
    "        \"val_animal_dir\": os.path.join(base_dir, \"Spectrograms\", \"validation\", \"Animal Sounds\"),\n",
    "        \"val_env_dir\": os.path.join(base_dir, \"Spectrograms\", \"validation\", \"Environment\"),\n",
    "    }\n",
    "\n",
    "    return create_directories(base_dir, subdirs)\n",
    "\n",
    "\n",
    "def create_directories(base_dir, subdirs):\n",
    "    directories = {key: os.path.join(base_dir, path) for key, path in subdirs.items()}\n",
    "\n",
    "    for path in directories.values():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"Directory created (or already exists): {path}\")\n",
    "\n",
    "    return directories\n",
    "\n",
    "\n",
    "# Base directory\n",
    "base_dir = r\"C:\\Users\\riley\\Documents\"\n",
    "\n",
    "# Setup directories\n",
    "directories = setup_directories(base_dir)\n",
    "\n",
    "# Processing paths for specific audio categories\n",
    "animal_dir = directories[\"animal_dir\"]\n",
    "env_dir = directories[\"env_dir\"]\n",
    "animal_spectrogram_dir = directories[\"animal_spectrogram_dir\"]\n",
    "env_spectrogram_dir = directories[\"env_spectrogram_dir\"]\n",
    "mixed_spectrogram_dir = directories[\"mixed_spectrogram_dir\"]\n",
    "train_dir = directories[\"train_dir\"]\n",
    "val_dir = directories[\"val_dir\"]\n",
    "\n",
    "# Processing paths for training and validation subcategories\n",
    "train_animal_dir = directories[\"train_animal_dir\"]\n",
    "train_env_dir = directories[\"train_env_dir\"]\n",
    "val_animal_dir = directories[\"val_animal_dir\"]\n",
    "val_env_dir = directories[\"val_env_dir\"]\n",
    "\n",
    "\n",
    "# Bandpass filter to retain frequencies between low and high\n",
    "def apply_bandpass(y, sr, low=500, high=8000):\n",
    "    sos = scipy.signal.butter(10, [low, high], btype='band', fs=sr, output='sos')\n",
    "    return scipy.signal.sosfilt(sos, y)\n",
    "\n",
    "\n",
    "# Normalize the audio signal to range between -1 and 1\n",
    "def normalize(y):\n",
    "    return y / np.max(np.abs(y))\n",
    "\n",
    "\n",
    "# Remove silent sections from the audio signal based on amplitude threshold\n",
    "def remove_silence(y, sr):\n",
    "    intervals = librosa.effects.split(y, top_db=20)\n",
    "    return np.concatenate([y[start:end] for start, end in intervals])\n",
    "\n",
    "\n",
    "# Convert an audio signal to a mel spectrogram, save as an image, and resize\n",
    "def saved_spectrogram(y, sr, output_path, size=(128, 128)):\n",
    "     # Generates mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "     # Convert power spectrogram to decibel (logarithmic scale)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    # Plot and saves the spectrogram\n",
    "    plt.figure(figsize=(6, 6))  # Turn off the axis in order for clean image\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', cmap='magma')  # Displays spectrogram\n",
    "    plt.axis('off')  # Close the plot to free memory\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)  # Resizes and convert the image to RGB format\n",
    "    plt.close()\n",
    "    Image.open(output_path).convert('RGB').resize(size).save(output_path)\n",
    "\n",
    "\n",
    "# Augmentation setup\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5), #Gaussian noise is added to the audio\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5), # Stretches or compresses time and makes sure it maintains pitch \n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),  #pitch is shifted up by up to +/- 4 semitones\n",
    "    Shift(min_shift=-0.5, max_shift=0.5, p=0.5) #audio is shifted in time by a fraction of the total duration\n",
    "])\n",
    "\n",
    "# Process audio files to generate spectrograms\n",
    "def processing_audio(input_dir, output_dir, augmentations):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.wav')] # Retrieve all .wav files in the input directory\n",
    "    for file in tqdm(files, desc=\"Processing audio files\", unit=\"file\"):  # Loop through each audio file\n",
    "        try:\n",
    "            for i in range(5):  # Generate 5 augmented versions per file\n",
    "                output_path = os.path.join(output_dir, f\"{os.path.splitext(file)[0]}_aug_{i}.png\")\n",
    "                if os.path.exists(output_path):\n",
    "                    continue # Skip processing if the file already exists\n",
    "                file_path = os.path.join(input_dir, file)\n",
    "                y, sr = librosa.load(file_path, sr=None)\n",
    "                y_aug = augmentations(samples=y, sample_rate=sr)  # Load with the original sampling rate\n",
    "                y_cleaned = remove_silence(normalize(apply_bandpass(y_aug, sr)), sr) # Apply audio augmentations\n",
    "                saved_spectrogram(y_cleaned, sr, output_path) # Generate and save the spectrogram\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")  # Notify any errors during processing\n",
    "\n",
    "\n",
    "# Generate mixed spectrograms by combining animal and environmental sounds\n",
    "def generate_mixed_sounds(animal_dir, env_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    animal_files = [f for f in os.listdir(animal_dir) if f.endswith('.wav')]\n",
    "    env_files = [f for f in os.listdir(env_dir) if f.endswith('.wav')]\n",
    "\n",
    "    for animal_file in tqdm(animal_files, desc=\"Generating mixed sounds\", unit=\"file\"):\n",
    "        try:\n",
    "            animal_path = os.path.join(animal_dir, animal_file)\n",
    "            env_file = np.random.choice(env_files)\n",
    "            env_path = os.path.join(env_dir, env_file)\n",
    "\n",
    "            y_animal, sr_animal = librosa.load(animal_path, sr=None)\n",
    "            y_env, sr_env = librosa.load(env_path, sr=None)\n",
    "\n",
    "            if sr_animal != sr_env:\n",
    "                y_env = librosa.resample(y_env, orig_sr=sr_env, target_sr=sr_animal)\n",
    "                sr_env = sr_animal\n",
    "\n",
    "            min_len = min(len(y_animal), len(y_env))\n",
    "            y_animal, y_env = y_animal[:min_len], y_env[:min_len]\n",
    "            y_mixed = normalize(y_animal + y_env)\n",
    "\n",
    "            output_path = os.path.join(output_dir, f\"{os.path.splitext(animal_file)[0]}_mixed.png\")\n",
    "            saved_spectrogram(y_mixed, sr_animal, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating mixed sound for {animal_file}: {e}\")\n",
    "\n",
    "\n",
    "# Distribute mixed spectrograms into Animal and Environment categories\n",
    "def mixed_spectrograms(mixed_dir, target_dirs):\n",
    "     # retrieve list of all mixed spectrogram files in the directory\n",
    "    mixed_files = [f for f in os.listdir(mixed_dir) if f.endswith('.png')]\n",
    "    # Iterate through each mixed spectrogram file\n",
    "    for file in mixed_files:\n",
    "        target_dir = np.random.choice(target_dirs)  # Randomly choose a target directory\n",
    "        # Construct full source and destination file paths\n",
    "        src = os.path.join(mixed_dir, file)\n",
    "        dest = os.path.join(target_dir, file)\n",
    "         # Copys the file from the mixed directory to the target directory\n",
    "        shutil.copy(src, dest)\n",
    "     \n",
    "\n",
    "\n",
    "# Process the animal, environmental, and mixed audio files to generate spectrograms\n",
    "processing_audio(animal_dir, animal_spectrogram_dir, augment)\n",
    "processing_audio(env_dir, env_spectrogram_dir, augment)\n",
    "generate_mixed_sounds(animal_dir, env_dir, mixed_spectrogram_dir)\n",
    "\n",
    "# Integrates the mixed spectrograms into the training and validation datasets\n",
    "mixed_spectrograms(\n",
    "    mixed_spectrogram_dir,\n",
    "    [directories[\"train_animal_dir\"], directories[\"train_env_dir\"]]\n",
    ")\n",
    "mixed_spectrograms(\n",
    "    mixed_spectrogram_dir,\n",
    "    [directories[\"val_animal_dir\"], directories[\"val_env_dir\"]]\n",
    ")\n",
    "\n",
    "# Split data into training and validation sets\n",
    "def split_data(input_dir, output_dirs, test_size=0.2):\n",
    "     # Retrieve the list of spectrogram files in the input directory\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "    train_files, val_files = train_test_split(files, test_size=test_size, random_state=42)\n",
    "\n",
    "    for files, out_dir in zip([train_files, val_files], output_dirs):\n",
    "        os.makedirs(out_dir, exist_ok=True) # Ensure the directory exists\n",
    "        for file in files:\n",
    "            src = os.path.join(input_dir, file)\n",
    "            dest = os.path.join(out_dir, file)\n",
    "            shutil.copy(src, dest)\n",
    "\n",
    "\n",
    "# Run the updated split_data function for animal and environmental spectrograms\n",
    "split_data(animal_spectrogram_dir, [directories[\"train_animal_dir\"], directories[\"val_animal_dir\"]])\n",
    "split_data(env_spectrogram_dir, [directories[\"train_env_dir\"], directories[\"val_env_dir\"]])\n",
    "\n",
    "# Data Generators\n",
    "# Training data generator with real-time data augmentation\n",
    "train_generator = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,  # Normalizes the pixel values to the range [0, 1]\n",
    "    rotation_range=45,  # Random rotation in the range [-30, 30] degrees\n",
    "    width_shift_range=0.5,  # Random horizontal shift by up to 30% of the image width\n",
    "    height_shift_range=0.5,  # Random vertical shift by up to 30% of the image height\n",
    "    zoom_range=0.5,  # Random zoom by up to 30%\n",
    "    horizontal_flip=True,  # Random horizontal flipping of images\n",
    "    vertical_flip=True,  # Vertical flipping\n",
    "    shear_range=0.2,  # Shear transformations\n",
    "    fill_mode='nearest'  # Fills empty pixels created by transformations\n",
    ").flow_from_directory(\n",
    "    train_dir,  # Directory containing training data\n",
    "    target_size=(128, 128),  # Resizes the images to 128x128\n",
    "    batch_size=16,  # Generates batches of 16 images\n",
    "    class_mode='categorical',  # Multi-class classification\n",
    "    classes=['Animal Sounds', 'Environment']  # Only two classes\n",
    ")\n",
    "\n",
    "# Validation data generator - only rescaling\n",
    "val_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0  # Normalizes pixel values to the range [0, 1]\n",
    ").flow_from_directory(\n",
    "    val_dir,  # Directory containing validation data\n",
    "    target_size=(128, 128),  # Resizes images to 128x128\n",
    "    batch_size=16,  # Generates batches of 16 images\n",
    "    class_mode='categorical',  # Multi-class classification\n",
    "    classes=['Animal Sounds', 'Environment']  # Only two classes\n",
    ")\n",
    "\n",
    "\n",
    "# CNN Model Definition\n",
    "def cnn_model(input_shape=(128, 128, 3), l2_strength=0.001, dropout_rate=0.4):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First convolutional block\n",
    "    x = Conv2D(32, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_strength))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    # Residual connection for the first block (apply MaxPooling2D to match spatial dimensions)\n",
    "    residual_1 = Conv2D(32, (1, 1), padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    residual_1 = MaxPooling2D((2, 2))(residual_1)\n",
    "    x = Add()([x, residual_1])\n",
    "\n",
    "    # Second convolutional block\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    # Residual connection for the second block\n",
    "    residual_2 = Conv2D(64, (1, 1), padding='same', kernel_initializer='he_normal')(residual_1)\n",
    "    residual_2 = MaxPooling2D((2, 2))(residual_2)\n",
    "    x = Add()([x, residual_2])\n",
    "\n",
    "    # Third convolutional block\n",
    "    x = Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    # Residual connection for the third block\n",
    "    residual_3 = Conv2D(128, (1, 1), padding='same', kernel_initializer='he_normal')(residual_2)\n",
    "    residual_3 = MaxPooling2D((2, 2))(residual_3)\n",
    "    x = Add()([x, residual_3])\n",
    "\n",
    "    # Fourth convolutional block\n",
    "    x = Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    # Residual connection for the fourth block\n",
    "    residual_4 = Conv2D(256, (1, 1), padding='same', kernel_initializer='he_normal')(residual_3)\n",
    "    residual_4 = MaxPooling2D((2, 2))(residual_4)\n",
    "    x = Add()([x, residual_4])\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Fully connected layer\n",
    "    x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=l2(l2_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(2, activation='softmax')(x)  # Adjust output to 2 classes (Animal, Environment)\n",
    "\n",
    "    # Model instantiation\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# CNN model\n",
    "model = cnn_model()\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Adam optimizer with low learning rate\n",
    "    loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
    "    metrics=['accuracy']  # Evaluation metric\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "stop_early = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,  # Training data generator\n",
    "    validation_data=val_gen,  # Validation data generator\n",
    "    epochs=20,  # Maximum number of epochs for training\n",
    "    callbacks=[stop_early]  # Early stopping callback\n",
    ")\n",
    "\n",
    "# generate classifcation rerport\n",
    "def gen_classification_report(model, data_generator):\n",
    "    # Reset the generator\n",
    "    data_generator.reset()\n",
    "    \n",
    "    # Predict on the data generator\n",
    "    print(\"Generating predictions...\")\n",
    "    y_pred_probs = model.predict(data_generator, verbose=1)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class indices\n",
    "    \n",
    "    # True labels\n",
    "    y_true = data_generator.classes\n",
    "    \n",
    "    # Class labels\n",
    "    class_labels = list(data_generator.class_indices.keys())\n",
    "    \n",
    "    # Generate classification report\n",
    "    print(\"Classification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    print(report)\n",
    "\n",
    "    # display confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_labels)\n",
    "    disp.plot(cmap='viridis', values_format='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "gen_classification_report(model, val_gen)\n",
    "\n",
    "\n",
    "# Path to save the model\n",
    "documents_path = os.path.expanduser(r\"C:\\Users\\riley\\Documents\")  \n",
    "model_save_path = os.path.join(documents_path, \"audio_model.h5\")\n",
    "\n",
    "# Save the trained model to the Documents directory\n",
    "model.save(model_save_path)  # Saves the model in HDF5 format\n",
    "print(f\"Model saved as '{model_save_path}'\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9425d889-e196-4000-bdb8-cbeaf45bcc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test audio files: 100%|██████████████████████████████████████████████████| 37/37 [00:00<00:00, 18538.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 199ms/step\n",
      "Incorrect: alligator hiss.png -> Predicted: Environment Sounds (0.85 confidence), Actual: Animal Sounds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Incorrect: baby pelican.png -> Predicted: Environment Sounds (0.67 confidence), Actual: Animal Sounds\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Incorrect: baby woodstorks.png -> Predicted: Environment Sounds (0.84 confidence), Actual: Animal Sounds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Incorrect: chirpy grey bird.png -> Predicted: Environment Sounds (0.89 confidence), Actual: Animal Sounds\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Incorrect: cribbler tree peeling.png -> Predicted: Animal Sounds (0.63 confidence), Actual: Environment Sounds\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Correct: daintree bat.png -> Animal Sounds (0.94 confidence)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Incorrect: falling branches.png -> Predicted: Animal Sounds (0.98 confidence), Actual: Environment Sounds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Correct: Giant Banjo Frogs.png -> Animal Sounds (0.78 confidence)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Incorrect: Glider Squirrel.png -> Predicted: Environment Sounds (0.80 confidence), Actual: Animal Sounds\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Correct: Golden Bowerbird.png -> Animal Sounds (0.72 confidence)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Correct: goldfinch.png -> Animal Sounds (0.66 confidence)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Correct: heavy hailstorm with thunder.png -> Environment Sounds (0.98 confidence)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Correct: humming bird.png -> Animal Sounds (0.69 confidence)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Incorrect: ice and water sounds.png -> Predicted: Animal Sounds (0.87 confidence), Actual: Environment Sounds\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Correct: indigobunting.png -> Animal Sounds (0.76 confidence)\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Correct: Light rain.png -> Environment Sounds (0.78 confidence)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Correct: little blueheron fishes.png -> Animal Sounds (0.78 confidence)\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Correct: mud squelching.png -> Environment Sounds (0.90 confidence)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Correct: night Frog.png -> Animal Sounds (0.57 confidence)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Correct: pademelon.png -> Animal Sounds (0.69 confidence)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Incorrect: Puerto Ricoterns.png -> Predicted: Environment Sounds (0.85 confidence), Actual: Animal Sounds\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Correct: rain hitting puddle.png -> Environment Sounds (1.00 confidence)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Correct: rain on wood.png -> Environment Sounds (0.93 confidence)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Correct: red wing black bird.png -> Animal Sounds (0.70 confidence)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Correct: river and stones.png -> Environment Sounds (0.92 confidence)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Correct: river-babbling.png -> Environment Sounds (0.92 confidence)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Correct: samp at night.png -> Environment Sounds (0.89 confidence)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Incorrect: shaking branch.png -> Predicted: Animal Sounds (0.88 confidence), Actual: Environment Sounds\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Incorrect: snow falling.png -> Predicted: Animal Sounds (0.76 confidence), Actual: Environment Sounds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Ground truth not found for sofsnow geese.png. Skipping.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Correct: storm wind in trees.png -> Environment Sounds (0.84 confidence)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Correct: Thunder.png -> Environment Sounds (0.65 confidence)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Ground truth not found for tundraswans.png. Skipping.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Correct: walking in the mud.png -> Environment Sounds (0.83 confidence)\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Correct: walking on log shells.png -> Environment Sounds (0.95 confidence)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Correct: water stream.png -> Environment Sounds (0.95 confidence)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Incorrect: whistling air.png -> Predicted: Animal Sounds (0.99 confidence), Actual: Environment Sounds\n",
      "\n",
      "Accuracy: 65.71%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#load the trained model\n",
    "def load_trained_model(model_path):\n",
    "\n",
    "    model = load_model(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "#generate spectrograms from audio files\n",
    "def test_spectrograms(input_folder, output_folder):\n",
    "    def save_spectrogram(y, sr, output_path):\n",
    "        plt.figure(figsize=(2, 2)) #figure size \n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128) #Compute mel spectrogram\n",
    "        S_DB = librosa.power_to_db(S, ref=np.max) #decibel scale\n",
    "        librosa.display.specshow(S_DB, sr=sr, cmap='inferno') #Display \n",
    "        plt.axis('off') #Hide axes \n",
    "        plt.savefig(output_path, bbox_inches='tight', pad_inches=0) # Save spectrogram\n",
    "        plt.close() # Close the plot \n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True) # Ensure the output folder exists\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.wav')] # Get all .wav files\n",
    "\n",
    "    for file in tqdm(files, desc=\"Processing test audio files\"): # Show progress\n",
    "        file_path = os.path.join(input_folder, file)   # Load audio file\n",
    "        output_path = os.path.join(output_folder, f\"{os.path.splitext(file)[0]}.png\")\n",
    "        if not os.path.exists(output_path): # Skip if the spectrogram already exists\n",
    "            try:\n",
    "                y, sr = librosa.load(file_path, sr=None) \n",
    "                save_spectrogram(y, sr, output_path) # Generate and save spectrogram\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")  # Handle errors\n",
    "\n",
    "# predict classes based on spectrograms\n",
    "def predict(model, spectrogram_folder, ground_truth_csv, class_names):\n",
    "    \n",
    "    def load_spectrogram(file_path):\n",
    "        img = load_img(file_path, target_size=(128, 128)) # Resize the image\n",
    "        img_array = img_to_array(img) / 255.0 # Normalize pixel values\n",
    "        return np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    test_files = [f for f in os.listdir(spectrogram_folder) if f.endswith('.png')]  # Get all spectrogram images\n",
    "    ground_truth = pd.read_csv(ground_truth_csv) # Load ground truth labels\n",
    "\n",
    "\n",
    "    actual_labels = [] #store class labels\n",
    "    predicted_labels = [] #predcit class labels \n",
    "\n",
    "    for file in test_files:\n",
    "        file_path = os.path.join(spectrogram_folder, file)\n",
    "        try:\n",
    "            spectrogram = load_spectrogram(file_path)  # Load and preprocess spectrogram\n",
    "            prediction = model.predict(spectrogram) # Predicts class probabilities\n",
    "            predicted_class = np.argmax(prediction, axis=1)[0] # predicted class index\n",
    "            confidence = np.max(prediction) #confidence score\n",
    "\n",
    "            ground_truth_row = ground_truth[ground_truth['file_name'] == file] # Finds ground truth label\n",
    "            if not ground_truth_row.empty:\n",
    "                actual_class = ground_truth_row['ground_truth'].values[0]\n",
    "                actual_labels.append(actual_class)\n",
    "                predicted_labels.append(class_names[predicted_class])\n",
    "\n",
    "                if class_names[predicted_class] == actual_class:\n",
    "                    print(f\"Correct: {file} -> {class_names[predicted_class]} ({confidence:.2f} confidence)\")\n",
    "                else:\n",
    "                    print(f\"Incorrect: {file} -> Predicted: {class_names[predicted_class]} ({confidence:.2f} confidence), \"\n",
    "                          f\"Actual: {actual_class}\")\n",
    "            else:\n",
    "                print(f\"Ground truth not found for {file}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting {file}: {e}\")\n",
    "            \n",
    "     # Calculate and display accuracy \n",
    "    if actual_labels:\n",
    "        accuracy = accuracy_score(actual_labels, predicted_labels) * 100\n",
    "        print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(\"Ground truth not available!\")\n",
    "\n",
    "\n",
    "\n",
    "# Paths\n",
    "test_audio_folder = r\"C:\\Users\\riley\\Documents\\Test Sounds\"  # Path to test audio files\n",
    "spectrogram_folder = r\"C:\\Users\\riley\\Documents\\Spectrograms\\test\"  # Path to save spectrograms\n",
    "model_path = r\"C:\\Users\\riley\\Documents\\audio_model.h5\"  # Path to the trained model\n",
    "ground_truth_csv = r\"C:\\Users\\riley\\Documents\\ground_truth_1.csv\"  # Path to ground truth CSV\n",
    "class_names = {0: \"Animal Sounds\", 1: \"Environment Sounds\"}  # Class index-to-name mapping\n",
    "\n",
    "# Load the trained model\n",
    "model = load_trained_model(model_path)\n",
    "\n",
    "# Generate spectrograms from test audio files\n",
    "test_spectrograms(test_audio_folder, spectrogram_folder)\n",
    "\n",
    "# Predict and evaluate\n",
    "predict(model, spectrogram_folder, ground_truth_csv, class_names)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlows",
   "language": "python",
   "name": "tensorflows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
