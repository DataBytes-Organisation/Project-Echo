[<- Back to Main Guide](model_integration_guide.md)

---

## What is Preprocessing and Why is it Necessary?

The classifier, a Convolutional Neural Network (CNN), is designed to process 2D data structures, similar to images. Raw audio, being a 1D waveform, requires transformation. The goal of preprocessing is to convert the 1D sound wave into a 2D, image-like format that a CNN can effectively analyse. This format is the Mel Spectrogram.

The creation process is as follows:

1.  **Create a Spectrogram (The "Audio Image")**:
    *   A spectrogram decomposes the sound into its constituent frequencies over time. This is analogous to creating an audio image from an audio file, where the image shows which notes (frequencies) are played at what times and with what intensity (loudness). The result is a chart with time on one axis, frequency on the other, and the colour/brightness of each point showing the loudness of that frequency at that moment.

2.  **Apply the "Mel" Scale**:
    *   Humans perceive sound frequencies non-linearly; we are more sensitive to changes in low frequencies than high ones.
    *   The **Mel scale** reorganises the spectrogram's frequency axis to mimic this perception. It allocates more resolution to lower frequencies and compresses higher frequencies. This helps the model focus on the frequency ranges most critical for distinguishing real-world sounds.

3.  **Use Decibels & Normalisation (Standardise the "Image")**:
    *   Loudness values are converted to a logarithmic scale (decibels), which aligns better with human hearing.
    *   Finally, all values are normalised to a standard range (e.g., 0 to 1). This is like adjusting the brightness and contrast of a photo, ensuring the final "image" is clear and consistent for the model, regardless of the original recording's volume.

In essence, preprocessing transforms a 1D sound wave into a standardised 2D "image" (the Mel Spectrogram) optimised for both human auditory perception and the pattern-recognition capabilities of a CNN.

This project provides two distinct methods for performing this transformation, each tailored to a specific deployment pipeline.

---

## Preprocessing Pipelines

### 1. General Backend: The TorchScript Preprocessor (`preprocessor.pt`)

For server environments where PyTorch is available, the preprocessing logic is encapsulated within a dedicated TorchScript model: `preprocessor.pt`.

-   **How it works**: This model takes a raw 1D audio waveform as a PyTorch tensor and uses the `torchaudio` library internally to perform all the steps described above (STFT, Mel scale conversion, dB conversion, and normalization).
-   **Output**: It outputs a 2D Mel Spectrogram as a PyTorch tensor, ready to be fed directly into the main classifier model (`model.pt`).
-   **Advantage**: It keeps the entire pipeline within the PyTorch ecosystem, simplifying dependencies.

### 2. IoT/Embedded: The `librosa`-based Python Function

For resource-constrained devices like a Raspberry Pi or any environment where PyTorch is not available, a `torch`-free approach is used. This is the pipeline used with the TFLite model.

-   **How it works**: Instead of a model file, the preprocessing is a pure Python function that uses the `librosa` and `numpy` libraries. It relies on a helper file, `preprocess.npy`.
-   **The `preprocess.npy` file**: This crucial file is a dictionary containing all the parameters needed to perfectly replicate the preprocessing used during training. This includes:
    -   Configuration values (sample rate, `n_fft`, `hop_length`, etc.).
    -   The exact Mel filter bank matrix that was generated by `torchaudio` during training. By using the same matrix, we ensure the `librosa`-based function produces a spectrogram that is numerically identical to the `torchaudio` version.
-   **Output**: It outputs a 2D Mel Spectrogram as a NumPy array, which is the expected input format for the `.tflite` classification model.
-   **Advantage**: It removes the need for PyTorch, resulting in a much lighter-weight deployment package. The `demo_iot.py` and `validate_models.py` scripts contain example implementations of this function.

Both pipelines are designed to be functionally identical, ensuring that the model's performance is consistent regardless of the deployment environment. The `deploy_model.py` script automatically generates all the necessary artifacts for both pipelines (`preprocessor.pt`, `preprocess.npy`, etc.) from a single trained model.

---
[<- Back to Main Guide](model_integration_guide.md)
