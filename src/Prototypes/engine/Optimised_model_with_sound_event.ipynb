{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Audio-Based Animal Classification and Event Detection\n",
    "\n",
    "*Author: Rohit and Andrew*\n",
    "\n",
    "### Overview\n",
    "\n",
    "This code comprises two primary sections. The first section is a pre-trained model(optmiised model) for predicting animal classes using audio files, while the second utilizes the YAMNet model to detect events in an audio file.\n",
    "\n",
    "### Components\n",
    "\n",
    "#### Configuration Parameters\n",
    "\n",
    "A dictionary, `config`, houses key settings and parameters that are essential for audio processing and the generation of mel-spectrogram images. It includes details like the audio sample rate, clip duration, NFFT settings, mel-spectrogram settings, and model input configurations.\n",
    "\n",
    "#### Animal Classes\n",
    "\n",
    "A list, `class_names`, contains the names of the animals that the pre-trained model can predict.\n",
    "\n",
    "#### Model Loading\n",
    "\n",
    "The model is loaded from a pre-defined path (`models/echo_model/1/`). This model is used to predict specific animal sounds from a given audio clip.\n",
    "\n",
    "#### Audio Processing and Prediction Functions\n",
    "\n",
    "Several helper functions preprocess the audio clips:\n",
    "\n",
    "- `combined_pipeline()`: Processes an audio clip and returns the mel-spectrogram image.\n",
    "- `predict_class()`: Takes in model predictions and returns the predicted class name and the probability.\n",
    "- `load_random_subsection()`: Retrieves a random subsection of an audio clip.\n",
    "- `predict_on_audio()`: Combines the above functions to predict an animal class given an audio binary.\n",
    "\n",
    "#### YAMNet Event Detection\n",
    "\n",
    "The YAMNet model is loaded and used to detect events in audio files. The main aim is to detect the 'Animal' event. This section uses the following main components:\n",
    "\n",
    "- `yamnet_frames_model`: A YAMNet model designed to predict events from frames of audio data.\n",
    "- `yamnet_classes`: A list of class names that YAMNet can predict.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. The code starts by setting up the animal prediction model.\n",
    "2. An audio file (`test.m4a`) is loaded and divided into 1-second chunks.\n",
    "3. YAMNet is then used to detect the 'Animal' event in these chunks. Intervals containing the 'Animal' event are stored.\n",
    "4. For each interval detected by YAMNet, the specific animal sound is predicted using the pre-trained model.\n",
    "5. The start and end times, YAMNet's prediction, YAMNet's probability, the specific animal prediction, and its associated probability are all logged in a DataFrame (`df`).\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Redundant code: A function, `predict_class`, is defined twice. Ensure to remove the redundant definition.\n",
    "- Unused functions: `audio_to_string` and `string_to_audio` are defined but not used in the provided context.\n",
    "- File paths: Ensure that paths to YAMNet weights and class maps are correct.\n",
    "- Image shapes: Ensure that the mel-spectrogram image shape aligns with the model's expectations.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The code provides a comprehensive way of detecting generic animal sounds using YAMNet and then refines the detection by predicting the specific animal type using a custom model.\n",
    "\n",
    "--- \n",
    "\n",
    "This markdown provides a thorough walkthrough of your code. Adjustments can be made based on specific requirements or additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "\n",
    "\n",
    "# Make sure to use the correct configuration\n",
    "config = {\n",
    "    'AUDIO_SAMPLE_RATE': 48000,\n",
    "    'AUDIO_CLIP_DURATION': 5,\n",
    "    'AUDIO_NFFT': 2048,\n",
    "    'AUDIO_STRIDE': 200,\n",
    "    'AUDIO_MELS': 260,\n",
    "    'AUDIO_FMIN': 20,\n",
    "    'AUDIO_FMAX': 13000,\n",
    "    'AUDIO_WINDOW': None,\n",
    "    'AUDIO_TOP_DB': 80,\n",
    "    'MODEL_INPUT_IMAGE_CHANNELS': 3,\n",
    "    'MODEL_INPUT_IMAGE_WIDTH': 260,\n",
    "    'MODEL_INPUT_IMAGE_HEIGHT': 260\n",
    "}\n",
    "\n",
    "\n",
    "class_names= ['Aegotheles Cristatus', 'Alauda Arvensis', 'Caligavis Chrysops', 'Capra Hircus', 'Cervus Unicolour', 'Colluricincla Harmonica', 'Corvus Coronoides',\n",
    "              'Dama Dama', 'Eopsaltria Australis', 'Felis Catus', 'Pachycephala Rufiventris', 'Ptilotula Penicillata', 'Rattus Norvegicus', 'Strepera Graculina', 'Sus Scrofa']\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('models/echo_model/1/')\n",
    "\n",
    "# Define the preprocessing steps as functions.\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "    # this function is adapted from generic_engine_pipeline.ipynb\n",
    "    # TODO: need to create a pipeline library and link same code into engine\n",
    "    ########################################################################################\n",
    "def combined_pipeline(config, audio_clip):\n",
    "    # Create a file-like object from the bytes.\n",
    "    #file = io.BytesIO(audio_clip)\n",
    "    \n",
    "\n",
    "    # Load the audio data with librosa\n",
    "    audio_clip, sample_rate = librosa.load(audio_clip, sr=config['AUDIO_SAMPLE_RATE'])\n",
    "        \n",
    "    # keep right channel only\n",
    "    if audio_clip.ndim == 2 and audio_clip.shape[0] == 2:\n",
    "        audio_clip = audio_clip[1, :]\n",
    "        \n",
    "    # cast to float32 type\n",
    "    audio_clip = audio_clip.astype(np.float32)\n",
    "        \n",
    "    # analyse a random 5 second subsection\n",
    "    audio_clip = load_random_subsection(audio_clip, duration_secs=config['AUDIO_CLIP_DURATION'])\n",
    "\n",
    "    # Compute the mel-spectrogram\n",
    "    image = librosa.feature.melspectrogram(\n",
    "        y=audio_clip, \n",
    "        sr=config['AUDIO_SAMPLE_RATE'], \n",
    "        n_fft=config['AUDIO_NFFT'], \n",
    "        hop_length=config['AUDIO_STRIDE'], \n",
    "        n_mels=config['AUDIO_MELS'],\n",
    "        fmin=config['AUDIO_FMIN'],\n",
    "        fmax=config['AUDIO_FMAX'],\n",
    "        win_length=config['AUDIO_WINDOW'])\n",
    "\n",
    "    # Optionally convert the mel-spectrogram to decibel scale\n",
    "    image = librosa.power_to_db(\n",
    "        image, \n",
    "        top_db=config['AUDIO_TOP_DB'], \n",
    "        ref=1.0)\n",
    "        \n",
    "    # Calculate the expected number of samples in a clip\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "        \n",
    "    # swap axis and clip to expected samples to avoid rounding errors\n",
    "    image = np.moveaxis(image, 1, 0)\n",
    "    image = image[0:expected_clip_samples,:]\n",
    "        \n",
    "    # reshape into standard 3 channels to add the color channel\n",
    "    image = tf.expand_dims(image, -1)\n",
    "        \n",
    "    # most pre-trained model classifer model expects 3 color channels\n",
    "    image = tf.repeat(image, config['MODEL_INPUT_IMAGE_CHANNELS'], axis=2)\n",
    "        \n",
    "    # calculate the image shape and ensure it is correct\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "    image = tf.ensure_shape(image, [expected_clip_samples, config['AUDIO_MELS'], config['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "        \n",
    "    # note here a high quality LANCZOS5 is applied to resize the image to match model image input size\n",
    "    image = tf.image.resize(image, (config['MODEL_INPUT_IMAGE_WIDTH'], config['MODEL_INPUT_IMAGE_HEIGHT']), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "\n",
    "\n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.0000001)\n",
    "        \n",
    "    return image, audio_clip, sample_rate\n",
    "\n",
    "\n",
    "\n",
    " ########################################################################################\n",
    "    # Function to predict class and probability given a prediction\n",
    "    ########################################################################################\n",
    "def predict_class( predictions):\n",
    "    # Get the index of the class with the highest predicted probability\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    print(predicted_index, type(predicted_index))\n",
    "\n",
    "    # Get the class name using the predicted index\n",
    "    predicted_class = self.class_names[predicted_index]\n",
    "    # Calculate the predicted probability for the selected class\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[predicted_index].numpy()\n",
    "    # Round the probability to 2 decimal places\n",
    "    predicted_probability = round(predicted_probability, 2)\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "# this method takes in binary audio data and encodes to string\n",
    "def audio_to_string( audio_binary):\n",
    "    base64_encoded_data = base64.b64encode(audio_binary)\n",
    "    base64_message = base64_encoded_data.decode('utf-8')\n",
    "    return base64_message    \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "    # this method takes in string and ecodes to audio binary data\n",
    "    ########################################################################################\n",
    "def string_to_audio( audio_string):\n",
    "    base64_img_bytes = audio_string.encode('utf-8')\n",
    "    decoded_data = base64.decodebytes(base64_img_bytes)\n",
    "    return decoded_data\n",
    "    \n",
    "def predict_class(predictions):\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    predicted_class = class_names[predicted_index]\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[0, predicted_index].numpy()\n",
    "    predicted_probability = round(predicted_probability, 2)\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "\n",
    "def load_random_subsection(audio_clip, duration_secs):\n",
    "    clip_length = len(audio_clip)\n",
    "    subsection_length = duration_secs * config['AUDIO_SAMPLE_RATE']\n",
    "    if clip_length > subsection_length:\n",
    "        start_idx = np.random.randint(0, clip_length - subsection_length)\n",
    "        return audio_clip[start_idx:start_idx+subsection_length]\n",
    "    else:\n",
    "        return audio_clip\n",
    "\n",
    "\n",
    "def predict_on_audio(audio_binary):\n",
    "    # Preprocess the audio to be suitable for your model\n",
    "    image, audio_clip, sample_rate = combined_pipeline(config, audio_binary)\n",
    "    \n",
    "    # Add a dimension to match the model's input shape\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    # Make the prediction\n",
    "    predictions = model.predict(image)\n",
    "    print(predictions.shape, predictions)\n",
    "\n",
    "    \n",
    "    # Predict class and probability using the prediction function\n",
    "    predicted_class, predicted_probability = predict_class(predictions)\n",
    "    \n",
    "    print(f'Predicted class: {predicted_class}')\n",
    "    print(f'Predicted probability: {predicted_probability}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use predict_on_audio function to predict on your audio binary data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/98811489.py:19: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data, sr = librosa.load(filename, sr=params.SAMPLE_RATE)\n",
      "/Users/ankush/anaconda3/envs/new_env/lib/python3.8/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shape of tensor input [101,64,3] is not compatible with expected shape [500,64,3]. [Op:EnsureShape] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m sf\u001b[39m.\u001b[39mwrite(temp_audio_file\u001b[39m.\u001b[39mname, segment_data, params\u001b[39m.\u001b[39mSAMPLE_RATE)\n\u001b[1;32m     55\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(temp_audio_file\u001b[39m.\u001b[39mname, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m binary_file:\n\u001b[0;32m---> 56\u001b[0m     image, _, _ \u001b[39m=\u001b[39m combined_pipeline(config, binary_file\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m     57\u001b[0m     \u001b[39m# Check the image shape and adjust if necessary\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39m500\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m):\n",
      "Cell \u001b[0;32mIn[8], line 100\u001b[0m, in \u001b[0;36mcombined_pipeline\u001b[0;34m(config, audio_clip)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m# calculate the image shape and ensure it is correct\u001b[39;00m\n\u001b[1;32m     99\u001b[0m expected_clip_samples \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mAUDIO_CLIP_DURATION\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mAUDIO_SAMPLE_RATE\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mAUDIO_STRIDE\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 100\u001b[0m image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mensure_shape(image, [expected_clip_samples, config[\u001b[39m'\u001b[39;49m\u001b[39mAUDIO_MELS\u001b[39;49m\u001b[39m'\u001b[39;49m], config[\u001b[39m'\u001b[39;49m\u001b[39mMODEL_INPUT_IMAGE_CHANNELS\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n\u001b[1;32m    102\u001b[0m \u001b[39m# note here a high quality LANCZOS5 is applied to resize the image to match model image input size\u001b[39;00m\n\u001b[1;32m    103\u001b[0m image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize(image, (config[\u001b[39m'\u001b[39m\u001b[39mMODEL_INPUT_IMAGE_WIDTH\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39mMODEL_INPUT_IMAGE_HEIGHT\u001b[39m\u001b[39m'\u001b[39m]), \n\u001b[1;32m    104\u001b[0m                         method\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mResizeMethod\u001b[39m.\u001b[39mLANCZOS5)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shape of tensor input [101,64,3] is not compatible with expected shape [500,64,3]. [Op:EnsureShape] name: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import yamnet.params as params\n",
    "import yamnet.yamnet as yamnet_model\n",
    "import librosa\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "# Load YAMNet model\n",
    "yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet.load_weights('yamnet/yamnet.h5')\n",
    "yamnet_classes = yamnet_model.class_names('yamnet/yamnet_class_map.csv')\n",
    "\n",
    "frame_len = int(params.SAMPLE_RATE * 1)  # 1sec\n",
    "# Read the whole audio file\n",
    "filename = 'test.m4a'\n",
    "data, sr = librosa.load(filename, sr=params.SAMPLE_RATE)\n",
    "\n",
    "# Split the audio data into 1 second chunks\n",
    "chunks = np.array_split(data, len(data) // frame_len)\n",
    "\n",
    "intervals = []\n",
    "current_interval = None\n",
    "\n",
    "for cnt, frame_data in enumerate(chunks):\n",
    "    start_time = cnt\n",
    "    end_time = cnt + 1\n",
    "    scores, _ = yamnet.predict(np.reshape(frame_data, [1, -1]), steps=1)\n",
    "    yamnet_prediction = np.mean(scores, axis=0)\n",
    "    top5_i = np.argsort(yamnet_prediction)[::-1][:5]\n",
    "\n",
    "    if yamnet_classes[top5_i[0]] == 'Animal' and yamnet_prediction[top5_i[0]] > 0.2:\n",
    "        if current_interval is None:\n",
    "            current_interval = {'start': cnt, 'end': cnt+1}\n",
    "        else:\n",
    "            current_interval['end'] = cnt+1\n",
    "    else:\n",
    "        if current_interval:\n",
    "            intervals.append(current_interval)\n",
    "            current_interval = None\n",
    "\n",
    "if current_interval:\n",
    "    intervals.append(current_interval)\n",
    "\n",
    "df = pd.DataFrame(columns=['start_time', 'end_time', 'yamnet_label', 'yamnet_probability', 'your_model_label', 'your_model_probability'])\n",
    "\n",
    "for interval in intervals:  \n",
    "    segment_data = data[interval['start']*frame_len : interval['end']*frame_len]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_audio_file:\n",
    "        sf.write(temp_audio_file.name, segment_data, params.SAMPLE_RATE)\n",
    "        \n",
    "        with open(temp_audio_file.name, 'rb') as binary_file:\n",
    "            image, _, _ = combined_pipeline(config, binary_file.read())\n",
    "            # Check the image shape and adjust if necessary\n",
    "            if image.shape != (500, 64, 3):\n",
    "                image = pad_tensor_to_shape(image, (500, 64, 3))\n",
    "            your_model_prediction, your_model_probability = predict_on_audio(binary_file.read())\n",
    "            \n",
    "        df = df.append({\n",
    "            'start_time': interval['start'],\n",
    "            'end_time': interval['end'],\n",
    "            'yamnet_label': 'Animal',\n",
    "            'yamnet_probability': np.mean(yamnet_prediction[top5_i]),\n",
    "            'your_model_label': your_model_prediction,\n",
    "            'your_model_probability': your_model_probability\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>yamnet_label</th>\n",
       "      <th>yamnet_probability</th>\n",
       "      <th>your_model_label</th>\n",
       "      <th>your_model_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start_time, end_time, yamnet_label, yamnet_probability, your_model_label, your_model_probability]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "\n",
    "\n",
    "# Make sure to use the correct configuration\n",
    "config = {\n",
    "    'AUDIO_SAMPLE_RATE': 48000,\n",
    "    'AUDIO_CLIP_DURATION': 5,\n",
    "    'AUDIO_NFFT': 2048,\n",
    "    'AUDIO_STRIDE': 200,\n",
    "    'AUDIO_MELS': 260,\n",
    "    'AUDIO_FMIN': 20,\n",
    "    'AUDIO_FMAX': 13000,\n",
    "    'AUDIO_WINDOW': None,\n",
    "    'AUDIO_TOP_DB': 80,\n",
    "    'MODEL_INPUT_IMAGE_CHANNELS': 3,\n",
    "    'MODEL_INPUT_IMAGE_WIDTH': 260,\n",
    "    'MODEL_INPUT_IMAGE_HEIGHT': 260\n",
    "}\n",
    "\n",
    "\n",
    "class_names= ['Aegotheles cristatus owlet-nightjar', 'Alauda arvensis European Skylark', 'Caligavis chrysops Yellow-faced honeyeater', 'Capra hircus Feral goat', 'Cervus unicolour Sambar deer', 'Colluricincla harmonica Grey shrikethrush', 'Corvus coronoides Australian raven',\n",
    "              'Dama dama Fallow Deer', 'Eopsaltria australis Eastern yellow robin', 'Felis Catus Cat', 'Pachycephala rufiventris Rufous whistler', 'Ptilotula penicillata White-plumed honeyeater', 'Rattus norvegicus Brown rat', 'Strepera graculina Pied currawong', 'sus scrofa Wild pig']\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('models/echo_model/1/')\n",
    "\n",
    "# Define the preprocessing steps as functions.\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "    # this function is adapted from generic_engine_pipeline.ipynb\n",
    "    # TODO: need to create a pipeline library and link same code into engine\n",
    "    ########################################################################################\n",
    "def combined_pipeline(config, audio_clip):\n",
    "\n",
    "    # Load the audio data with librosa(works only while give direct audio to it)\n",
    "    #audio_clip, sample_rate = librosa.load(audio_clip, sr=config['AUDIO_SAMPLE_RATE'])\n",
    "    \n",
    "    #to use it with yamnet\n",
    "    file = io.BytesIO(audio_clip)\n",
    "    audio_clip, sample_rate = librosa.load(file, sr=config['AUDIO_SAMPLE_RATE'])\n",
    "        \n",
    "    # keep right channel only\n",
    "    if audio_clip.ndim == 2 and audio_clip.shape[0] == 2:\n",
    "        audio_clip = audio_clip[1, :]\n",
    "        \n",
    "    # cast to float32 type\n",
    "    audio_clip = audio_clip.astype(np.float32)\n",
    "        \n",
    "    # analyse a random 5 second subsection\n",
    "    audio_clip = load_random_subsection(audio_clip, duration_secs=config['AUDIO_CLIP_DURATION'])\n",
    "\n",
    "    # Compute the mel-spectrogram\n",
    "    image = librosa.feature.melspectrogram(\n",
    "        y=audio_clip, \n",
    "        sr=config['AUDIO_SAMPLE_RATE'], \n",
    "        n_fft=config['AUDIO_NFFT'], \n",
    "        hop_length=config['AUDIO_STRIDE'], \n",
    "        n_mels=config['AUDIO_MELS'],\n",
    "        fmin=config['AUDIO_FMIN'],\n",
    "        fmax=config['AUDIO_FMAX'],\n",
    "        win_length=config['AUDIO_WINDOW'])\n",
    "\n",
    "    # Optionally convert the mel-spectrogram to decibel scale\n",
    "    image = librosa.power_to_db(\n",
    "        image, \n",
    "        top_db=config['AUDIO_TOP_DB'], \n",
    "        ref=1.0)\n",
    "        \n",
    "    # Calculate the expected number of samples in a clip\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "        \n",
    "    # swap axis and clip to expected samples to avoid rounding errors\n",
    "    image = np.moveaxis(image, 1, 0)\n",
    "    image = image[0:expected_clip_samples,:]\n",
    "        \n",
    "    # reshape into standard 3 channels to add the color channel\n",
    "    image = tf.expand_dims(image, -1)\n",
    "        \n",
    "    # most pre-trained model classifer model expects 3 color channels\n",
    "    image = tf.repeat(image, config['MODEL_INPUT_IMAGE_CHANNELS'], axis=2)\n",
    "        \n",
    "    # calculate the image shape and ensure it is correct\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "    image = tf.ensure_shape(image, [expected_clip_samples, config['AUDIO_MELS'], config['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "        \n",
    "    # note here a high quality LANCZOS5 is applied to resize the image to match model image input size\n",
    "    image = tf.image.resize(image, (config['MODEL_INPUT_IMAGE_WIDTH'], config['MODEL_INPUT_IMAGE_HEIGHT']), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "\n",
    "\n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.0000001)\n",
    "        \n",
    "    return image, audio_clip, sample_rate\n",
    "\n",
    "\n",
    "\n",
    " ########################################################################################\n",
    "    # Function to predict class and probability given a prediction\n",
    "    ########################################################################################\n",
    "def predict_class( predictions):\n",
    "    # Get the index of the class with the highest predicted probability\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    print(predicted_index, type(predicted_index))\n",
    "\n",
    "    # Get the class name using the predicted index\n",
    "    predicted_class = self.class_names[predicted_index]\n",
    "    # Calculate the predicted probability for the selected class\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[predicted_index].numpy()\n",
    "    # Round the probability to 2 decimal places\n",
    "    predicted_probability = round(predicted_probability, 2)\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "# this method takes in binary audio data and encodes to string\n",
    "def audio_to_string( audio_binary):\n",
    "    base64_encoded_data = base64.b64encode(audio_binary)\n",
    "    base64_message = base64_encoded_data.decode('utf-8')\n",
    "    return base64_message    \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "    # this method takes in string and ecodes to audio binary data\n",
    "    ########################################################################################\n",
    "def string_to_audio( audio_string):\n",
    "    base64_img_bytes = audio_string.encode('utf-8')\n",
    "    decoded_data = base64.decodebytes(base64_img_bytes)\n",
    "    return decoded_data\n",
    "    \n",
    "def predict_class(predictions):\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    predicted_class = class_names[predicted_index]\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[0, predicted_index].numpy()\n",
    "    predicted_probability = round(predicted_probability, 2)\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "\n",
    "\n",
    "def load_random_subsection(audio_clip, duration_secs):\n",
    "    clip_length = len(audio_clip)\n",
    "    subsection_length = duration_secs * config['AUDIO_SAMPLE_RATE']\n",
    "    \n",
    "    if clip_length > subsection_length:\n",
    "        start_idx = np.random.randint(0, clip_length - subsection_length)\n",
    "        return audio_clip[start_idx:start_idx+subsection_length]\n",
    "    elif clip_length < subsection_length:\n",
    "        padding = np.zeros(int(subsection_length - clip_length))\n",
    "        return np.concatenate((audio_clip, padding))\n",
    "    else:\n",
    "        return audio_clip\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "#tis is standartd , works with audio more then 5 sec \n",
    "\n",
    "def load_random_subsection(audio_clip, duration_secs):\n",
    "    clip_length = len(audio_clip)\n",
    "    subsection_length = duration_secs * config['AUDIO_SAMPLE_RATE']\n",
    "    if clip_length > subsection_length:\n",
    "        start_idx = np.random.randint(0, clip_length - subsection_length)\n",
    "        return audio_clip[start_idx:start_idx+subsection_length]\n",
    "    else:\n",
    "        return audio_clip\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#works with when you directly give audio to predicyt \n",
    "\n",
    "\n",
    "def predict_on_audio(audio_binary):\n",
    "    # Preprocess the audio to be suitable for your model\n",
    "    image, audio_clip, sample_rate = combined_pipeline(config, audio_binary)\n",
    "    \n",
    "    # Add a dimension to match the model's input shape\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    # Make the prediction\n",
    "    predictions = model.predict(image)\n",
    "    print(predictions.shape, predictions)\n",
    "\n",
    "    \n",
    "    # Predict class and probability using the prediction function\n",
    "    predicted_class, predicted_probability = predict_class(predictions)\n",
    "    \n",
    "    #print(f'Predicted class: {predicted_class}')\n",
    "    #print(f'Predicted probability: {predicted_probability}')\n",
    "    # Return the results\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "\n",
    "def predict_on_audio(audio_binary):\n",
    "    # Preprocess the audio to be suitable for your model\n",
    "    image, audio_clip, sample_rate = combined_pipeline(config, audio_binary)\n",
    "    \n",
    "    # Add a dimension to match the model's input shape\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    # Make the prediction\n",
    "    predictions_array = model.predict(image)[0]  # Assuming the model returns 2D array, take the first element\n",
    "    \n",
    "    # Pair the class names with the predictions\n",
    "    paired_predictions = list(zip(class_names, predictions_array))\n",
    "    \n",
    "    # Sort the paired predictions based on probability\n",
    "    sorted_predictions = sorted(paired_predictions, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_predictions[:3]\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use predict_on_audio function to predict on your audio binary data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_audio(audio_binary):\n",
    "    # Preprocess the audio to be suitable for your model\n",
    "    image, audio_clip, sample_rate = combined_pipeline(config, audio_binary)\n",
    "    \n",
    "    # Add a dimension to match the model's input shape\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    # Make the prediction\n",
    "    predictions_array = model.predict(image)[0]  # Assuming the model returns 2D array, take the first element\n",
    "    \n",
    "    # Pair the class names with the predictions\n",
    "    paired_predictions = list(zip(class_names, predictions_array))\n",
    "    \n",
    "    # Sort the paired predictions based on probability\n",
    "    sorted_predictions = sorted(paired_predictions, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_predictions[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shape of tensor input [540,260,3] is not compatible with expected shape [1200,260,3]. [Op:EnsureShape] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Use the function with the binary data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predict_on_audio(\u001b[39m'\u001b[39;49m\u001b[39m/Users/ankush/Downloads/deakin-units/data/b2/Alauda arvensis European Skylark/Alauda_arvensis_PF03884_short.mp3\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m, in \u001b[0;36mpredict_on_audio\u001b[0;34m(audio_binary)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_on_audio\u001b[39m(audio_binary):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Preprocess the audio to be suitable for your model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     image, audio_clip, sample_rate \u001b[39m=\u001b[39m combined_pipeline(config, audio_binary)\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Add a dimension to match the model's input shape\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(image, \u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 91\u001b[0m, in \u001b[0;36mcombined_pipeline\u001b[0;34m(config, audio_clip)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m# calculate the image shape and ensure it is correct\u001b[39;00m\n\u001b[1;32m     90\u001b[0m expected_clip_samples \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mAUDIO_CLIP_DURATION\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mAUDIO_SAMPLE_RATE\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mAUDIO_STRIDE\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 91\u001b[0m image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mensure_shape(image, [expected_clip_samples, config[\u001b[39m'\u001b[39;49m\u001b[39mAUDIO_MELS\u001b[39;49m\u001b[39m'\u001b[39;49m], config[\u001b[39m'\u001b[39;49m\u001b[39mMODEL_INPUT_IMAGE_CHANNELS\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n\u001b[1;32m     93\u001b[0m \u001b[39m# note here a high quality LANCZOS5 is applied to resize the image to match model image input size\u001b[39;00m\n\u001b[1;32m     94\u001b[0m image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize(image, (config[\u001b[39m'\u001b[39m\u001b[39mMODEL_INPUT_IMAGE_WIDTH\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39mMODEL_INPUT_IMAGE_HEIGHT\u001b[39m\u001b[39m'\u001b[39m]), \n\u001b[1;32m     95\u001b[0m                         method\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mResizeMethod\u001b[39m.\u001b[39mLANCZOS5)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shape of tensor input [540,260,3] is not compatible with expected shape [1200,260,3]. [Op:EnsureShape] name: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use the function with the binary data\n",
    "predict_on_audio('/Users/ankush/Downloads/deakin-units/data/b2/Alauda arvensis European Skylark/Alauda_arvensis_PF03884_short.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/464147925.py:19: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data, sr = librosa.load(filename, sr=params.SAMPLE_RATE)\n",
      "/Users/ankush/anaconda3/envs/new_env/lib/python3.8/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "(1, 15) [[ 0.7405799  -0.1215523  -3.3350132   2.3348048   3.4266036   0.7246958\n",
      "   0.3850828  -0.7148199  -4.122674    9.141884   -3.6063426  -1.2421137\n",
      "  -0.25307405 -2.0979192  -0.1465312 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/464147925.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 286ms/step\n",
      "(1, 15) [[-0.2944098  -2.3573754   1.0701752  -0.4338679   2.1659954  -0.2703556\n",
      "   2.1128     -1.1273507  -1.835596    1.7622149  -0.7875129  -0.72051674\n",
      "  -0.15483226  0.9607781  -0.41029572]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/464147925.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 217ms/step\n",
      "(1, 15) [[ 0.93047553 -1.6647632  -1.2673922  -0.4354485  -0.31507757 -1.6520909\n",
      "  -1.2490606  -0.05418235  4.0025015   1.6061516   2.4417086  -2.505131\n",
      "   0.11330611 -2.9678547   0.7121459 ]]\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/464147925.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "(1, 15) [[ 1.2425679   0.14110121 -3.3561156  -0.55636466  0.06118035 -0.836532\n",
      "  -2.3813221  -0.5147153   0.5666767   2.772831   -0.8777431  -0.6715298\n",
      "   1.8792427  -2.782655    1.1477658 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/464147925.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step\n",
      "(1, 15) [[-1.088898   -0.7406041   0.38333064 -0.01174553  2.6887362  -0.72896075\n",
      "   0.85128415 -2.555808   -2.3863757   3.1148634  -1.2484775  -1.1477927\n",
      "   0.08408052 -0.45379427 -0.49087188]]\n",
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/464147925.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15) [[-0.14137267 -0.7150199  -1.5210944  -0.8631474   0.9455284  -1.1775297\n",
      "  -0.57695854 -1.0064554  -0.60633785  2.2444437  -1.0986612  -1.2718554\n",
      "   1.1775382  -0.11007012 -0.2058347 ]]\n",
      "  start_time end_time yamnet_label  yamnet_probability  \\\n",
      "0          5        6       Animal            0.159492   \n",
      "1         16       18       Animal            0.159492   \n",
      "2         20       21       Animal            0.159492   \n",
      "3         27       28       Animal            0.159492   \n",
      "4         31       32       Animal            0.159492   \n",
      "5         40       41       Animal            0.159492   \n",
      "\n",
      "                            your_model_label  your_model_probability  \n",
      "0                            Felis Catus Cat                   99.46  \n",
      "1               Cervus unicolour Sambar deer                   26.00  \n",
      "2  Eopsaltria australis Eastern yellow robin                   68.15  \n",
      "3                            Felis Catus Cat                   44.62  \n",
      "4                            Felis Catus Cat                   48.51  \n",
      "5                            Felis Catus Cat                   44.18  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/464147925.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import yamnet.params as params\n",
    "import yamnet.yamnet as yamnet_model\n",
    "import librosa\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "# Load YAMNet model\n",
    "yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet.load_weights('yamnet/yamnet.h5')\n",
    "yamnet_classes = yamnet_model.class_names('yamnet/yamnet_class_map.csv')\n",
    "\n",
    "frame_len = int(params.SAMPLE_RATE * 1)  # 1sec\n",
    "# Read the whole audio file\n",
    "filename = 'test.m4a'\n",
    "data, sr = librosa.load(filename, sr=params.SAMPLE_RATE)\n",
    "\n",
    "# Split the audio data into 1 second chunks\n",
    "chunks = np.array_split(data, len(data) // frame_len)\n",
    "\n",
    "intervals = []\n",
    "current_interval = None\n",
    "\n",
    "for cnt, frame_data in enumerate(chunks):\n",
    "    start_time = cnt\n",
    "    end_time = cnt + 1\n",
    "    scores, _ = yamnet.predict(np.reshape(frame_data, [1, -1]), steps=1)\n",
    "    yamnet_prediction = np.mean(scores, axis=0)\n",
    "    top5_i = np.argsort(yamnet_prediction)[::-1][:5]\n",
    "\n",
    "    if yamnet_classes[top5_i[0]] == 'Animal' and yamnet_prediction[top5_i[0]] > 0.2:\n",
    "        if current_interval is None:\n",
    "            current_interval = {'start': cnt, 'end': cnt+1}\n",
    "        else:\n",
    "            current_interval['end'] = cnt+1\n",
    "    else:\n",
    "        if current_interval:\n",
    "            intervals.append(current_interval)\n",
    "            current_interval = None\n",
    "\n",
    "if current_interval:\n",
    "    intervals.append(current_interval)\n",
    "\n",
    "df = pd.DataFrame(columns=['start_time', 'end_time', 'yamnet_label', 'yamnet_probability', 'your_model_label', 'your_model_probability'])\n",
    "\n",
    "for interval in intervals:  \n",
    "    segment_data = data[interval['start']*frame_len : interval['end']*frame_len]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_audio_file:\n",
    "        sf.write(temp_audio_file.name, segment_data, params.SAMPLE_RATE)\n",
    "        with open(temp_audio_file.name, 'rb') as binary_file:\n",
    "            predicted_class, predicted_probability = predict_on_audio(binary_file.read())\n",
    "            \n",
    "        df = df.append({\n",
    "            'start_time': interval['start'],\n",
    "            'end_time': interval['end'],\n",
    "            'yamnet_label': 'Animal',\n",
    "            'yamnet_probability': np.mean(yamnet_prediction[top5_i]),\n",
    "            'your_model_label': predicted_class,\n",
    "            'your_model_probability': predicted_probability\n",
    "        }, ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying different output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/jx74xxwd0qgct2nvjkzx_m1h0000gn/T/ipykernel_60496/639613434.py:19: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data, sr = librosa.load(filename, sr=params.SAMPLE_RATE)\n",
      "/Users/ankush/anaconda3/envs/new_env/lib/python3.8/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 664ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 1s 584ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "    start_time  end_time                            yamnet_label_1  \\\n",
      "0            5         6                                    Animal   \n",
      "1           15        16                              Wild animals   \n",
      "2           16        17                                    Animal   \n",
      "3           17        18                                    Animal   \n",
      "4           20        21                                    Animal   \n",
      "5           27        28                                    Animal   \n",
      "6           30        31                              Wild animals   \n",
      "7           31        32                                    Animal   \n",
      "8           32        33                                      Bird   \n",
      "9           33        34                                      Bird   \n",
      "10          40        41                                    Animal   \n",
      "11          41        42  Livestock, farm animals, working animals   \n",
      "12          42        43   Bird vocalization, bird call, bird song   \n",
      "\n",
      "    yamnet_probability_1                            yamnet_label_2  \\\n",
      "0               0.968866  Livestock, farm animals, working animals   \n",
      "1               0.563790                                      Bird   \n",
      "2               0.773564                              Wild animals   \n",
      "3               0.903251                              Wild animals   \n",
      "4               0.388485                              Wild animals   \n",
      "5               0.386347                                      Bird   \n",
      "6               0.798768                                      Bird   \n",
      "7               0.892200                              Wild animals   \n",
      "8               0.677822                              Wild animals   \n",
      "9               0.616023   Bird vocalization, bird call, bird song   \n",
      "10              0.938175                                      Bird   \n",
      "11              0.513519                                    Animal   \n",
      "12              0.654657                                      Bird   \n",
      "\n",
      "    yamnet_probability_2                           yamnet_label_3  \\\n",
      "0               0.939845                                     Fowl   \n",
      "1               0.477609                                   Animal   \n",
      "2               0.638726                                     Bird   \n",
      "3               0.899348                                     Bird   \n",
      "4               0.334280                                     Bird   \n",
      "5               0.301350                Outside, rural or natural   \n",
      "6               0.794923  Bird vocalization, bird call, bird song   \n",
      "7               0.875330                                     Bird   \n",
      "8               0.667248                                   Animal   \n",
      "9               0.576408                             Wild animals   \n",
      "10              0.936621                             Wild animals   \n",
      "11              0.458732                                    Bleat   \n",
      "12              0.643022                             Wild animals   \n",
      "\n",
      "    yamnet_probability_3                         your_model_label_1  \\\n",
      "0               0.924176                            Felis Catus Cat   \n",
      "1               0.391069                            Felis Catus Cat   \n",
      "2               0.621845                            Felis Catus Cat   \n",
      "3               0.871762                            Felis Catus Cat   \n",
      "4               0.206926  Eopsaltria australis Eastern yellow robin   \n",
      "5               0.284338                            Felis Catus Cat   \n",
      "6               0.785069                            Felis Catus Cat   \n",
      "7               0.851301                            Felis Catus Cat   \n",
      "8               0.658058                            Felis Catus Cat   \n",
      "9               0.563931                Rattus norvegicus Brown rat   \n",
      "10              0.935774                            Felis Catus Cat   \n",
      "11              0.382022                            Felis Catus Cat   \n",
      "12              0.627438                            Felis Catus Cat   \n",
      "\n",
      "    your_model_probability_1                        your_model_label_2  \\\n",
      "0                   9.141884              Cervus unicolour Sambar deer   \n",
      "1                   2.229960               Rattus norvegicus Brown rat   \n",
      "2                   7.215248              Cervus unicolour Sambar deer   \n",
      "3                   2.530861              Cervus unicolour Sambar deer   \n",
      "4                   4.002501  Pachycephala rufiventris Rufous whistler   \n",
      "5                   2.772831               Rattus norvegicus Brown rat   \n",
      "6                   4.783803              Cervus unicolour Sambar deer   \n",
      "7                   3.114863              Cervus unicolour Sambar deer   \n",
      "8                   3.450251               Rattus norvegicus Brown rat   \n",
      "9                   1.809888                           Felis Catus Cat   \n",
      "10                  2.244444               Rattus norvegicus Brown rat   \n",
      "11                  6.267097        Corvus coronoides Australian raven   \n",
      "12                  6.941738              Cervus unicolour Sambar deer   \n",
      "\n",
      "    your_model_probability_2                   your_model_label_3  \\\n",
      "0                   3.426604              Capra hircus Feral goat   \n",
      "1                   0.863575         Cervus unicolour Sambar deer   \n",
      "2                   2.566250              Capra hircus Feral goat   \n",
      "3                   1.385957                  sus scrofa Wild pig   \n",
      "4                   2.441709                      Felis Catus Cat   \n",
      "5                   1.879243  Aegotheles cristatus owlet-nightjar   \n",
      "6                   1.872087   Corvus coronoides Australian raven   \n",
      "7                   2.688736   Corvus coronoides Australian raven   \n",
      "8                   1.835899     Alauda arvensis European Skylark   \n",
      "9                   1.304349                  sus scrofa Wild pig   \n",
      "10                  1.177538         Cervus unicolour Sambar deer   \n",
      "11                  2.299851              Capra hircus Feral goat   \n",
      "12                  3.017252              Capra hircus Feral goat   \n",
      "\n",
      "    your_model_probability_3  \n",
      "0                   2.334805  \n",
      "1                   0.706902  \n",
      "2                   1.487156  \n",
      "3                   1.051897  \n",
      "4                   1.606152  \n",
      "5                   1.242568  \n",
      "6                   1.492450  \n",
      "7                   0.851284  \n",
      "8                   0.970522  \n",
      "9                   0.725475  \n",
      "10                  0.945528  \n",
      "11                  2.188631  \n",
      "12                  2.253137  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import yamnet.params as params\n",
    "import yamnet.yamnet as yamnet_model\n",
    "import librosa\n",
    "import tempfile\n",
    "\n",
    "# Load YAMNet model\n",
    "yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet.load_weights('yamnet/yamnet.h5')\n",
    "yamnet_classes = yamnet_model.class_names('yamnet/yamnet_class_map.csv')\n",
    "\n",
    "frame_len = int(params.SAMPLE_RATE * 1)  # 1sec\n",
    "\n",
    "# Read the whole audio file\n",
    "filename = 'test.m4a'\n",
    "data, sr = librosa.load(filename, sr=params.SAMPLE_RATE)\n",
    "\n",
    "# Split the audio data into 1 second chunks\n",
    "chunks = np.array_split(data, len(data) // frame_len)\n",
    "\n",
    "intervals = []\n",
    "current_interval = None\n",
    "\n",
    "yamnet_predictions = []\n",
    "top_indices = []\n",
    "# ... [initial imports and model loading here]\n",
    "\n",
    "df_rows = []\n",
    "\n",
    "for cnt, frame_data in enumerate(chunks):\n",
    "    # Get YAMNet predictions\n",
    "    scores, _ = yamnet.predict(np.reshape(frame_data, [1, -1]), steps=1)\n",
    "    yamnet_prediction = np.mean(scores, axis=0)\n",
    "    top5_i = np.argsort(yamnet_prediction)[::-1][:5]\n",
    "\n",
    "    if (yamnet_classes[top5_i[0]] in ['Animal', 'Bird'] and yamnet_prediction[top5_i[0]] > 0.2) or (yamnet_classes[top5_i[1]] in ['Animal', 'Bird'] and yamnet_prediction[top5_i[1]] > 0.2):\n",
    "\n",
    "        # Extract segment data for your model\n",
    "        segment_data = data[cnt*frame_len : (cnt+1)*frame_len]\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_audio_file:\n",
    "            sf.write(temp_audio_file.name, segment_data, params.SAMPLE_RATE)\n",
    "            with open(temp_audio_file.name, 'rb') as binary_file:\n",
    "                top3_predictions = predict_on_audio(binary_file.read())\n",
    "\n",
    "        # Build a row for our dataframe\n",
    "        df_row = {\n",
    "            'start_time': cnt,\n",
    "            'end_time': cnt+1,\n",
    "            'yamnet_label_1': yamnet_classes[top5_i[0]],\n",
    "            'yamnet_probability_1': yamnet_prediction[top5_i[0]],\n",
    "            'yamnet_label_2': yamnet_classes[top5_i[1]],\n",
    "            'yamnet_probability_2': yamnet_prediction[top5_i[1]],\n",
    "            'yamnet_label_3': yamnet_classes[top5_i[2]],\n",
    "            'yamnet_probability_3': yamnet_prediction[top5_i[2]],\n",
    "        }\n",
    "\n",
    "        for i, pred in enumerate(top3_predictions):\n",
    "            df_row[f'your_model_label_{i+1}'] = pred[0] if len(pred) > 0 else None\n",
    "            df_row[f'your_model_probability_{i+1}'] = pred[1] if len(pred) > 1 else None\n",
    "\n",
    "        df_rows.append(df_row)\n",
    "\n",
    "df = pd.DataFrame(df_rows)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>yamnet_label_1</th>\n",
       "      <th>yamnet_probability_1</th>\n",
       "      <th>yamnet_label_2</th>\n",
       "      <th>yamnet_probability_2</th>\n",
       "      <th>yamnet_label_3</th>\n",
       "      <th>yamnet_probability_3</th>\n",
       "      <th>your_model_label_1</th>\n",
       "      <th>your_model_probability_1</th>\n",
       "      <th>your_model_label_2</th>\n",
       "      <th>your_model_probability_2</th>\n",
       "      <th>your_model_label_3</th>\n",
       "      <th>your_model_probability_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.968866</td>\n",
       "      <td>Livestock, farm animals, working animals</td>\n",
       "      <td>0.939845</td>\n",
       "      <td>Fowl</td>\n",
       "      <td>0.924176</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>9.141884</td>\n",
       "      <td>Cervus unicolour Sambar deer</td>\n",
       "      <td>3.426604</td>\n",
       "      <td>Capra hircus Feral goat</td>\n",
       "      <td>2.334805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.563790</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.477609</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.391069</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>2.229960</td>\n",
       "      <td>Rattus norvegicus Brown rat</td>\n",
       "      <td>0.863575</td>\n",
       "      <td>Cervus unicolour Sambar deer</td>\n",
       "      <td>0.706902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.773564</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.638726</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.621845</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>7.215248</td>\n",
       "      <td>Cervus unicolour Sambar deer</td>\n",
       "      <td>2.566250</td>\n",
       "      <td>Capra hircus Feral goat</td>\n",
       "      <td>1.487156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.903251</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.871762</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>2.530861</td>\n",
       "      <td>Cervus unicolour Sambar deer</td>\n",
       "      <td>1.385957</td>\n",
       "      <td>sus scrofa Wild pig</td>\n",
       "      <td>1.051897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.388485</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.334280</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.206926</td>\n",
       "      <td>Eopsaltria australis Eastern yellow robin</td>\n",
       "      <td>4.002501</td>\n",
       "      <td>Pachycephala rufiventris Rufous whistler</td>\n",
       "      <td>2.441709</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>1.606152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.386347</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.301350</td>\n",
       "      <td>Outside, rural or natural</td>\n",
       "      <td>0.284338</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>2.772831</td>\n",
       "      <td>Rattus norvegicus Brown rat</td>\n",
       "      <td>1.879243</td>\n",
       "      <td>Aegotheles cristatus owlet-nightjar</td>\n",
       "      <td>1.242568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.798768</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.794923</td>\n",
       "      <td>Bird vocalization, bird call, bird song</td>\n",
       "      <td>0.785069</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>4.783803</td>\n",
       "      <td>Cervus unicolour Sambar deer</td>\n",
       "      <td>1.872087</td>\n",
       "      <td>Corvus coronoides Australian raven</td>\n",
       "      <td>1.492450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.892200</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.875330</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>3.114863</td>\n",
       "      <td>Cervus unicolour Sambar deer</td>\n",
       "      <td>2.688736</td>\n",
       "      <td>Corvus coronoides Australian raven</td>\n",
       "      <td>0.851284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.677822</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.667248</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.658058</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>3.450251</td>\n",
       "      <td>Rattus norvegicus Brown rat</td>\n",
       "      <td>1.835899</td>\n",
       "      <td>Alauda arvensis European Skylark</td>\n",
       "      <td>0.970522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.616023</td>\n",
       "      <td>Bird vocalization, bird call, bird song</td>\n",
       "      <td>0.576408</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.563931</td>\n",
       "      <td>Rattus norvegicus Brown rat</td>\n",
       "      <td>1.809888</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>1.304349</td>\n",
       "      <td>sus scrofa Wild pig</td>\n",
       "      <td>0.725475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.938175</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.936621</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.935774</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>2.244444</td>\n",
       "      <td>Rattus norvegicus Brown rat</td>\n",
       "      <td>1.177538</td>\n",
       "      <td>Cervus unicolour Sambar deer</td>\n",
       "      <td>0.945528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>Livestock, farm animals, working animals</td>\n",
       "      <td>0.513519</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.458732</td>\n",
       "      <td>Bleat</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>6.267097</td>\n",
       "      <td>Corvus coronoides Australian raven</td>\n",
       "      <td>2.299851</td>\n",
       "      <td>Capra hircus Feral goat</td>\n",
       "      <td>2.188631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>Bird vocalization, bird call, bird song</td>\n",
       "      <td>0.654657</td>\n",
       "      <td>Bird</td>\n",
       "      <td>0.643022</td>\n",
       "      <td>Wild animals</td>\n",
       "      <td>0.627438</td>\n",
       "      <td>Felis Catus Cat</td>\n",
       "      <td>6.941738</td>\n",
       "      <td>Cervus unicolour Sambar deer</td>\n",
       "      <td>3.017252</td>\n",
       "      <td>Capra hircus Feral goat</td>\n",
       "      <td>2.253137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_time  end_time                            yamnet_label_1  \\\n",
       "0            5         6                                    Animal   \n",
       "1           15        16                              Wild animals   \n",
       "2           16        17                                    Animal   \n",
       "3           17        18                                    Animal   \n",
       "4           20        21                                    Animal   \n",
       "5           27        28                                    Animal   \n",
       "6           30        31                              Wild animals   \n",
       "7           31        32                                    Animal   \n",
       "8           32        33                                      Bird   \n",
       "9           33        34                                      Bird   \n",
       "10          40        41                                    Animal   \n",
       "11          41        42  Livestock, farm animals, working animals   \n",
       "12          42        43   Bird vocalization, bird call, bird song   \n",
       "\n",
       "    yamnet_probability_1                            yamnet_label_2  \\\n",
       "0               0.968866  Livestock, farm animals, working animals   \n",
       "1               0.563790                                      Bird   \n",
       "2               0.773564                              Wild animals   \n",
       "3               0.903251                              Wild animals   \n",
       "4               0.388485                              Wild animals   \n",
       "5               0.386347                                      Bird   \n",
       "6               0.798768                                      Bird   \n",
       "7               0.892200                              Wild animals   \n",
       "8               0.677822                              Wild animals   \n",
       "9               0.616023   Bird vocalization, bird call, bird song   \n",
       "10              0.938175                                      Bird   \n",
       "11              0.513519                                    Animal   \n",
       "12              0.654657                                      Bird   \n",
       "\n",
       "    yamnet_probability_2                           yamnet_label_3  \\\n",
       "0               0.939845                                     Fowl   \n",
       "1               0.477609                                   Animal   \n",
       "2               0.638726                                     Bird   \n",
       "3               0.899348                                     Bird   \n",
       "4               0.334280                                     Bird   \n",
       "5               0.301350                Outside, rural or natural   \n",
       "6               0.794923  Bird vocalization, bird call, bird song   \n",
       "7               0.875330                                     Bird   \n",
       "8               0.667248                                   Animal   \n",
       "9               0.576408                             Wild animals   \n",
       "10              0.936621                             Wild animals   \n",
       "11              0.458732                                    Bleat   \n",
       "12              0.643022                             Wild animals   \n",
       "\n",
       "    yamnet_probability_3                         your_model_label_1  \\\n",
       "0               0.924176                            Felis Catus Cat   \n",
       "1               0.391069                            Felis Catus Cat   \n",
       "2               0.621845                            Felis Catus Cat   \n",
       "3               0.871762                            Felis Catus Cat   \n",
       "4               0.206926  Eopsaltria australis Eastern yellow robin   \n",
       "5               0.284338                            Felis Catus Cat   \n",
       "6               0.785069                            Felis Catus Cat   \n",
       "7               0.851301                            Felis Catus Cat   \n",
       "8               0.658058                            Felis Catus Cat   \n",
       "9               0.563931                Rattus norvegicus Brown rat   \n",
       "10              0.935774                            Felis Catus Cat   \n",
       "11              0.382022                            Felis Catus Cat   \n",
       "12              0.627438                            Felis Catus Cat   \n",
       "\n",
       "    your_model_probability_1                        your_model_label_2  \\\n",
       "0                   9.141884              Cervus unicolour Sambar deer   \n",
       "1                   2.229960               Rattus norvegicus Brown rat   \n",
       "2                   7.215248              Cervus unicolour Sambar deer   \n",
       "3                   2.530861              Cervus unicolour Sambar deer   \n",
       "4                   4.002501  Pachycephala rufiventris Rufous whistler   \n",
       "5                   2.772831               Rattus norvegicus Brown rat   \n",
       "6                   4.783803              Cervus unicolour Sambar deer   \n",
       "7                   3.114863              Cervus unicolour Sambar deer   \n",
       "8                   3.450251               Rattus norvegicus Brown rat   \n",
       "9                   1.809888                           Felis Catus Cat   \n",
       "10                  2.244444               Rattus norvegicus Brown rat   \n",
       "11                  6.267097        Corvus coronoides Australian raven   \n",
       "12                  6.941738              Cervus unicolour Sambar deer   \n",
       "\n",
       "    your_model_probability_2                   your_model_label_3  \\\n",
       "0                   3.426604              Capra hircus Feral goat   \n",
       "1                   0.863575         Cervus unicolour Sambar deer   \n",
       "2                   2.566250              Capra hircus Feral goat   \n",
       "3                   1.385957                  sus scrofa Wild pig   \n",
       "4                   2.441709                      Felis Catus Cat   \n",
       "5                   1.879243  Aegotheles cristatus owlet-nightjar   \n",
       "6                   1.872087   Corvus coronoides Australian raven   \n",
       "7                   2.688736   Corvus coronoides Australian raven   \n",
       "8                   1.835899     Alauda arvensis European Skylark   \n",
       "9                   1.304349                  sus scrofa Wild pig   \n",
       "10                  1.177538         Cervus unicolour Sambar deer   \n",
       "11                  2.299851              Capra hircus Feral goat   \n",
       "12                  3.017252              Capra hircus Feral goat   \n",
       "\n",
       "    your_model_probability_3  \n",
       "0                   2.334805  \n",
       "1                   0.706902  \n",
       "2                   1.487156  \n",
       "3                   1.051897  \n",
       "4                   1.606152  \n",
       "5                   1.242568  \n",
       "6                   1.492450  \n",
       "7                   0.851284  \n",
       "8                   0.970522  \n",
       "9                   0.725475  \n",
       "10                  0.945528  \n",
       "11                  2.188631  \n",
       "12                  2.253137  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 14)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
