{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55203f1a",
   "metadata": {},
   "source": [
    "# The following are plans for changes to the optimized_engine_pipeline_MASTER \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0543eb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1000bc",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "### There are several categories of augmentations that can be trialled. These are listed below. \n",
    "\n",
    "#### In addition to they type of augmentation, is the way in which i can be applied. Options include\n",
    "\n",
    "- Applying each augmentation to the audiofiles in the Species folder, to increase the dataset file size. This would ensure each sample receives each augmentation type. This approach could be used in the case of overlaying background audio to the files. \n",
    "\n",
    "- Audio file augmentations with the Audiomentations library would likely be more appropriate to have as a probability of the effect being applied, and a range in which it is applied selected from randomly. This could be applied separately, or as a combined augmentation of many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d87e95",
   "metadata": {},
   "source": [
    "#### Augmentations to improve generalisibility:\n",
    "\n",
    "This includes overlaying background audio to the clips. This provides a distinctly different file to train with and helps the model learn what the features are. The audio overlay will include noises that will  be commonly heard in the forest environment in the Otways National Park. Static noise is included as there may be a background humm of the microphone and processor for which the model should be trained against. Other suggestions are also to be added. A method of applying this is to have a clean sample of a background audio file, and randomly select a segment of this to overlay for each file. This could be run for each noise type separately, or using a combined approach, or both.\n",
    "\n",
    "- Rain noise\n",
    "- Wind noise\n",
    "- Water/stream noise\n",
    "- Static noise\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4ec2a",
   "metadata": {},
   "source": [
    "### Augmentations to audio files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def5d32",
   "metadata": {},
   "source": [
    "### Audiomentations:\n",
    "\n",
    "Augmentating the raw audio will help to increase the training dataset by applying small but significant changes to the audiofile that result in a different Mel Spectogram. There are many to choose from, but the ones most relevant to try are included below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dfeed",
   "metadata": {},
   "source": [
    "- TimeStretch\n",
    "- PitchShift\n",
    "- TimeMask\n",
    "- FrequencyMask\n",
    "- SevenBandParametricEQ\n",
    "- AddGausianSNR\n",
    "- AddBackgroundNoise\n",
    "- Reverberation\n",
    "- (volume shift)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f90ad",
   "metadata": {},
   "source": [
    "The parameters for which there are applied should be selectrd for within a range that is suitable for that genus.\n",
    "\n",
    "For example, PitchShift on a bird will likely not be suitable, as their calls have a distinctive pitch. Likewise for TimeStretch.\n",
    "\n",
    "However for other categories such as goats, cats, boar this could be a meaningful augmentation as vocalisaitons naturally lie within a range, depending on individual animals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b78d14",
   "metadata": {},
   "source": [
    "### Image augmentations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59b0d1",
   "metadata": {},
   "source": [
    "After the audio file has been converted to a Mel Spectogram, the usual computer vision-based augmentations can be applied. However, once again there is only a subset for which are applicable for Mel Spectograms. Options to try include:\n",
    "\n",
    "- time masking with black, white, and mean colour masks\n",
    "- frequency masking with black, white, and mean colour masks\n",
    "- contrast adjustments\n",
    "- SpecAugment involving time warping, frequency masking, and time masking.\n",
    "- Random scaling of the spectrogram for amplitude variation simulation.\n",
    "- Application of Gaussian blur for resilience against quality degradations.\n",
    "- Horizontal and vertical flips for simulating different sound directions/sources.\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Event Detection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "461da476737163fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Event Detection Using YamNet\n",
    "\n",
    "#### Current Methodology\n",
    "The approach to sound event detection in our system currently utilizes **YamNet**, a deep neural network trained to identify a broad array of sound categories from a large and diverse dataset. The process involves:\n",
    "\n",
    "- **Initial Sound Classification**: YamNet is employed to analyze incoming audio streams and classify sounds into predefined categories. This step is crucial for filtering relevant sound events from the background noise.\n",
    "- **Targeted Processing**: After classification, sounds that match the categories of interest are routed to a specialized engine. This engine is optimized for more refined and detailed recognition tasks, focusing on specific characteristics of the selected sound categories.\n",
    "\n",
    "#### Challenges with Overlapping Sounds\n",
    "One of the major challenges in our current setup is handling overlapping sounds. This issue arises when multiple sound sources are active simultaneously, complicating the task of accurate sound classification and recognition:\n",
    "\n",
    "- **Accuracy Degradation**: Overlapping sounds can lead to misclassifications or missed detections as the model struggles to isolate and identify individual sound signatures within the mix.\n",
    "- **Segmentation Difficulty**: Properly segmenting audio to accurately identify and classify overlapping sounds remains a technically challenging problem, often requiring advanced decomposition and classification techniques.\n",
    "\n",
    "#### Focus for Next Trimester: Overlapping Sound Recognition\n",
    "Recognizing and accurately classifying overlapping sounds has been identified as one of the primary candidate focuses for the next trimester. We aim to enhance our system's capability to deal with this complex issue through several strategic initiatives:\n",
    "\n",
    "- **Enhanced Signal Separation**: We plan to implement advanced digital signal processing techniques such as Blind Source Separation (BSS) and Computational Auditory Scene Analysis (CASA) to improve the segregation of sound sources.\n",
    "- **Model Improvement and Re-training**: By augmenting our training datasets to include a greater variety of overlapping sound scenarios and employing robust training methods, we anticipate improvements in the model's performance in complex auditory environments.\n",
    "- **Utilization of Spatial Audio Techniques**: Integrating spatial audio data can provide additional context that helps distinguish between sound sources based on their spatial characteristics, further aiding in the effective segmentation and classification of overlapping sounds.\n",
    "\n",
    "#### Advanced Research and Collaboration\n",
    "To further our goals, we will engage in research collaborations and partnerships, seeking insights and technologies from leading experts in audio processing and machine learning. By leveraging cutting-edge research and technology, we aim to develop a more sophisticated and capable system for real-time event detection in acoustically challenging settings.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aefa930562823b12"
  },
  {
   "cell_type": "markdown",
   "id": "4da93f16",
   "metadata": {},
   "source": [
    "## Changes to the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The model architecture itself may not be optimal for training on the current dataset. \n",
    "\n",
    "### The current limitations to the dataset include:\n",
    "\n",
    "#### Small number of audio files for some species\n",
    "\n",
    "Endangeres species tend to have few files available, but are priority spcecies to include, so pose a issue to be resolved. Possibile solutions include:\n",
    "- selectively applying many augmentations to those files to artificially increase dataset size\n",
    "- creating a separate model that is trained on under-represented species\n",
    "- expanding the search for more data\n",
    "\n",
    "#### Large number of audio files for some species\n",
    "\n",
    "There are many species for which there is an abundance of audio availaboe, and selectively decreasing the number of files used may be an option. Appling augmentations to species that are represented around the mean/median could be an option also, to match the number of files. There are commonly large number of files for pests, domestic animals, common species of birds, and frogs.\n",
    "\n",
    "#### Together leades to an imbalance in the model's training files\n",
    "\n",
    "---\n",
    "\n",
    "#### Raw full-length audio files that have non-animal vocalisations. \n",
    "\n",
    "The audio sourced is almost always not *clean* and without proper pre-processing and cleaning of the datasource, the model not learn correctly. The commonly found issues include:\n",
    "\n",
    "- Vocalisations of other species of that genus (eg. several species of birds in the one file)\n",
    "- Vocalisations of different genus (eg. frog, insect and bird noises in the one file)\n",
    "- Human voices are often present at the beginning of files, introducing that species.\n",
    "    \n",
    "####  Incorrectly classified segments\n",
    "\n",
    "The cleaning process currently is to use YamNet as event detection for classes for which that vocalisation falls under. The limitations to this approach include:\n",
    "\n",
    "- Limited set of audio event types that YamNet can classify\n",
    "- Misclassificaiton of audio events by YamNet\n",
    "\n",
    "Together this can lead to a dataset of segments of audio for which many are not vocalisations of that species. \n",
    "\n",
    "Other parameters include the length of audio extracted when an event is detected.\n",
    "\n",
    "#### Current solutions being investifgated\n",
    "\n",
    "Fine tuning of the parameters used in YamNet is the first approach to take. \n",
    "A ML approach of cluster analysis is also being trialed to group events into clusters. The number of clusters can be tweaked.\n",
    "Manual cleaning of the audio files is the immediate solution to clean the data. \n",
    "Further, combining these approaches is running clustering on a speceis, and grouping clusters for which include positive audio of that species.\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58fd5809cc25804a"
  },
  {
   "cell_type": "markdown",
   "id": "3221c41a",
   "metadata": {},
   "source": [
    "## Changes to the model "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Model Architecture Can Also Be Investigated\n",
    "This involves:\n",
    "\n",
    "#### Using Different Imported (Transfer-Learning) Models\n",
    "There are many computer vision models that are pre-trained on images. The current engine pipeline uses ***EfficientNetV2B0***; however, there are many other models to trial:\n",
    "\n",
    "- **VGG16**\n",
    "- **InceptionV3** (trained)\n",
    "- **Xception**\n",
    "- **MobileNetV2** (trained)\n",
    "- **ResNet50V2** (trained)\n",
    "- **DenseNet121**\n",
    "- **DenseNet169**\n",
    "- **DenseNet201**\n",
    "- **EfficientNetV2B0** (trained)\n",
    "- **EfficientNetV2L**\n",
    "- **EfficientNetV2M**\n",
    "- **EfficientNetV2S**\n",
    "- **InceptionResNetV2**\n",
    "- **MobileNetV3Large**\n",
    "- **MobileNetV3Small** (trained)\n",
    "- **ResNet101V2**\n",
    "\n",
    "These models have been developed during the term and are listed in your document as available for trial. Models marked as \"trained\" have been evaluated this semester, and detailed versions are now specified for models with multiple configurations.\n",
    "\n",
    "#### Using Different Imported Models Trained for Audio Classification\n",
    "These are models that have been trained on Mel Spectrograms and may provide more relevant feature embeddings for audio-based machine learning. Models to try include:\n",
    "\n",
    "- **Bird Vocalisation Classifier**\n",
    "- **SoundNet**\n",
    "- **VGGish**\n",
    "\n",
    "These models specialize in audio signal processing and could be more suitable for tasks involving sound data.\n",
    "\n",
    "### Modifying the Layers Used After the Imported Model\n",
    "To prevent overfitting, it may be necessary to make changes to the current architecture of the model. This could include:\n",
    "\n",
    "- **Adding more dropout layers**: Increasing the dropout rate can help in regularizing the model to avoid overfitting.\n",
    "- **Condensing the network layers**: Simplifying the model by reducing the number of layers or the number of neurons in each layer can lead to less complexity and better generalization.\n",
    "- **Using different activation functions**: Experimenting with different activation functions (e.g., ReLU, ELU, LeakyReLU) might affect the learning dynamics and performance of the model.\n",
    "\n",
    "These strategies are aimed at refining the model to enhance performance and generalizability on unseen data. This approach ensures a thorough exploration of available models and configurations to optimize performance.\n",
    "\n",
    "[Reference Link](https://teams.microsoft.com/l/message/19:046547dcc8874fd5a1dcec5878a856c5@thread.tacv2/1715910578623?tenantId=d02378ec-1688-46d5-8540-1c28b5f470f6&groupId=dba65977-e9c1-41ec-b994-6e593c0efe03&parentMessageId=1715910578623&teamName=DataBytes&channelName=Project%20Echo&createdTime=1715910578623)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d56649852ca37cab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Current Model Architecture and Augmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "717c0e85c945e1aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Audio and Image Augmentation Methods:\n",
    "\n",
    "In the process of model training and validation, data augmentation plays a pivotal role in enhancing the model's ability to generalize. Our dataset comprises both audio clips and their corresponding Mel spectrogram images. To cater to the unique characteristics of each modality, we employ distinct augmentation techniques.\n",
    "\n",
    "#### Audio Augmentation:\n",
    "\n",
    "- **Gaussian Noise Addition**: Intermittently, Gaussian noise is introduced to the audio samples. This simulates the real-world scenario where recordings may have background noise.\n",
    "  \n",
    "- **Time Stretching**: Audio samples are occasionally stretched or compressed in time. This variation simulates different speaking rates and can make the model more tolerant to variations in speech speed.\n",
    "\n",
    "#### Image Augmentation:\n",
    "\n",
    "- **Random Rotations**: The Mel spectrogram images undergo random rotations within a range of -2 to 2 degrees. This ensures the model is robust to minor shifts or distortions in the spectrogram.\n",
    "\n",
    "These augmentations not only enhance the diversity of our training data but also ensure that our model remains robust against minor perturbations and variations in real-world scenarios.\n",
    "\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "1. **Input Layer**: \n",
    "   - This layer is responsible for receiving the images and has a shape of `(MODEL_INPUT_IMAGE_HEIGHT, MODEL_INPUT_IMAGE_WIDTH, MODEL_INPUT_IMAGE_CHANNELS)`.\n",
    "   \n",
    "2. **EfficientNetV2 Feature Generator**:\n",
    "   - Utilizes the EfficientNetV2 architecture from TensorFlow Hub for feature extraction. \n",
    "   - Specifically, we are using the `efficientnet_v2_imagenet1k_b0` variant. \n",
    "   - This layer expects images of size `260x260x3`.\n",
    "\n",
    "3. **Flatten Layer**:\n",
    "   - This layer is used to convert the 2D feature maps into 1D vectors, which can be input to the subsequent dense layers.\n",
    "\n",
    "4. **Batch Normalization**:\n",
    "   - Applied post flattening to normalize the activations, promoting faster training and convergence.\n",
    "\n",
    "5. **Dense Layers**:\n",
    "   - Two fully connected layers with multiple of the number of classes (`len(class_names) * 8` and `len(class_names) * 4`). \n",
    "   - Both layers use ReLU activation and are followed by batch normalization.\n",
    "\n",
    "6. **Dropout Layer**:\n",
    "   - Dropout of `0.50` is applied to prevent overfitting.\n",
    "\n",
    "7. **Output Layer**:\n",
    "   - The final layer is a dense layer with nodes equal to the number of classes (`len(class_names)`). \n",
    "   - It provides the class scores without activation.\n",
    "\n",
    "By utilizing the EfficientNetV2 architecture combined with custom classification layers, we aim to achieve high accuracy on our specific classification task.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "281e5c6bcfd25173"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proposed Strategies for Model Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26064a6ac852e01c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on our current training diagnostics, the model exhibits signs of overfitting when trained on the available dataset. This section outlines a set of structured strategies to counteract this behavior and optimize the model's performance.\n",
    "\n",
    "### 1. **Regularization Techniques**:\n",
    "   - **L1 Regularization**: Introduce an L1 penalty on the model's weights. This encourages the model to have a sparse representation, which can be beneficial in preventing overfitting.\n",
    "   - **L2 Regularization**: By adding an L2 penalty to the loss function, the model's weights are prevented from growing excessively large, a common symptom leading to overfitting.\n",
    "\n",
    "### 2. **Dropout Rate Adjustment**:\n",
    "While the current dropout rate stands at `0.50`, a meticulous analysis of validation results can guide its fine-tuning:\n",
    "   - **Increase Rate**: If overfitting persists, consider increasing the dropout rate to introduce higher regularization.\n",
    "   - **Decrease Rate**: In scenarios where the model underfits, reducing the dropout rate may allow the model to capture more intricate patterns.\n",
    "\n",
    "### 3. **Model Simplification**:\n",
    "   - **Architecture Refinement**: Evaluate the necessity of each layer in the model. Removing superfluous layers or neurons can lead to a simpler model less prone to overfitting.\n",
    "   - **Feature Selection**: Limiting the input features to only the most informative ones can reduce the complexity of the decision boundary, potentially improving generalization.\n",
    "\n",
    "### 4. **Hyperparameter Optimization**:\n",
    "   - **Grid Search**: Implement a systematic exploration of hyperparameters by testing all possible combinations within a predefined range.\n",
    "   - **Random Search**: Instead of exhaustively searching all combinations, random search samples hyperparameters from a distribution over possible parameter values, offering a more efficient search strategy.\n",
    "   - **Bayesian Optimization**: This probabilistic model-based approach seeks to find the hyperparameter combination that gives the maximum value of the target function, making it a robust choice for hyperparameter tuning.\n",
    "\n",
    "### 5. **Expanding the Dataset**:\n",
    "While not a direct model adjustment, obtaining more diverse data points or leveraging external datasets can assist the model in better generalization.\n",
    "\n",
    "In light of the aforementioned strategies, continuous monitoring, testing, and iterative refinement will be pivotal in achieving optimal model performance on unseen data.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dfe4d17fd247042"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## New Strategies for Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3579535923386b3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the pursuit of enhancing model training methodologies, integrating robust platforms such as Google Colab and Vertex AI Workbench can provide substantial benefits. Below are strategies to leverage these platforms effectively:\n",
    "\n",
    "### 1. **Utilizing Google Colab**:\n",
    "   - **Collaborative Environment**: Google Colab offers a cloud-based collaborative platform where multiple users can work on a project simultaneously. This feature enables seamless sharing and editing of code among team members, enhancing productivity and communication.\n",
    "   - **Free Access to GPUs and TPUs**: Google Colab provides free access to powerful computing resources like GPUs and TPUs, which can significantly accelerate the training process of complex models.\n",
    "   - **Integration with Google Drive**: Colab seamlessly integrates with Google Drive, allowing for easy access and management of datasets, notebooks, and trained models stored in the cloud.\n",
    "\n",
    "### 2. **Leveraging Vertex AI Workbench**:\n",
    "   - **Scalable Environment**: Vertex AI Workbench offers a managed Jupyter environment that scales automatically based on the computational needs. This feature ensures that resources are efficiently utilized, supporting larger datasets and more complex model architectures.\n",
    "   - **Pre-built Algorithms and Models**: Take advantage of the extensive library of pre-built algorithms and machine learning models provided by Vertex AI. This can drastically reduce the development time and allow for rapid prototyping and deployment.\n",
    "   - **MLOps Integration**: Vertex AI Workbench is designed to support MLOps practices, facilitating continuous integration and deployment (CI/CD) of machine learning models. It includes features for model monitoring, version control, and performance tracking, which are crucial for maintaining high-quality models in production.\n",
    "   - **Script Storage**: Related scripts have been uploaded to the \"echo_training_script\" storage bucket, simplifying access and management of training resources.\n",
    "\n",
    "### 3. **Enhanced Data Handling and Processing**:\n",
    "   - **Efficient Data Preprocessing**: Both platforms offer libraries and tools to efficiently handle and preprocess large datasets. Utilizing these tools can reduce the time spent on data cleaning and transformation, allowing more focus on model development and training.\n",
    "   - **Hybrid Cloud Approach**: With Vertex AI, you can adopt a hybrid cloud approach, maintaining sensitive data on-premises while leveraging the cloud for computational tasks. This strategy ensures data security while benefiting from cloud scalability.\n",
    "\n",
    "### 4. **Experiment Tracking and Management**:\n",
    "   - **Systematic Experiment Tracking**: Use tools like TensorBoard in Colab or integrated solutions in Vertex AI to track experiments systematically. These tools help visualize model performance metrics and hyperparameters over different training runs, aiding in the fine-tuning and optimization of models.\n",
    "\n",
    "### 5. **Advanced Model Training Techniques**:\n",
    "   - **Distributed Training**: Implement distributed training strategies available in Vertex AI to handle very large models and datasets. Distributed training can significantly speed up the learning process and is a key strategy for training state-of-the-art models.\n",
    "\n",
    "By adopting these strategies and leveraging the strengths of Google Colab and Vertex AI Workbench, teams can enhance their machine learning workflows, reduce development time, and improve the accuracy and efficiency of their models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "613a584402c16b7c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
