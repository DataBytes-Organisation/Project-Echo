{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55203f1a",
   "metadata": {},
   "source": [
    "# The following are plans for changes to the currnet engine pipeline\n",
    "\n",
    "(formarly called the optmised engin pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0543eb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1000bc",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "### There are several categories of augmentations that can be trialled. These are listed below. \n",
    "\n",
    "#### In addition to they type of augmentation, is the way in which i can be applied. Options include\n",
    "\n",
    "- Applying each augmentation to the audiofiles in the Species folder, to increase the dataset file size. This would ensure each sample receives each augmentation type. This approach could be used in the case of overlaying background audio to the files. \n",
    "\n",
    "- Audio file augmentations with the Audiomentations library would likely be more appropriate to have as a probability of the effect being applied, and a range in which it is applied selected from randomly. This could be applied separately, or as a combined augmentation of many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d87e95",
   "metadata": {},
   "source": [
    "#### Augmentations to improve generalisibility:\n",
    "\n",
    "This includes overlaying background audio to the clips. This provides a distinctly different file to train with and helps the model learn what the features are. The audio overlay will include noises that will  be commonly heard in the forest environment in the Otways National Park. Static noise is included as there may be a background humm of the microphone and processor for which the model should be trained against. Other suggestions are also to be added. A method of applying this is to have a clean sample of a background audio file, and randomly select a segment of this to overlay for each file. This could be run for each each noise type separately, or using a combined approach, or both.\n",
    "\n",
    "- Rain noise\n",
    "- Wind noise\n",
    "- Water/stream noise\n",
    "- Static noise\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4ec2a",
   "metadata": {},
   "source": [
    "### Augmentations to audio files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def5d32",
   "metadata": {},
   "source": [
    "### Audiomentations:\n",
    "\n",
    "Augmentating the raw audio will help to increase the training dataset by applying small but significant changes to the audiofile that result in a different Mel Spectogram. There are many to choose from, but the ones most relevant to try are included below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dfeed",
   "metadata": {},
   "source": [
    "- TimeStretch\n",
    "- PitchShift\n",
    "- TimeMask\n",
    "- FrequencyMask\n",
    "- SevenBandParametricEQ\n",
    "- AddGausianSNR\n",
    "- AddBackgroundNoise\n",
    "- Reverberation\n",
    "- (volume shift)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f90ad",
   "metadata": {},
   "source": [
    "The parameters for which there are applied should be selectrd for within a range that is suitable for that genus.\n",
    "\n",
    "For example, PitchShift on a bird will likely not be suitable, as their calls have a distinctive pitch. Likewise for TimeStretch.\n",
    "\n",
    "However for other categories such as goats, cats, boar this could be a meaningful augmentation as vocalisaitons naturally lie within a range, depending on individual animals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b78d14",
   "metadata": {},
   "source": [
    "### Image augmentations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59b0d1",
   "metadata": {},
   "source": [
    "After the audio file has been converted to a Mel Spectogram, the usual computer vision-based augmentations can be applied. However, once again there is only a subset for which are applicable for Mel Spectograms. Options to try include:\n",
    "\n",
    "- time masking with black, white, and mean colour masks\n",
    "- frequency masking with black, white, and mean colour masks\n",
    "- contrast adjustments\n",
    "- SpecAugment involving time warping, frequency masking, and time masking.\n",
    "- Random scaling of the spectrogram for amplitude variation simulation.\n",
    "- Application of Gaussian blur for resilience against quality degradations.\n",
    "- Horizontal and vertical flips for simulating different sound directions/sources.\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da93f16",
   "metadata": {},
   "source": [
    "## Changes to the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The model architecture itself may not be optimal for training on the current dataset. \n",
    "\n",
    "### The current limitations to the dataset include:\n",
    "\n",
    "#### Small number of audio files for some species\n",
    "\n",
    "Endangeres species tend to have few files available, but are priority spcecies to include, so pose a issue to be resolved. Possibile solutions include:\n",
    "- selectively applying many augmentations to those files to artificially increase dataset size\n",
    "- creating a separate model that is trained on under-represented species\n",
    "- expanding the search for more data\n",
    "\n",
    "#### Large number of audio files for some species\n",
    "\n",
    "There are many species for which there is an abundance of audio availaboe, and selectively decreasing the number of files used may be an option. Appling augmentations to species that are represented around the mean/median could be an option also, to match the number of files. There are commonly large number of files for pests, domestic animals, common species of birds, and frogs.\n",
    "\n",
    "#### Together leades to an imbalance in the model's training files\n",
    "\n",
    "---\n",
    "\n",
    "#### Raw full-length audio files that have non-animal vocalisations. \n",
    "\n",
    "The audio sourced is almost always not *clean* and without proper pre-processing and cleaning of the datasource, the model not learn correctly. The commonly found issues include:\n",
    "\n",
    "- Vocalisations of other species of that genus (eg. several species of birds in the one file)\n",
    "- Vocalisations of different genus (eg. frog, insect and bird noises in the one file)\n",
    "- Human voices are often present at the beginning of files, introducing that species.\n",
    "    \n",
    "####  Incorrectly classified segments\n",
    "\n",
    "The cleaning process currently is to use YamNet as event detection for classes for which that vocalisation falls under. The limitations to this approach include:\n",
    "\n",
    "- Limited set of audio event types that YamNet can classify\n",
    "- Misclassificaiton of audio events by YamNet\n",
    "\n",
    "Together this can lead to a dataset of segments of audio for which many are not vocalisations of that species. \n",
    "\n",
    "Other parameters include the length of audio extracted when an event is detected.\n",
    "\n",
    "#### Current solutions being investifgated\n",
    "\n",
    "Fine tuning of the parameters used in YamNet is the first approach to take. \n",
    "A ML approach of cluster analysis is also being trialed to group events into clusters. The number of clusters can be tweaked.\n",
    "Manual cleaning of the audio files is the immediate solution to clean the data. \n",
    "Further, combining these approaches is running clustering on a speceis, and grouping clusters for which include positive audio of that species.\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58fd5809cc25804a"
  },
  {
   "cell_type": "markdown",
   "id": "3221c41a",
   "metadata": {},
   "source": [
    "## Changes to the model "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The model architecture can also be investigated. This involves:\n",
    "\n",
    "#### Using different imported (transfer-learning) models\n",
    "\n",
    "There are many computer-vision models that are pre-trained on images. The current enging pipeline use ***EfficientNetV2*** however the are many others to trial:\n",
    "\n",
    "- VGG16\n",
    "- Inception\n",
    "- Xception\n",
    "- EfficientNet\n",
    "- \n",
    "-\n",
    "- \n",
    "\n",
    "#### Using different imported models trained for audio classification. \n",
    "\n",
    "These are models that have been trained on Mel Spectograms and may provide more relevant feature embeddings for audio-based machine learning. Model to try include:\n",
    "\n",
    "- Bird Vocalisation Classifier\n",
    "- SoundNet\n",
    "- VGGish\n",
    "\n",
    "### Modifying the layers used after the imported model.\n",
    "\n",
    "To prevent overfitting, it may be necessary to make changes to the current architecture of the model. This could include more dropout layers, or differnt kinds of condensing the network layers and using different activation functions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d56649852ca37cab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Current Model Architecture and Augmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "717c0e85c945e1aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Audio and Image Augmentation Methods:\n",
    "\n",
    "In the process of model training and validation, data augmentation plays a pivotal role in enhancing the model's ability to generalize. Our dataset comprises both audio clips and their corresponding Mel spectrogram images. To cater to the unique characteristics of each modality, we employ distinct augmentation techniques.\n",
    "\n",
    "#### Audio Augmentation:\n",
    "\n",
    "- **Gaussian Noise Addition**: Intermittently, Gaussian noise is introduced to the audio samples. This simulates the real-world scenario where recordings may have background noise.\n",
    "  \n",
    "- **Time Stretching**: Audio samples are occasionally stretched or compressed in time. This variation simulates different speaking rates and can make the model more tolerant to variations in speech speed.\n",
    "\n",
    "#### Image Augmentation:\n",
    "\n",
    "- **Random Rotations**: The Mel spectrogram images undergo random rotations within a range of -2 to 2 degrees. This ensures the model is robust to minor shifts or distortions in the spectrogram.\n",
    "\n",
    "These augmentations not only enhance the diversity of our training data but also ensure that our model remains robust against minor perturbations and variations in real-world scenarios.\n",
    "\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "1. **Input Layer**: \n",
    "   - This layer is responsible for receiving the images and has a shape of `(MODEL_INPUT_IMAGE_HEIGHT, MODEL_INPUT_IMAGE_WIDTH, MODEL_INPUT_IMAGE_CHANNELS)`.\n",
    "   \n",
    "2. **EfficientNetV2 Feature Generator**:\n",
    "   - Utilizes the EfficientNetV2 architecture from TensorFlow Hub for feature extraction. \n",
    "   - Specifically, we are using the `efficientnet_v2_imagenet1k_b0` variant. \n",
    "   - This layer expects images of size `260x260x3`.\n",
    "\n",
    "3. **Flatten Layer**:\n",
    "   - This layer is used to convert the 2D feature maps into 1D vectors, which can be input to the subsequent dense layers.\n",
    "\n",
    "4. **Batch Normalization**:\n",
    "   - Applied post flattening to normalize the activations, promoting faster training and convergence.\n",
    "\n",
    "5. **Dense Layers**:\n",
    "   - Two fully connected layers with multiple of the number of classes (`len(class_names) * 8` and `len(class_names) * 4`). \n",
    "   - Both layers use ReLU activation and are followed by batch normalization.\n",
    "\n",
    "6. **Dropout Layer**:\n",
    "   - Dropout of `0.50` is applied to prevent overfitting.\n",
    "\n",
    "7. **Output Layer**:\n",
    "   - The final layer is a dense layer with nodes equal to the number of classes (`len(class_names)`). \n",
    "   - It provides the class scores without activation.\n",
    "\n",
    "By utilizing the EfficientNetV2 architecture combined with custom classification layers, we aim to achieve high accuracy on our specific classification task.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "281e5c6bcfd25173"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proposed Strategies for Model Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26064a6ac852e01c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on our current training diagnostics, the model exhibits signs of overfitting when trained on the available dataset. This section outlines a set of structured strategies to counteract this behavior and optimize the model's performance.\n",
    "\n",
    "### 1. **Regularization Techniques**:\n",
    "   - **L1 Regularization**: Introduce an L1 penalty on the model's weights. This encourages the model to have a sparse representation, which can be beneficial in preventing overfitting.\n",
    "   - **L2 Regularization**: By adding an L2 penalty to the loss function, the model's weights are prevented from growing excessively large, a common symptom leading to overfitting.\n",
    "\n",
    "### 2. **Dropout Rate Adjustment**:\n",
    "While the current dropout rate stands at `0.50`, a meticulous analysis of validation results can guide its fine-tuning:\n",
    "   - **Increase Rate**: If overfitting persists, consider increasing the dropout rate to introduce higher regularization.\n",
    "   - **Decrease Rate**: In scenarios where the model underfits, reducing the dropout rate may allow the model to capture more intricate patterns.\n",
    "\n",
    "### 3. **Model Simplification**:\n",
    "   - **Architecture Refinement**: Evaluate the necessity of each layer in the model. Removing superfluous layers or neurons can lead to a simpler model less prone to overfitting.\n",
    "   - **Feature Selection**: Limiting the input features to only the most informative ones can reduce the complexity of the decision boundary, potentially improving generalization.\n",
    "\n",
    "### 4. **Hyperparameter Optimization**:\n",
    "   - **Grid Search**: Implement a systematic exploration of hyperparameters by testing all possible combinations within a predefined range.\n",
    "   - **Random Search**: Instead of exhaustively searching all combinations, random search samples hyperparameters from a distribution over possible parameter values, offering a more efficient search strategy.\n",
    "   - **Bayesian Optimization**: This probabilistic model-based approach seeks to find the hyperparameter combination that gives the maximum value of the target function, making it a robust choice for hyperparameter tuning.\n",
    "\n",
    "### 5. **Expanding the Dataset**:\n",
    "While not a direct model adjustment, obtaining more diverse data points or leveraging external datasets can assist the model in better generalization.\n",
    "\n",
    "In light of the aforementioned strategies, continuous monitoring, testing, and iterative refinement will be pivotal in achieving optimal model performance on unseen data.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dfe4d17fd247042"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
