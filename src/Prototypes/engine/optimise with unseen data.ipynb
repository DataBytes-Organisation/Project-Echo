{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Optimised Model Testing\n",
    "\n",
    "Use this code to test the audio classification model that's been trained using an optimised pipeline. This modified version allows for testing with any type of audio input.\n",
    "\n",
    "## **Usage**:\n",
    "\n",
    "1. **Input Your Audio**: Provide the desired audio segment for classification.\n",
    "2. **Execute the Code**: Run the code, which will classify the provided audio segment.\n",
    "3. **Check Results**: The model will then output the top two probable classifications for the given audio.\n",
    "\n",
    "## **Author**:\n",
    "Rohit Dhanda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import base64\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "# Make sure to use the correct configuration\n",
    "config = {\n",
    "    'AUDIO_SAMPLE_RATE': 48000,\n",
    "    'AUDIO_CLIP_DURATION': 1,\n",
    "    'AUDIO_NFFT': 2048,\n",
    "    'AUDIO_STRIDE': 200,\n",
    "    'AUDIO_MELS': 260,\n",
    "    'AUDIO_FMIN': 20,\n",
    "    'AUDIO_FMAX': 13000,\n",
    "    'AUDIO_WINDOW': None,\n",
    "    'AUDIO_TOP_DB': 80,\n",
    "    'MODEL_INPUT_IMAGE_CHANNELS': 3,\n",
    "    'MODEL_INPUT_IMAGE_WIDTH': 260,\n",
    "    'MODEL_INPUT_IMAGE_HEIGHT': 260\n",
    "}\n",
    "\n",
    "\n",
    "class_names= ['Aegotheles Cristatus', 'Alauda Arvensis', 'Caligavis Chrysops', 'Capra Hircus', 'Cervus Unicolour', 'Colluricincla Harmonica', 'Corvus Coronoides',\n",
    "              'Dama Dama', 'Eopsaltria Australis', 'Felis Catus', 'Pachycephala Rufiventris', 'Ptilotula Penicillata', 'Rattus Norvegicus', 'Strepera Graculina', 'Sus Scrofa']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('models/echo_model/2/')\n",
    "\n",
    "# Define the preprocessing steps as functions.\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "    # this function is adapted from generic_engine_pipeline.ipynb\n",
    "    # TODO: need to create a pipeline library and link same code into engine\n",
    "    ########################################################################################\n",
    "def combined_pipeline(config, audio_clip):\n",
    "\n",
    "    # Load the audio data with librosa(works only while give direct audio to it)\n",
    "    audio_clip, sample_rate = librosa.load(audio_clip, sr=config['AUDIO_SAMPLE_RATE'])\n",
    "    \n",
    "    #to use it with yamnet\n",
    "    #file = io.BytesIO(audio_clip)\n",
    "    #audio_clip, sample_rate = librosa.load(file, sr=config['AUDIO_SAMPLE_RATE'])\n",
    "        \n",
    "    # keep right channel only\n",
    "    if audio_clip.ndim == 2 and audio_clip.shape[0] == 2:\n",
    "        audio_clip = audio_clip[1, :]\n",
    "        \n",
    "    # cast to float32 type\n",
    "    audio_clip = audio_clip.astype(np.float32)\n",
    "        \n",
    "    # analyse a random 5 second subsection\n",
    "    audio_clip = load_random_subsection(audio_clip, duration_secs=config['AUDIO_CLIP_DURATION'])\n",
    "\n",
    "    # Compute the mel-spectrogram\n",
    "    image = librosa.feature.melspectrogram(\n",
    "        y=audio_clip, \n",
    "        sr=config['AUDIO_SAMPLE_RATE'], \n",
    "        n_fft=config['AUDIO_NFFT'], \n",
    "        hop_length=config['AUDIO_STRIDE'], \n",
    "        n_mels=config['AUDIO_MELS'],\n",
    "        fmin=config['AUDIO_FMIN'],\n",
    "        fmax=config['AUDIO_FMAX'],\n",
    "        win_length=config['AUDIO_WINDOW'])\n",
    "\n",
    "    # Optionally convert the mel-spectrogram to decibel scale\n",
    "    image = librosa.power_to_db(\n",
    "        image, \n",
    "        top_db=config['AUDIO_TOP_DB'], \n",
    "        ref=1.0)\n",
    "        \n",
    "    # Calculate the expected number of samples in a clip\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "        \n",
    "    # swap axis and clip to expected samples to avoid rounding errors\n",
    "    image = np.moveaxis(image, 1, 0)\n",
    "    image = image[0:expected_clip_samples,:]\n",
    "        \n",
    "    # reshape into standard 3 channels to add the color channel\n",
    "    image = tf.expand_dims(image, -1)\n",
    "        \n",
    "    # most pre-trained model classifer model expects 3 color channels\n",
    "    image = tf.repeat(image, config['MODEL_INPUT_IMAGE_CHANNELS'], axis=2)\n",
    "        \n",
    "    # calculate the image shape and ensure it is correct\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "    image = tf.ensure_shape(image, [expected_clip_samples, config['AUDIO_MELS'], config['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "        \n",
    "    # note here a high quality LANCZOS5 is applied to resize the image to match model image input size\n",
    "    image = tf.image.resize(image, (config['MODEL_INPUT_IMAGE_WIDTH'], config['MODEL_INPUT_IMAGE_HEIGHT']), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "\n",
    "\n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.0000001)\n",
    "        \n",
    "    return image, audio_clip, sample_rate\n",
    "\n",
    "\n",
    "\n",
    " ########################################################################################\n",
    "    # Function to predict class and probability given a prediction\n",
    "    ########################################################################################\n",
    "def predict_class( predictions):\n",
    "    # Get the index of the class with the highest predicted probability\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    print(predicted_index, type(predicted_index))\n",
    "\n",
    "    # Get the class name using the predicted index\n",
    "    predicted_class = self.class_names[predicted_index]\n",
    "    # Calculate the predicted probability for the selected class\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[predicted_index].numpy()\n",
    "    # Round the probability to 2 decimal places\n",
    "    predicted_probability = round(predicted_probability, 2)\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "# this method takes in binary audio data and encodes to string\n",
    "def audio_to_string( audio_binary):\n",
    "    base64_encoded_data = base64.b64encode(audio_binary)\n",
    "    base64_message = base64_encoded_data.decode('utf-8')\n",
    "    return base64_message    \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "    # this method takes in string and ecodes to audio binary data\n",
    "#############################################################################################\n",
    "def string_to_audio( audio_string):\n",
    "    base64_img_bytes = audio_string.encode('utf-8')\n",
    "    decoded_data = base64.decodebytes(base64_img_bytes)\n",
    "    return decoded_data\n",
    "    \n",
    "def predict_class(predictions):\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    predicted_class = class_names[predicted_index]\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[0, predicted_index].numpy()\n",
    "    predicted_probability = round(predicted_probability, 2)\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_random_subsection(sample, duration_secs):\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Determine the audio file's duration in samples\n",
    "    audio_duration_samples = tf.shape(sample)[0]\n",
    "        \n",
    "        # Determine the required padding length to reach 1-second duration\n",
    "    padding_length = tf.maximum(0, duration_secs * config['AUDIO_SAMPLE_RATE'] - audio_duration_samples)\n",
    "        \n",
    "        # If padding_length is zero or negative, clip the audio to the desired length\n",
    "    if padding_length <= 0:\n",
    "        sample = sample[:duration_secs * config['AUDIO_SAMPLE_RATE']]\n",
    "    else:\n",
    "            # Apply padding if necessary (if padding_length is zero, this will not affect the sample)\n",
    "        padding = tf.zeros([padding_length], dtype=sample.dtype)\n",
    "        sample = tf.concat([sample, padding], axis=0)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_on_audio(audio_binary):\n",
    "    # Preprocess the audio to be suitable for your model\n",
    "    image, audio_clip, sample_rate = combined_pipeline(config, audio_binary)\n",
    "    \n",
    "    # Add a dimension to match the model's input shape\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    # Make the prediction\n",
    "    predictions_array = model.predict(image)[0]  # Assuming the model returns 2D array, take the first element\n",
    "    \n",
    "    # Pair the class names with the predictions\n",
    "    paired_predictions = list(zip(class_names, predictions_array))\n",
    "    \n",
    "    # Sort the paired predictions based on probability\n",
    "    sorted_predictions = sorted(paired_predictions, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_predictions[:2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 331ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Felis Catus', 21.47073), ('Corvus Coronoides', 2.9087145)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_on_audio('/Users/ankush/Downloads/deakin-units/data/cleaned/Felis Catus/cat_30-0_chunk-0_segment-5.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
