{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "Author: stephankokkas\n",
    "\n",
    "This notebook defines a pipeline that tasks an input directory of audio files and converts them to images using mel-spectrogram transofrmation and preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version     :  3.7.3\n",
      "TensorFlow Version :  2.10.0\n"
     ]
    }
   ],
   "source": [
    "# disable warnings to tidy up output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# some basic libraries \n",
    "from platform import python_version\n",
    "#import pandas as pd\n",
    "#import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "import pandas as pd\n",
    "\n",
    "# plot support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow support\n",
    "import tensorflow as tf\n",
    "#import tensorflow_transform as tft\n",
    "import tensorflow_io as tfio\n",
    "#from tensorflow.contrib.framework.python.ops import audio_ops\n",
    "\n",
    "# scipy\n",
    "import scipy\n",
    "from pydub import AudioSegment, effects\n",
    "\n",
    "# turn off tensorflow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# turn off absl warnings\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "# print system information\n",
    "print('Python Version     : ', python_version())\n",
    "print('TensorFlow Version : ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code adapted from:\n",
    "# https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(123)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "random.seed(123)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vram limit set to 5120MB\n"
     ]
    }
   ],
   "source": [
    "def enforce_memory_limit(mem_mb):\n",
    "  # enforce memory limit on GPU\n",
    "\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if gpus:\n",
    "    try:\n",
    "      tf.config.experimental.set_virtual_device_configuration(\n",
    "          gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=mem_mb)])\n",
    "      print(f\"vram limit set to {mem_mb}MB\")\n",
    "    except RuntimeError as e:\n",
    "      print(e)\n",
    "      \n",
    "# enforce max 5GB memory on GPU for this notebook\n",
    "enforce_memory_limit(5120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "This pipeline will go through a root directory and find all the audio files that exist and are of accepted format. Then, depending on the params set, it with normalise, trim and split the data. Please ensure you specify the self.DATASET_PATH with the directory of the data, and the self._SET_OUTPUT_DIR with the location you want the output files to end up in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class raw_file_pre_processing():\n",
    "    def __init__(self) -> None:\n",
    "        self.CLIP_LENGTH   = 5000   # only look at 5000 milliseconds of clip at the start of loaded audio file\n",
    "        self.BITRATE = \"160k\"        # all the samples are converted to bit rate of 32000 (Samples/Second)\n",
    "        self.labels = []\n",
    "        self.raw_dirs = {}\n",
    "        self.dataset = pd.DataFrame(columns=['Label', 'FileName', 'FileType', 'Directory'])\n",
    "        self.TARGET_FORMAT = 'flac'\n",
    "        self.ACCEPTED_FORMAT = ['.mp3', '.flac', '.aiff', '.mp4', '.m4a', '.wav', '.ogg']\n",
    "        self.CLEAN_DIR_ = True\n",
    "        self.TRAIN_SPLIT      = 0.80\n",
    "        self.TEST_SPLIT       = 0.01\n",
    "        self.VALIDATION_SPLIT = 0.19\n",
    "        self.split_file_names = {}\n",
    "\n",
    "        # WIN\n",
    "        #self.DATASET_PATH = 'C:\\\\Users\\\\steph\\\\Documents\\\\birdclef2022\\\\'\n",
    "        # MAC\n",
    "        self.DATASET_PATH  = 'D:\\\\Data\\\\b3\\\\'\n",
    "        \n",
    "        # make sure this is in the same format for either window or mac\n",
    "        #WIN\n",
    "        #self._SET_OUTPUT_DIR = 'C:\\\\Users\\\\steph\\\\Downloads\\\\'\n",
    "        #MAC\n",
    "        self._SET_OUTPUT_DIR = 'D:\\\\Data\\\\b3_out\\\\'\n",
    "        \n",
    "        \n",
    "        if '/' in self.DATASET_PATH:\n",
    "            self.DATASET_PATH = os.path.join('/', *self.DATASET_PATH.split('/'))\n",
    "            self.OUTPUT_DIR = os.path.join('/', *self._SET_OUTPUT_DIR.split('/'), 'OUTPUT_raw_flac')\n",
    "            if not os.path.exists(self.OUTPUT_DIR):\n",
    "                os.mkdir(self.OUTPUT_DIR)\n",
    "        elif '\\\\' in str(self.DATASET_PATH):\n",
    "            self.DATASET_PATH = str(os.path.join(*self.DATASET_PATH.split('\\\\'))).replace(':', ':\\\\')\n",
    "            self.OUTPUT_DIR = str(os.path.join(*self._SET_OUTPUT_DIR.split('\\\\'), 'OUTPUT_raw_flac')).replace(':', ':\\\\')\n",
    "            if not os.path.exists(self.OUTPUT_DIR):\n",
    "                os.makedirs(self.OUTPUT_DIR)   \n",
    "                \n",
    "\n",
    "        self.TRAIN_DIR = os.path.join(str(self.OUTPUT_DIR).replace('OUTPUT_raw_flac', ''), 'TRAIN_raw_flac')\n",
    "        self.TEST_DIR = os.path.join(str(self.OUTPUT_DIR).replace('OUTPUT_raw_flac', ''), 'TEST_raw_flac')\n",
    "        self.VALIDATION_DIR = os.path.join(str(self.OUTPUT_DIR).replace('OUTPUT_raw_flac', ''), 'VALIDATION_raw_flac')\n",
    "\n",
    "        def clean_dir(self):\n",
    "            if os.path.exists(self.OUTPUT_DIR):\n",
    "                shutil.rmtree(self.OUTPUT_DIR)\n",
    "            if os.path.exists(self.TRAIN_DIR):\n",
    "                shutil.rmtree(self.TRAIN_DIR)\n",
    "            if os.path.exists(self.TEST_DIR):\n",
    "                shutil.rmtree(self.TEST_DIR)\n",
    "            if os.path.exists(self.VALIDATION_DIR):\n",
    "                shutil.rmtree(self.VALIDATION_DIR)\n",
    "        if self.CLEAN_DIR_: \n",
    "            clean_dir(self)\n",
    "\n",
    "    def handle_duplicate_files(self) -> bool:\n",
    "        try:\n",
    "            for root, dir, files in os.walk(self.DATASET_PATH):\n",
    "                for _class_ in dir:\n",
    "                    _file_ = [f.split('.')[0] for f in listdir(os.path.join(root, _class_)) if isfile(join(os.path.join(root, _class_), f))]\n",
    "                    _set_ = set([x for x in _file_ if _file_.count(x) > 1])\n",
    "\n",
    "                    if len(_set_) > 0:\n",
    "                        print(f'Class {_class_} has the following duplicates: {_set_}. Will remove dupliactes...')\n",
    "                        for _elem_ in _set_:\n",
    "                            for dir_file in [f for f in listdir(os.path.join(root, _class_)) if isfile(join(os.path.join(root, _class_), f))]:\n",
    "                                if _elem_ in dir_file:\n",
    "                                    os.remove(os.path.join(root, _class_, dir_file))      \n",
    "                break\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Terminating...')\n",
    "            return False\n",
    "                    \n",
    "    def get_raw_file_paths(self):\n",
    "        if self.handle_duplicate_files():\n",
    "            print(f'Looking for files... acceptable formats include: {self.ACCEPTED_FORMAT}')\n",
    "            for root, dir, files in os.walk(self.DATASET_PATH):\n",
    "                if dir == []:\n",
    "                    tmp_lable = os.path.split(root)[-1]\n",
    "                    tmp_file_dir = []\n",
    "                    tmp_filename_ = []\n",
    "                    for file in files:\n",
    "                        for ext in self.ACCEPTED_FORMAT:\n",
    "                            if ext in str(file):\n",
    "                                tmp_file_dir.append(os.path.join(root, file))\n",
    "                                tmp_filename_.append(str(str(os.path.split(os.path.join(root, file))[-1]).split('.')[0]).replace('_', ''))\n",
    "\n",
    "                            \n",
    "                    self.raw_dirs.update({tmp_lable:tmp_file_dir})\n",
    "                    self.split_file_names.update({tmp_lable:tmp_filename_})\n",
    "\n",
    "            for key in self.raw_dirs:\n",
    "                print(f'FOUND: {key} -> {len(self.raw_dirs[key])}')\n",
    "\n",
    "    def audio_preprocessing(self, TRIM_AUDIO:bool = False, \n",
    "                                  NORM_AUDIO:bool = False):\n",
    "        print('\\nConvering audio files....\\n')\n",
    "        if not os.path.exists(self.OUTPUT_DIR):\n",
    "            os.makedirs(self.OUTPUT_DIR)\n",
    "\n",
    "        \n",
    "        for key, item in self.split_file_names.items():\n",
    "            train = item[int(len(item) * .00) : int(len(item) * self.TRAIN_SPLIT)]\n",
    "            vali = item[int(len(item) * self.TRAIN_SPLIT) : int(len(item) * (self.TRAIN_SPLIT + self.VALIDATION_SPLIT))]\n",
    "            test = item[int(len(item) * (self.TRAIN_SPLIT + self.VALIDATION_SPLIT)) : int(len(item) * 1.00)]\n",
    "            self.split_file_names.update({key: [train, vali, test]})\n",
    "\n",
    "        for key, item in self.raw_dirs.items():\n",
    "            print(f'Converting {key} data ->> ...')\n",
    "            tmp_dir_key = os.path.join(self.OUTPUT_DIR, key)\n",
    "            if not os.path.exists(tmp_dir_key):\n",
    "                os.makedirs(tmp_dir_key)\n",
    "\n",
    "            for dir in item:\n",
    "                try:\n",
    "                    # read file\n",
    "                    tmp_file_name = str(os.path.split(dir)[-1].split('.')[0]).replace('_', '')\n",
    "                    raw_sound = AudioSegment.from_file(dir, format=dir.split('.')[-1])\n",
    "\n",
    "                    if NORM_AUDIO:\n",
    "                        # normalise file\n",
    "                        raw_sound = effects.normalize(raw_sound)\n",
    "\n",
    "                    # trim file\n",
    "                    if TRIM_AUDIO:\n",
    "                        arr_split_file = [raw_sound[idx:idx + self.CLIP_LENGTH] for idx in range(0, len(raw_sound), self.CLIP_LENGTH)]             \n",
    "                        for count_sample, sample in enumerate(arr_split_file):\n",
    "                            # padding audio < 5s\n",
    "                            if len(sample) < self.CLIP_LENGTH:\n",
    "                                silence = AudioSegment.silent(duration=((self.CLIP_LENGTH-len(sample))))\n",
    "                                sample = sample + silence  # Adding silence after the audio\n",
    "\n",
    "                            # export raw file\n",
    "                            tmp_raw_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_raw_trim_sample_' + str(count_sample) + '.' + self.TARGET_FORMAT)\n",
    "                            sample.export(tmp_raw_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE, parameters = [])\n",
    "\n",
    "                            new_row = pd.Series({\"Label\": key,\n",
    "                                        \"FileName\": tmp_file_name + '_raw_trim_sample_' + str(count_sample) + '.' + self.TARGET_FORMAT,\n",
    "                                        \"FileType\": self.TARGET_FORMAT,\n",
    "                                        \"Directory\": tmp_raw_new_dir})\n",
    "                            self.dataset = pd.concat([self.dataset, new_row.to_frame().T], ignore_index=True)\n",
    "                    else:\n",
    "                        tmp_raw_new_dir = os.path.join(tmp_dir_key, tmp_file_name + '_raw_' + '.' + self.TARGET_FORMAT)\n",
    "                        raw_sound.export(tmp_raw_new_dir, format=self.TARGET_FORMAT, bitrate=self.BITRATE, parameters = [])\n",
    "\n",
    "                        new_row = pd.Series({\"Label\": key,\n",
    "                                    \"FileName\": tmp_file_name + '_raw_' + '.' + self.TARGET_FORMAT,\n",
    "                                    \"FileType\": self.TARGET_FORMAT,\n",
    "                                    \"Directory\": tmp_raw_new_dir})\n",
    "                        self.dataset = pd.concat([self.dataset, new_row.to_frame().T], ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    def train_test_split_fun(self):\n",
    "        print(f'\\nSplitting data into sub-directories Train, Test and Validation...')\n",
    "\n",
    "        if not os.path.exists(self.TRAIN_DIR):\n",
    "            os.mkdir(self.TRAIN_DIR)\n",
    "        if not os.path.exists(self.TEST_DIR):\n",
    "            os.mkdir(self.TEST_DIR)\n",
    "        if not os.path.exists(self.VALIDATION_DIR):\n",
    "            os.mkdir(self.VALIDATION_DIR)\n",
    "\n",
    "\n",
    "        dict_keys = self.dataset['Label'].value_counts().to_dict()\n",
    "        for key, item in dict_keys.items():\n",
    "            if not os.path.exists(os.path.join(self.TRAIN_DIR, key)):\n",
    "                os.mkdir(os.path.join(self.TRAIN_DIR, key))\n",
    "            if not os.path.exists(os.path.join(self.TEST_DIR, key)):\n",
    "                os.mkdir(os.path.join(self.TEST_DIR, key))\n",
    "            if not os.path.exists(os.path.join(self.VALIDATION_DIR, key)):\n",
    "                os.mkdir(os.path.join(self.VALIDATION_DIR, key))\n",
    "\n",
    "\n",
    "        # self.dataset.to_csv(os.path.join(self.OUTPUT_DIR, 'raw_files.csv'))\n",
    "\n",
    "        for index, row in self.dataset.iterrows():\n",
    "            i = str(row.Directory)\n",
    "            for key, item in self.split_file_names.items():\n",
    "                if str(row.FileName).split('_')[0] in item[0]:\n",
    "                    os.replace(i, i.replace('OUTPUT_raw_flac', 'TRAIN_raw_flac'))\n",
    "                if str(row.FileName).split('_')[0] in item[1]:\n",
    "                    os.replace(i, i.replace('OUTPUT_raw_flac', 'VALIDATION_raw_flac'))\n",
    "                if str(row.FileName).split('_')[0] in item[2]:\n",
    "                    os.replace(i, i.replace('OUTPUT_raw_flac', 'TEST_raw_flac'))\n",
    "\n",
    "        shutil.rmtree(self.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files... acceptable formats include: ['.mp3', '.flac', '.aiff', '.mp4', '.m4a', '.wav', '.ogg']\n",
      "FOUND: Acanthagenys rufogularis -> 7\n",
      "FOUND: Acanthiza apicalis -> 5\n",
      "FOUND: Acanthiza chrysorrhoa -> 13\n",
      "FOUND: Acanthiza lineata -> 5\n",
      "FOUND: Acanthiza nana -> 22\n",
      "FOUND: Acanthiza pusilla -> 21\n",
      "FOUND: Acanthiza reguloides -> 23\n",
      "FOUND: Acanthiza uropygialis -> 2\n",
      "FOUND: Acanthorhynchus tenuirostris -> 9\n",
      "FOUND: Accipiter cirrocephalus -> 2\n",
      "FOUND: Accipiter fasciatus -> 3\n",
      "FOUND: Accipiter cooperi -> 1\n",
      "FOUND: Acrocephalus australis -> 20\n",
      "FOUND: Aegotheles cristatus -> 73\n",
      "FOUND: Aidemosyne modesta -> 14\n",
      "FOUND: Alauda arvensis European Skylark -> 54\n",
      "FOUND: Alisterus scapularis -> 11\n",
      "FOUND: Amytornis purnelli -> 1\n",
      "FOUND: Anas superciliosa -> 5\n",
      "FOUND: Anhinga novaehollandiae -> 2\n",
      "FOUND: Anseranas semipalmata -> 3\n",
      "FOUND: Anthochaera carunculata -> 47\n",
      "FOUND: Anthochaera chrysoptera -> 17\n",
      "FOUND: Anthochaera phrygia -> 139\n",
      "FOUND: Anthus novaeseelandiae -> 7\n",
      "FOUND: Antigone antigone -> 1\n",
      "FOUND: Antigone rubicunda -> 1\n",
      "FOUND: Aphelocephala leucopsis -> 4\n",
      "FOUND: Apocrita sp -> 1\n",
      "FOUND: Aprosmictus erythropterus -> 1\n",
      "FOUND: Aquila audax -> 8\n",
      "FOUND: Artamus cinereus -> 2\n",
      "FOUND: Artamus cyanopterus -> 4\n",
      "FOUND: Artamus minor -> 2\n",
      "FOUND: Artamus superciliosus -> 14\n",
      "FOUND: Atrapsalta encaustica -> 4\n",
      "FOUND: Auscala spinosa -> 1\n",
      "FOUND: Austrochaperina pluvialis -> 1\n",
      "FOUND: Austronomus australis -> 6\n",
      "FOUND: Aves sp -> 1\n",
      "FOUND: Aythya australis -> 1\n",
      "FOUND: Barnardius zonarius -> 3\n",
      "FOUND: Biziura lobata -> 2\n",
      "FOUND: Burhinus grallarius -> 1\n",
      "FOUND: Cacatua galerita -> 11\n",
      "FOUND: Cacatua sanguinea -> 8\n",
      "FOUND: Cacomantis flabelliformis -> 22\n",
      "FOUND: Cacomantis variolosus -> 4\n",
      "FOUND: Caelifera sp -> 8\n",
      "FOUND: Caligavis chrysops -> 55\n",
      "FOUND: Callocephalon fimbriatum -> 8\n",
      "FOUND: Calyptorhynchus banksii -> 3\n",
      "FOUND: Calyptorhynchus lathami -> 11\n",
      "FOUND: Canis familiaris -> 2\n",
      "FOUND: Capra hircus Feral goat -> 31\n",
      "FOUND: Carduelis carduelis -> 1\n",
      "FOUND: Carterornis leucotis -> 1\n",
      "FOUND: Centropus phasianinus -> 2\n",
      "FOUND: Cervus unicolour Sambar deer -> 6\n",
      "FOUND: Ceyx azureus -> 2\n",
      "FOUND: Chalcites basalis -> 11\n",
      "FOUND: Chalcites lucidus -> 9\n",
      "FOUND: Chalcites osculans -> 8\n",
      "FOUND: Chelapsalta puer -> 1\n",
      "FOUND: Chenonetta jubata -> 7\n",
      "FOUND: Cherax destructor -> 1\n",
      "FOUND: Chlamydera guttata -> 1\n",
      "FOUND: Chlamydera nuchalis -> 3\n",
      "FOUND: Cicadidae sp -> 7\n",
      "FOUND: Cincloramphus mathewsi -> 17\n",
      "FOUND: Cinclosoma cinnamomeum -> 1\n",
      "FOUND: Cinclosoma punctatum -> 2\n",
      "FOUND: Cisticola exilis -> 1\n",
      "FOUND: Climacteris erythrops -> 3\n",
      "FOUND: Climacteris picumnus -> 12\n",
      "FOUND: Colluricincla harmonica -> 75\n",
      "FOUND: Colluricincla megarhyncha -> 7\n",
      "FOUND: Colluricincla woodwardi -> 4\n",
      "FOUND: Conocephalus albescens -> 5\n",
      "FOUND: Conocephalus semivittatus -> 3\n",
      "FOUND: Conocephalus upoluensis -> 13\n",
      "FOUND: Conocephalus sp -> 6\n",
      "FOUND: Conopophila albogularis -> 2\n",
      "FOUND: Cophixalus bombiens -> 1\n",
      "FOUND: Cophixalus exiguus -> 1\n",
      "FOUND: Cophixalus infacetus -> 1\n",
      "FOUND: Cophixalus ornatus -> 5\n",
      "FOUND: Coracina novaehollandiae -> 6\n",
      "FOUND: Coracina papuensis -> 4\n",
      "FOUND: Corcorax melanorhamphos -> 18\n",
      "FOUND: Cormobates leucophaea -> 38\n",
      "FOUND: Corvus bennetti -> 1\n",
      "FOUND: Corvus coronoides -> 55\n",
      "FOUND: Corvus mellori -> 12\n",
      "FOUND: Corvus orru -> 7\n",
      "FOUND: Coturnix pectoralis -> 1\n",
      "FOUND: Cracticus nigrogularis -> 44\n",
      "FOUND: Cracticus torquatus -> 46\n",
      "FOUND: Crinia signifera -> 30\n",
      "FOUND: Cygnus atratus -> 1\n",
      "FOUND: Dacelo leachii -> 2\n",
      "FOUND: Dacelo novaeguineae -> 51\n",
      "FOUND: Dama dama Fallow Deer -> 33\n",
      "FOUND: Daphoenositta chrysoptera -> 9\n",
      "FOUND: Dendrocygna arcuata -> 4\n",
      "FOUND: Dicaeum hirundinaceum -> 22\n",
      "FOUND: Dicrurus bracteatus -> 6\n",
      "FOUND: Ducula spilorrhoa -> 1\n",
      "FOUND: Edolisoma tenuirostre -> 3\n",
      "FOUND: Egretta novaehollandiae -> 3\n",
      "FOUND: Elanus axillaris -> 1\n",
      "FOUND: Elseyornis melanops -> 10\n",
      "FOUND: Entomyzon cyanotis -> 3\n",
      "FOUND: Eolophus roseicapilla -> 10\n",
      "FOUND: Eopsaltria australis -> 69\n",
      "FOUND: Ephippitytha trigintiduoguttata -> 1\n",
      "FOUND: Eudynamys orientalis -> 3\n",
      "FOUND: Eurostopodus argus -> 2\n",
      "FOUND: Eurostopodus mystacalis -> 18\n",
      "FOUND: Eurystomus orientalis -> 4\n",
      "FOUND: Falco berigora -> 4\n",
      "FOUND: Falco cenchroides -> 1\n",
      "FOUND: Falco peregrinus -> 1\n",
      "FOUND: Falcunculus frontatus -> 10\n",
      "FOUND: Froggattina australis -> 17\n",
      "FOUND: Fulica atra -> 9\n",
      "FOUND: Galanga labeculata -> 2\n",
      "FOUND: Gallinula tenebrosa -> 9\n",
      "FOUND: Gavicalis virescens -> 4\n",
      "FOUND: Geopelia cuneata -> 1\n",
      "FOUND: Geopelia humeralis -> 6\n",
      "FOUND: Geopelia placida -> 30\n",
      "FOUND: Geophaps plumifera -> 2\n",
      "FOUND: Gerygone fusca -> 12\n",
      "FOUND: Gerygone mouki -> 5\n",
      "FOUND: Gerygone olivacea -> 14\n",
      "FOUND: Glossopsitta concinna -> 21\n",
      "FOUND: Grallina cyanoleuca -> 8\n",
      "FOUND: Grantiella picta -> 2\n",
      "FOUND: Gryllidae sp -> 57\n",
      "FOUND: Gryllotalpidae sp -> 13\n",
      "FOUND: Gymnorhina tibicen -> 65\n",
      "FOUND: Haliastur sphenurus -> 2\n",
      "FOUND: Henicopsaltria eydouxii -> 8\n",
      "FOUND: Heteroscenes pallidus -> 17\n",
      "FOUND: Hieraaetus morphnoides -> 7\n",
      "FOUND: Himantopus himantopus -> 1\n",
      "FOUND: Hirundo neoxena -> 11\n",
      "FOUND: Hylacola pyrrhopygia -> 1\n",
      "FOUND: Irediparra gallinacea -> 1\n",
      "FOUND: Iridomyrmex purpureus -> 1\n",
      "FOUND: Lalage leucomela -> 3\n",
      "FOUND: Lalage tricolor -> 3\n",
      "FOUND: Lathamus discolor -> 2\n",
      "FOUND: Lepus europaeus brown hare -> 2\n",
      "FOUND: Leucosarcia melanoleuca -> 6\n",
      "FOUND: Lichenostomus melanops -> 1\n",
      "FOUND: Lichmera indistincta -> 4\n",
      "FOUND: Limnodynastes dumerilii -> 23\n",
      "FOUND: Limnodynastes peronii -> 15\n",
      "FOUND: Limnodynastes tasmaniensis -> 20\n",
      "FOUND: Litoria caerulea -> 31\n",
      "FOUND: Litoria gracilenta -> 1\n",
      "FOUND: Litoria inermis -> 1\n",
      "FOUND: Litoria latopalmata -> 9\n",
      "FOUND: Litoria myola -> 25\n",
      "FOUND: Litoria peronii -> 15\n",
      "FOUND: Litoria serrata -> 110\n",
      "FOUND: Litoria verreauxii -> 7\n",
      "FOUND: Litoria xanthomera -> 1\n",
      "FOUND: Lophochroa leadbeateri -> 1\n",
      "FOUND: Macropus giganteus -> 8\n",
      "FOUND: Malacostraca sp -> 1\n",
      "FOUND: Malurus coronatus -> 2\n",
      "FOUND: Malurus cyaneus -> 44\n",
      "FOUND: Malurus lamberti -> 3\n",
      "FOUND: Malurus leucopterus -> 1\n",
      "FOUND: Malurus splendens -> 1\n",
      "FOUND: Manorina flavigula -> 1\n",
      "FOUND: Manorina melanocephala -> 36\n",
      "FOUND: Manorina melanophrys -> 6\n",
      "FOUND: Megapodius reinwardt -> 2\n",
      "FOUND: Melanodryas cucullata -> 17\n",
      "FOUND: Meliphaga lewinii -> 10\n",
      "FOUND: Melithreptus albogularis -> 5\n",
      "FOUND: Melithreptus brevirostris -> 11\n",
      "FOUND: Melithreptus gularis -> 11\n",
      "FOUND: Melithreptus lunatus -> 11\n",
      "FOUND: Melopsittacus undulatus -> 4\n",
      "FOUND: Menura alberti -> 7\n",
      "FOUND: Menura novaehollandiae -> 207\n",
      "FOUND: Merops ornatus -> 3\n",
      "FOUND: Microchiroptera sp -> 18\n",
      "FOUND: Microeca fascinans -> 19\n",
      "FOUND: Microeca flavigaster -> 8\n",
      "FOUND: Milvus migrans -> 1\n",
      "FOUND: Mirafra javanica -> 4\n",
      "FOUND: Mus musculus House mouse -> 4\n",
      "FOUND: Myiagra alecto -> 2\n",
      "FOUND: Myiagra cyanoleuca -> 9\n",
      "FOUND: Myiagra inquieta -> 21\n",
      "FOUND: Myiagra rubecula -> 23\n",
      "FOUND: Myzomela erythrocephala -> 1\n",
      "FOUND: Myzomela sanguinolenta -> 20\n",
      "FOUND: Neochmia phaeton -> 4\n",
      "FOUND: Neochmia temporalis -> 8\n",
      "FOUND: Neophema pulchella -> 8\n",
      "FOUND: Neosericornis citreogularis -> 3\n",
      "FOUND: Nesoptilotis leucotis -> 10\n",
      "FOUND: Nettapus pulchellus -> 1\n",
      "FOUND: Ninox boobook -> 22\n",
      "FOUND: Ninox connivens -> 11\n",
      "FOUND: Ninox strenua -> 1\n",
      "FOUND: Notamacropus rufogriseus -> 8\n",
      "FOUND: Notopsaltra atrata -> 1\n",
      "FOUND: Nycticorax caledonicus -> 1\n",
      "FOUND: Nymphicus hollandicus -> 1\n",
      "FOUND: Ocyphaps lophotes -> 9\n",
      "FOUND: Oreoica gutturalis -> 2\n",
      "FOUND: Origma solitaria -> 2\n",
      "FOUND: Oriolus flavocinctus -> 8\n",
      "FOUND: Oriolus sagittatus -> 21\n",
      "FOUND: Oryctolagus cuniculus European Rabbit -> 3\n",
      "FOUND: Osphranter robustus -> 7\n",
      "FOUND: Pachycephala inornata -> 6\n",
      "FOUND: Pachycephala pectoralis -> 14\n",
      "FOUND: Pachycephala rufiventris -> 76\n",
      "FOUND: Pachycephala simplex -> 5\n",
      "FOUND: Pardalotus punctatus -> 16\n",
      "FOUND: Pardalotus rubricatus -> 3\n",
      "FOUND: Pardalotus striatus -> 22\n",
      "FOUND: Parvipsitta pusilla -> 11\n",
      "FOUND: Pelecanus conspicillatus -> 1\n",
      "FOUND: Petrochelidon ariel -> 11\n",
      "FOUND: Petrochelidon nigricans -> 4\n",
      "FOUND: Petroica boodang -> 12\n",
      "FOUND: Petroica goodenovii -> 52\n",
      "FOUND: Petroica phoenicea -> 7\n",
      "FOUND: Petroica rosea -> 7\n",
      "FOUND: Phaneropterinae sp -> 8\n",
      "FOUND: Phaps chalcoptera -> 13\n",
      "FOUND: Phaps elegans -> 7\n",
      "FOUND: Phaps sp -> 2\n",
      "FOUND: Philemon argenticeps -> 5\n",
      "FOUND: Philemon buceroides -> 9\n",
      "FOUND: Philemon citreogularis -> 14\n",
      "FOUND: Philemon corniculatus -> 157\n",
      "FOUND: Phylidonyris novaehollandiae -> 5\n",
      "FOUND: Phylidonyris pyrrhopterus -> 5\n",
      "FOUND: Phylidonyris nigra -> 1\n",
      "FOUND: Pitta iris -> 1\n",
      "FOUND: Pitta versicolor -> 1\n",
      "FOUND: Platycercus adscitus -> 17\n",
      "FOUND: Platycercus elegans -> 30\n",
      "FOUND: Platyplectrum ornatum -> 5\n",
      "FOUND: Plectorhyncha lanceolata -> 8\n",
      "FOUND: Podargus strigoides -> 13\n",
      "FOUND: Poecilodryas superciliosa -> 12\n",
      "FOUND: Pomatostomus superciliosus -> 18\n",
      "FOUND: Pomatostomus temporalis -> 3\n",
      "FOUND: Poodytes gramineus -> 2\n",
      "FOUND: Porphyrio porphyrio -> 3\n",
      "FOUND: Psaltoda moerens -> 2\n",
      "FOUND: Psaltoda plaga -> 5\n",
      "FOUND: Psephotellus varius -> 2\n",
      "FOUND: Psephotus haematonotus -> 11\n",
      "FOUND: Pseudocheirus peregrinus -> 2\n",
      "FOUND: Psitteuteles versicolor -> 1\n",
      "FOUND: Psophodes cristatus -> 2\n",
      "FOUND: Psophodes nigrogularis -> 92\n",
      "FOUND: Psophodes occidentalis -> 1\n",
      "FOUND: Psophodes olivaceus -> 28\n",
      "FOUND: Pteropus scapulatus -> 8\n",
      "FOUND: Pteropus sp -> 1\n",
      "FOUND: Ptilinopus regina -> 1\n",
      "FOUND: Ptilonorhynchus violaceus -> 3\n",
      "FOUND: Ptilotula flavescens -> 5\n",
      "FOUND: Ptilotula fusca -> 50\n",
      "FOUND: Ptilotula keartlandi -> 1\n",
      "FOUND: Ptilotula penicillata -> 58\n",
      "FOUND: Purnella albifrons -> 1\n",
      "FOUND: Purpureicephalus spurius -> 1\n",
      "FOUND: Pycnoptilus floccosus -> 3\n",
      "FOUND: Pyrrholaemus brunneus -> 2\n",
      "FOUND: Pyrrholaemus sagittata -> 39\n",
      "FOUND: Radjah radjah -> 2\n",
      "FOUND: Ramsayornis fasciatus -> 1\n",
      "FOUND: Rattus norvegicus Brown rat -> 22\n",
      "FOUND: Rhinella marina -> 1\n",
      "FOUND: Rhipidura albiscapa -> 34\n",
      "FOUND: Rhipidura leucophrys -> 37\n",
      "FOUND: Rhipidura rufifrons -> 9\n",
      "FOUND: Rhipidura rufiventris -> 8\n",
      "FOUND: Scythrops novaehollandiae -> 12\n",
      "FOUND: Sericornis frontalis -> 22\n",
      "FOUND: Sericornis magnirostra -> 2\n",
      "FOUND: Sericulus chrysocephalus -> 3\n",
      "FOUND: Smicrornis brevirostris -> 49\n",
      "FOUND: Sphecotheres vieilloti -> 7\n",
      "FOUND: Spilopelia chinensis -> 4\n",
      "FOUND: Stagonopleura guttata -> 15\n",
      "FOUND: Stipiturus ruficeps -> 1\n",
      "FOUND: Stizoptera bichenovii -> 18\n",
      "FOUND: Stomiopera unicolor -> 7\n",
      "FOUND: Strepera graculina -> 68\n",
      "FOUND: Strepera versicolor -> 5\n",
      "FOUND: Struthidea cinerea -> 2\n",
      "FOUND: Sugomel nigrum -> 1\n",
      "FOUND: sus scrofa Wild pig -> 64\n",
      "FOUND: Symposiachrus trivirgatus -> 1\n",
      "FOUND: Synoicus ypsilophora -> 7\n",
      "FOUND: Tachybaptus novaehollandiae -> 7\n",
      "FOUND: Tachybaptus ruficollis -> 1\n",
      "FOUND: Taeniopygia guttata -> 8\n",
      "FOUND: Territornis albilineata -> 2\n",
      "FOUND: Tettigoniidae sp -> 5\n",
      "FOUND: Threskiornis moluccus -> 1\n",
      "FOUND: Todiramphus macleayii -> 3\n",
      "FOUND: Todiramphus pyrrhopygius -> 1\n",
      "FOUND: Todiramphus sanctus -> 7\n",
      "FOUND: Tregellasia capito -> 1\n",
      "FOUND: Trichoglossus chlorolepidotus -> 1\n",
      "FOUND: Trichoglossus haematodus -> 9\n",
      "FOUND: Trichosurus vulpecula -> 16\n",
      "FOUND: Turdus merula -> 13\n",
      "FOUND: Turnix varius -> 13\n",
      "FOUND: Tyto javanica -> 1\n",
      "FOUND: Uperoleia altissima -> 1\n",
      "FOUND: Uperoleia laevigata -> 8\n",
      "FOUND: Uperoleia lithomoda -> 1\n",
      "FOUND: Uperoleia littlejohni -> 1\n",
      "FOUND: Uperoleia mimula -> 1\n",
      "FOUND: Vanellus miles -> 9\n",
      "FOUND: Vanellus spinosus -> 1\n",
      "FOUND: Vulpes vulpes -> 1\n",
      "FOUND: vulpes vulpes red fox -> 47\n",
      "FOUND: Wallabia bicolor -> 2\n",
      "FOUND: Yoyetta tristrigata -> 2\n",
      "FOUND: Zanda funerea -> 2\n",
      "FOUND: Zanda sp -> 1\n",
      "FOUND: Zoothera heinei -> 2\n",
      "FOUND: Zoothera lunulata -> 3\n",
      "FOUND: Zosterops lateralis -> 30\n",
      "\n",
      "This next process will take approx 20 mins for the current bird dataset depending on the speed of your computer\n",
      "\n",
      "Convering audio files....\n",
      "\n",
      "Converting Acanthagenys rufogularis data ->> ...\n",
      "Converting Acanthiza apicalis data ->> ...\n",
      "Converting Acanthiza chrysorrhoa data ->> ...\n",
      "Converting Acanthiza lineata data ->> ...\n",
      "Converting Acanthiza nana data ->> ...\n",
      "Converting Acanthiza pusilla data ->> ...\n",
      "Converting Acanthiza reguloides data ->> ...\n",
      "Converting Acanthiza uropygialis data ->> ...\n",
      "Converting Acanthorhynchus tenuirostris data ->> ...\n",
      "Converting Accipiter cirrocephalus data ->> ...\n",
      "Converting Accipiter fasciatus data ->> ...\n",
      "Converting Accipiter cooperi data ->> ...\n",
      "Converting Acrocephalus australis data ->> ...\n",
      "Converting Aegotheles cristatus data ->> ...\n",
      "list index out of range\n",
      "list index out of range\n",
      "Converting Aidemosyne modesta data ->> ...\n",
      "Converting Alauda arvensis European Skylark data ->> ...\n",
      "Converting Alisterus scapularis data ->> ...\n",
      "Converting Amytornis purnelli data ->> ...\n",
      "Converting Anas superciliosa data ->> ...\n",
      "Converting Anhinga novaehollandiae data ->> ...\n",
      "Converting Anseranas semipalmata data ->> ...\n",
      "Converting Anthochaera carunculata data ->> ...\n",
      "list index out of range\n",
      "Converting Anthochaera chrysoptera data ->> ...\n",
      "Converting Anthochaera phrygia data ->> ...\n",
      "Converting Anthus novaeseelandiae data ->> ...\n",
      "Converting Antigone antigone data ->> ...\n",
      "Converting Antigone rubicunda data ->> ...\n",
      "Converting Aphelocephala leucopsis data ->> ...\n",
      "Converting Apocrita sp data ->> ...\n",
      "Converting Aprosmictus erythropterus data ->> ...\n",
      "Converting Aquila audax data ->> ...\n",
      "Converting Artamus cinereus data ->> ...\n",
      "Converting Artamus cyanopterus data ->> ...\n",
      "Converting Artamus minor data ->> ...\n",
      "Converting Artamus superciliosus data ->> ...\n",
      "Converting Atrapsalta encaustica data ->> ...\n",
      "Converting Auscala spinosa data ->> ...\n",
      "Converting Austrochaperina pluvialis data ->> ...\n",
      "Converting Austronomus australis data ->> ...\n",
      "list index out of range\n",
      "Converting Aves sp data ->> ...\n",
      "Converting Aythya australis data ->> ...\n",
      "Converting Barnardius zonarius data ->> ...\n",
      "Converting Biziura lobata data ->> ...\n",
      "Converting Burhinus grallarius data ->> ...\n",
      "Converting Cacatua galerita data ->> ...\n",
      "Converting Cacatua sanguinea data ->> ...\n",
      "Converting Cacomantis flabelliformis data ->> ...\n",
      "Converting Cacomantis variolosus data ->> ...\n",
      "Converting Caelifera sp data ->> ...\n",
      "Converting Caligavis chrysops data ->> ...\n",
      "Converting Callocephalon fimbriatum data ->> ...\n",
      "Converting Calyptorhynchus banksii data ->> ...\n",
      "Converting Calyptorhynchus lathami data ->> ...\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 0000014F096D5580] Failed to read frame size: Could not seek to 1266.\n",
      "D:\\Data\\b1\\Calyptorhynchus lathami\\X02824.mp3: Invalid argument\n",
      "\n",
      "Converting Canis familiaris data ->> ...\n",
      "Converting Capra hircus Feral goat data ->> ...\n",
      "Converting Carduelis carduelis data ->> ...\n",
      "Converting Carterornis leucotis data ->> ...\n",
      "Converting Centropus phasianinus data ->> ...\n",
      "Converting Cervus unicolour Sambar deer data ->> ...\n",
      "Converting Ceyx azureus data ->> ...\n",
      "Converting Chalcites basalis data ->> ...\n",
      "Converting Chalcites lucidus data ->> ...\n",
      "Converting Chalcites osculans data ->> ...\n",
      "Converting Chelapsalta puer data ->> ...\n",
      "Converting Chenonetta jubata data ->> ...\n",
      "Converting Cherax destructor data ->> ...\n",
      "Converting Chlamydera guttata data ->> ...\n",
      "Converting Chlamydera nuchalis data ->> ...\n",
      "Converting Cicadidae sp data ->> ...\n",
      "Converting Cincloramphus mathewsi data ->> ...\n",
      "Converting Cinclosoma cinnamomeum data ->> ...\n",
      "Converting Cinclosoma punctatum data ->> ...\n",
      "Converting Cisticola exilis data ->> ...\n",
      "Converting Climacteris erythrops data ->> ...\n",
      "Converting Climacteris picumnus data ->> ...\n",
      "Converting Colluricincla harmonica data ->> ...\n",
      "Converting Colluricincla megarhyncha data ->> ...\n",
      "Converting Colluricincla woodwardi data ->> ...\n",
      "Converting Conocephalus albescens data ->> ...\n",
      "Converting Conocephalus semivittatus data ->> ...\n",
      "Converting Conocephalus upoluensis data ->> ...\n",
      "Converting Conocephalus sp data ->> ...\n",
      "Converting Conopophila albogularis data ->> ...\n",
      "Converting Cophixalus bombiens data ->> ...\n",
      "Converting Cophixalus exiguus data ->> ...\n",
      "Converting Cophixalus infacetus data ->> ...\n",
      "Converting Cophixalus ornatus data ->> ...\n",
      "Converting Coracina novaehollandiae data ->> ...\n",
      "Converting Coracina papuensis data ->> ...\n",
      "Converting Corcorax melanorhamphos data ->> ...\n",
      "Converting Cormobates leucophaea data ->> ...\n",
      "list index out of range\n",
      "Converting Corvus bennetti data ->> ...\n",
      "Converting Corvus coronoides data ->> ...\n",
      "Converting Corvus mellori data ->> ...\n",
      "Converting Corvus orru data ->> ...\n",
      "list index out of range\n",
      "Converting Coturnix pectoralis data ->> ...\n",
      "Converting Cracticus nigrogularis data ->> ...\n",
      "Converting Cracticus torquatus data ->> ...\n",
      "list index out of range\n",
      "Converting Crinia signifera data ->> ...\n",
      "Converting Cygnus atratus data ->> ...\n",
      "Converting Dacelo leachii data ->> ...\n",
      "Converting Dacelo novaeguineae data ->> ...\n",
      "list index out of range\n",
      "Converting Dama dama Fallow Deer data ->> ...\n",
      "Converting Daphoenositta chrysoptera data ->> ...\n",
      "Converting Dendrocygna arcuata data ->> ...\n",
      "Converting Dicaeum hirundinaceum data ->> ...\n",
      "Converting Dicrurus bracteatus data ->> ...\n",
      "Converting Ducula spilorrhoa data ->> ...\n",
      "Converting Edolisoma tenuirostre data ->> ...\n",
      "Converting Egretta novaehollandiae data ->> ...\n",
      "Converting Elanus axillaris data ->> ...\n",
      "Converting Elseyornis melanops data ->> ...\n",
      "Converting Entomyzon cyanotis data ->> ...\n",
      "Converting Eolophus roseicapilla data ->> ...\n",
      "Converting Eopsaltria australis data ->> ...\n",
      "Converting Ephippitytha trigintiduoguttata data ->> ...\n",
      "Converting Eudynamys orientalis data ->> ...\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 00000214F27155C0] Failed to read frame size: Could not seek to 1278.\n",
      "D:\\Data\\b1\\Eudynamys orientalis\\X02918.mp3: Invalid argument\n",
      "\n",
      "Converting Eurostopodus argus data ->> ...\n",
      "Converting Eurostopodus mystacalis data ->> ...\n",
      "Converting Eurystomus orientalis data ->> ...\n",
      "Converting Falco berigora data ->> ...\n",
      "Converting Falco cenchroides data ->> ...\n",
      "Converting Falco peregrinus data ->> ...\n",
      "Converting Falcunculus frontatus data ->> ...\n",
      "Converting Froggattina australis data ->> ...\n",
      "Converting Fulica atra data ->> ...\n",
      "Converting Galanga labeculata data ->> ...\n",
      "Converting Gallinula tenebrosa data ->> ...\n",
      "Converting Gavicalis virescens data ->> ...\n",
      "Converting Geopelia cuneata data ->> ...\n",
      "Converting Geopelia humeralis data ->> ...\n",
      "Converting Geopelia placida data ->> ...\n",
      "Converting Geophaps plumifera data ->> ...\n",
      "Converting Gerygone fusca data ->> ...\n",
      "list index out of range\n",
      "Converting Gerygone mouki data ->> ...\n",
      "Converting Gerygone olivacea data ->> ...\n",
      "Converting Glossopsitta concinna data ->> ...\n",
      "Converting Grallina cyanoleuca data ->> ...\n",
      "Converting Grantiella picta data ->> ...\n",
      "Converting Gryllidae sp data ->> ...\n",
      "Converting Gryllotalpidae sp data ->> ...\n",
      "list index out of range\n",
      "Converting Gymnorhina tibicen data ->> ...\n",
      "Converting Haliastur sphenurus data ->> ...\n",
      "Converting Henicopsaltria eydouxii data ->> ...\n",
      "Converting Heteroscenes pallidus data ->> ...\n",
      "Converting Hieraaetus morphnoides data ->> ...\n",
      "Converting Himantopus himantopus data ->> ...\n",
      "Converting Hirundo neoxena data ->> ...\n",
      "Converting Hylacola pyrrhopygia data ->> ...\n",
      "Converting Irediparra gallinacea data ->> ...\n",
      "Converting Iridomyrmex purpureus data ->> ...\n",
      "Converting Lalage leucomela data ->> ...\n",
      "Converting Lalage tricolor data ->> ...\n",
      "Converting Lathamus discolor data ->> ...\n",
      "Converting Lepus europaeus brown hare data ->> ...\n",
      "Converting Leucosarcia melanoleuca data ->> ...\n",
      "Converting Lichenostomus melanops data ->> ...\n",
      "Converting Lichmera indistincta data ->> ...\n",
      "Converting Limnodynastes dumerilii data ->> ...\n",
      "Converting Limnodynastes peronii data ->> ...\n",
      "Converting Limnodynastes tasmaniensis data ->> ...\n",
      "Converting Litoria caerulea data ->> ...\n",
      "Converting Litoria gracilenta data ->> ...\n",
      "Converting Litoria inermis data ->> ...\n",
      "Converting Litoria latopalmata data ->> ...\n",
      "Converting Litoria myola data ->> ...\n",
      "Converting Litoria peronii data ->> ...\n",
      "Converting Litoria serrata data ->> ...\n",
      "list index out of range\n",
      "Converting Litoria verreauxii data ->> ...\n",
      "Converting Litoria xanthomera data ->> ...\n",
      "Converting Lophochroa leadbeateri data ->> ...\n",
      "Converting Macropus giganteus data ->> ...\n",
      "Converting Malacostraca sp data ->> ...\n",
      "Converting Malurus coronatus data ->> ...\n",
      "Converting Malurus cyaneus data ->> ...\n",
      "Converting Malurus lamberti data ->> ...\n",
      "Converting Malurus leucopterus data ->> ...\n",
      "Converting Malurus splendens data ->> ...\n",
      "Converting Manorina flavigula data ->> ...\n",
      "Converting Manorina melanocephala data ->> ...\n",
      "Converting Manorina melanophrys data ->> ...\n",
      "Converting Megapodius reinwardt data ->> ...\n",
      "Converting Melanodryas cucullata data ->> ...\n",
      "Converting Meliphaga lewinii data ->> ...\n",
      "Converting Melithreptus albogularis data ->> ...\n",
      "Converting Melithreptus brevirostris data ->> ...\n",
      "Converting Melithreptus gularis data ->> ...\n",
      "Converting Melithreptus lunatus data ->> ...\n",
      "list index out of range\n",
      "Converting Melopsittacus undulatus data ->> ...\n",
      "Converting Menura alberti data ->> ...\n",
      "Converting Menura novaehollandiae data ->> ...\n",
      "list index out of range\n",
      "list index out of range\n",
      "Converting Merops ornatus data ->> ...\n",
      "Converting Microchiroptera sp data ->> ...\n",
      "Converting Microeca fascinans data ->> ...\n",
      "Converting Microeca flavigaster data ->> ...\n",
      "Converting Milvus migrans data ->> ...\n",
      "Converting Mirafra javanica data ->> ...\n",
      "Converting Mus musculus House mouse data ->> ...\n",
      "Converting Myiagra alecto data ->> ...\n",
      "Converting Myiagra cyanoleuca data ->> ...\n",
      "Converting Myiagra inquieta data ->> ...\n",
      "Converting Myiagra rubecula data ->> ...\n",
      "Converting Myzomela erythrocephala data ->> ...\n",
      "Converting Myzomela sanguinolenta data ->> ...\n",
      "Converting Neochmia phaeton data ->> ...\n",
      "Converting Neochmia temporalis data ->> ...\n",
      "Converting Neophema pulchella data ->> ...\n",
      "Converting Neosericornis citreogularis data ->> ...\n",
      "Converting Nesoptilotis leucotis data ->> ...\n",
      "Converting Nettapus pulchellus data ->> ...\n",
      "Converting Ninox boobook data ->> ...\n",
      "Converting Ninox connivens data ->> ...\n",
      "Converting Ninox strenua data ->> ...\n",
      "Converting Notamacropus rufogriseus data ->> ...\n",
      "Converting Notopsaltra atrata data ->> ...\n",
      "Converting Nycticorax caledonicus data ->> ...\n",
      "Converting Nymphicus hollandicus data ->> ...\n",
      "Converting Ocyphaps lophotes data ->> ...\n",
      "Converting Oreoica gutturalis data ->> ...\n",
      "Converting Origma solitaria data ->> ...\n",
      "Converting Oriolus flavocinctus data ->> ...\n",
      "Converting Oriolus sagittatus data ->> ...\n",
      "list index out of range\n",
      "Converting Oryctolagus cuniculus European Rabbit data ->> ...\n",
      "Converting Osphranter robustus data ->> ...\n",
      "Converting Pachycephala inornata data ->> ...\n",
      "Converting Pachycephala pectoralis data ->> ...\n",
      "Converting Pachycephala rufiventris data ->> ...\n",
      "Converting Pachycephala simplex data ->> ...\n",
      "Converting Pardalotus punctatus data ->> ...\n",
      "Converting Pardalotus rubricatus data ->> ...\n",
      "Converting Pardalotus striatus data ->> ...\n",
      "Converting Parvipsitta pusilla data ->> ...\n",
      "Converting Pelecanus conspicillatus data ->> ...\n",
      "Converting Petrochelidon ariel data ->> ...\n",
      "Converting Petrochelidon nigricans data ->> ...\n",
      "Converting Petroica boodang data ->> ...\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 00000241E44F5800] Failed to read frame size: Could not seek to 1266.\n",
      "D:\\Data\\b1\\Petroica boodang\\X03600.mp3: Invalid argument\n",
      "\n",
      "Converting Petroica goodenovii data ->> ...\n",
      "Converting Petroica phoenicea data ->> ...\n",
      "Converting Petroica rosea data ->> ...\n",
      "Converting Phaneropterinae sp data ->> ...\n",
      "Converting Phaps chalcoptera data ->> ...\n",
      "Converting Phaps elegans data ->> ...\n",
      "Converting Phaps sp data ->> ...\n",
      "Converting Philemon argenticeps data ->> ...\n",
      "Converting Philemon buceroides data ->> ...\n",
      "Converting Philemon citreogularis data ->> ...\n",
      "Converting Philemon corniculatus data ->> ...\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 0000023E3CAB5580] Failed to read frame size: Could not seek to 1266.\n",
      "D:\\Data\\b1\\Philemon corniculatus\\X03320.mp3: Invalid argument\n",
      "\n",
      "list index out of range\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 0000027A29E15C40] Failed to read frame size: Could not seek to 1266.\n",
      "D:\\Data\\b1\\Philemon corniculatus\\X03329.mp3: Invalid argument\n",
      "\n",
      "list index out of range\n",
      "list index out of range\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 0000020740024340] Failed to read frame size: Could not seek to 1278.\n",
      "D:\\Data\\b1\\Philemon corniculatus\\X03348.mp3: Invalid argument\n",
      "\n",
      "Converting Phylidonyris novaehollandiae data ->> ...\n",
      "Converting Phylidonyris pyrrhopterus data ->> ...\n",
      "Converting Phylidonyris nigra data ->> ...\n",
      "Converting Pitta iris data ->> ...\n",
      "Converting Pitta versicolor data ->> ...\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 000001C760677B40] Failed to read frame size: Could not seek to 1266.\n",
      "D:\\Data\\b1\\Pitta versicolor\\X03058.mp3: Invalid argument\n",
      "\n",
      "Converting Platycercus adscitus data ->> ...\n",
      "Converting Platycercus elegans data ->> ...\n",
      "Converting Platyplectrum ornatum data ->> ...\n",
      "Converting Plectorhyncha lanceolata data ->> ...\n",
      "Converting Podargus strigoides data ->> ...\n",
      "Converting Poecilodryas superciliosa data ->> ...\n",
      "Converting Pomatostomus superciliosus data ->> ...\n",
      "Converting Pomatostomus temporalis data ->> ...\n",
      "Converting Poodytes gramineus data ->> ...\n",
      "Converting Porphyrio porphyrio data ->> ...\n",
      "Converting Psaltoda moerens data ->> ...\n",
      "Converting Psaltoda plaga data ->> ...\n",
      "Converting Psephotellus varius data ->> ...\n",
      "Converting Psephotus haematonotus data ->> ...\n",
      "Converting Pseudocheirus peregrinus data ->> ...\n",
      "Converting Psitteuteles versicolor data ->> ...\n",
      "Converting Psophodes cristatus data ->> ...\n",
      "Converting Psophodes nigrogularis data ->> ...\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 00000230D9C35940] Failed to read frame size: Could not seek to 1278.\n",
      "D:\\Data\\b1\\Psophodes nigrogularis\\X04178.mp3: Invalid argument\n",
      "\n",
      "Decoding failed. ffmpeg returned error code: 1\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with clang version 15.0.7\n",
      "  configuration: --prefix=/d/bld/ffmpeg_1674566436592/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/d/bld/ffmpeg_1674566436592/_build_env/Library/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[mp3 @ 000001C0101F6FC0] Failed to read frame size: Could not seek to 1266.\n",
      "D:\\Data\\b1\\Psophodes nigrogularis\\X04186.mp3: Invalid argument\n",
      "\n",
      "Converting Psophodes occidentalis data ->> ...\n",
      "Converting Psophodes olivaceus data ->> ...\n",
      "Converting Pteropus scapulatus data ->> ...\n",
      "Converting Pteropus sp data ->> ...\n",
      "Converting Ptilinopus regina data ->> ...\n",
      "Converting Ptilonorhynchus violaceus data ->> ...\n",
      "Converting Ptilotula flavescens data ->> ...\n",
      "Converting Ptilotula fusca data ->> ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7152\\4242700727.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nThis next process will take approx 20 mins for the current bird dataset depending on the speed of your computer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata_preprocessing_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudio_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRIM_AUDIO\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNORM_AUDIO\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdata_preprocessing_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_test_split_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7152\\3816187713.py\u001b[0m in \u001b[0;36maudio_preprocessing\u001b[1;34m(self, TRIM_AUDIO, NORM_AUDIO)\u001b[0m\n\u001b[0;32m    135\u001b[0m                             \u001b[1;31m# export raw file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[0mtmp_raw_new_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_file_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_raw_trim_sample_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_sample\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTARGET_FORMAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                             \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_raw_new_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTARGET_FORMAT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbitrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBITRATE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                             new_row = pd.Series({\"Label\": key,\n",
      "\u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\site-packages\\pydub\\audio_segment.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(self, out_f, format, codec, bitrate, parameters, tags, id3v2_version, cover)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdevnull\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconversion_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m         \u001b[0mp_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0mlog_subprocess_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m                 \u001b[1;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1265\u001b[0m             \u001b[1;31m# calls communicate again.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remaining_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_preprocessing_pipeline = raw_file_pre_processing()\n",
    "\n",
    "data_preprocessing_pipeline.get_raw_file_paths()\n",
    "\n",
    "print('\\nThis next process will take approx 20 mins for the current bird dataset depending on the speed of your computer')\n",
    "data_preprocessing_pipeline.audio_preprocessing(TRIM_AUDIO=True, NORM_AUDIO=True)\n",
    "data_preprocessing_pipeline.train_test_split_fun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melspectrogram Pipeline\n",
    "\n",
    "This pipeline will get all the new flac files, convert them to tfio tensors, convert to spectrograms, convert again to mel-spectrograms. It will then save the tensors as .pt files which can be read again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mel_spectrogram_pipeline():\n",
    "    def __init__(self, output_directory) -> None:\n",
    "        self.target_dir = ''\n",
    "        self.labels = []\n",
    "        self.augmented_dirs = {}\n",
    "        self.OUTPUT_DIR = {}\n",
    "        self.ACCEPTED_FORMAT = '.flac'\n",
    "\n",
    "        # do not change, this will be brought across from the previous pipeline\n",
    "        self._SET_OUTPUT_DIR = output_directory\n",
    "        if '/' in self._SET_OUTPUT_DIR:\n",
    "            self.DATASET_PATH = self._SET_OUTPUT_DIR\n",
    "            self.TENSOR_OUTPUT_DIR = os.path.join(self.DATASET_PATH, 'OUTPUT_tensors')\n",
    "        elif '\\\\' in self._SET_OUTPUT_DIR:\n",
    "            self.DATASET_PATH = self._SET_OUTPUT_DIR\n",
    "            self.TENSOR_OUTPUT_DIR = os.path.join(self.DATASET_PATH, 'OUTPUT_tensors')\n",
    "\n",
    "        self.NFFT = 512\n",
    "        self.WINDOW = 512\n",
    "        self.STRIDE = 512\n",
    "        self.SAMPLE_RATE = int(44100/2)\n",
    "        self.MELS = 128\n",
    "        self.FMIN = 0\n",
    "        self.FMAX = int(self.SAMPLE_RATE)/2\n",
    "        self.TOP_DB = 80\n",
    "\n",
    "    \n",
    "    def clean_dirs(self) -> None:\n",
    "        print('Cleaning tensor directory')\n",
    "        if os.path.exists(self.TENSOR_OUTPUT_DIR):\n",
    "            shutil.rmtree(self.TENSOR_OUTPUT_DIR)\n",
    "\n",
    "    def get_output_dir(self) -> None:\n",
    "        print('Finding all pre-processed files')\n",
    "        for root, dir, files in os.walk(self.DATASET_PATH):\n",
    "            if \"TRAIN_raw_flac\" not in dir:\n",
    "                if \"OUTPUT\" not in dir:\n",
    "                    raise ValueError('Cant find any directories with pre-processed data. Looking for OUTPUT or TRAIN, TEST, and VAIDATION')\n",
    "                else:\n",
    "                    self.OUTPUT_DIR.update({\"OUTPUT\": os.path.join(root, \"OUTPUT\")})\n",
    "            else:\n",
    "                if \"TEST_raw_flac\" in dir and \"VALIDATION_raw_flac\" in dir:\n",
    "                    self.OUTPUT_DIR.update({\"TRAIN\": os.path.join(root, \"TRAIN_raw_flac\")})\n",
    "                    self.OUTPUT_DIR.update({\"TEST\": os.path.join(root, \"TEST_raw_flac\")})\n",
    "                    self.OUTPUT_DIR.update({\"VALIDATION\": os.path.join(root, \"VALIDATION_raw_flac\")})\n",
    "            if not self.OUTPUT_DIR:\n",
    "                raise ValueError('Cant find any directories with pre-processed data. Looking for OUTPUT or TRAIN, TEST, and VAIDATION')\n",
    "\n",
    "            print(f'\\nFound the following directories {self.OUTPUT_DIR}\\n')\n",
    "            break\n",
    "            \n",
    "    def get_preprocessed_files(self) -> None:\n",
    "        self.get_output_dir()\n",
    "\n",
    "        for key, item in self.OUTPUT_DIR.items():\n",
    "            for root, dir, files in os.walk(item):\n",
    "                if dir == []:\n",
    "                    tmp_lable = str(os.path.split(root)[-1]) + \"~\" + key\n",
    "                    tmp_file_dir = []\n",
    "                    for file in files:\n",
    "                        if self.ACCEPTED_FORMAT in str(file):\n",
    "                            tmp_file_dir.append(os.path.join(root, file))\n",
    "                            \n",
    "                    self.augmented_dirs.update({tmp_lable:tmp_file_dir})\n",
    "\n",
    "        for key in self.augmented_dirs:\n",
    "            print(f'FOUND: {key} -> {len(self.augmented_dirs[key])}')\n",
    "\n",
    "    def generate_mel_spectrograms(self, MEL_SPECTRO:bool = False, \n",
    "                                        SHOW_PLOT:bool = False, \n",
    "                                        FREQ_MASK:bool = False, \n",
    "                                        TIME_MASK:bool = False,\n",
    "                                        TORCH_EXPORT:bool = False, \n",
    "                                        TFIO_EXPORT:bool = False) -> None:\n",
    "                                        \n",
    "        print(f'\\nGenerating tensors... \\n')\n",
    "        for key, item in self.augmented_dirs.items():\n",
    "            for dir in item:\n",
    "                tmp_label = key.split('~')[0]\n",
    "                tmp_set = key.split('~')[1]\n",
    "                if not os.path.exists(os.path.join(self.TENSOR_OUTPUT_DIR)):\n",
    "                    os.mkdir(os.path.join(self.TENSOR_OUTPUT_DIR))\n",
    "                if not os.path.exists(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set)):\n",
    "                    os.mkdir(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set))\n",
    "                if not os.path.exists(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label)):\n",
    "                    os.mkdir(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label))\n",
    "       \n",
    "                file_contents=tf.io.read_file(dir)\n",
    "                try:\n",
    "                    tmp_audio_t = tfio.audio.decode_flac(input=file_contents, dtype=tf.int16)\n",
    "                except:\n",
    "                    tmp_audio_t = tfio.audio.decode_flac(input=file_contents, dtype=tf.int32)\n",
    "                    \n",
    "                tmp_audio_t = tf.cast(tmp_audio_t, tf.float32)\n",
    "                    \n",
    "                tmp_audio_t = tfio.audio.resample(tmp_audio_t, tfio.audio.AudioIOTensor(dir)._rate.numpy(), self.SAMPLE_RATE)\n",
    "\n",
    "                # Convert to spectrogram\n",
    "                spectrogram = tfio.audio.spectrogram(\n",
    "                    tmp_audio_t[:, 0], nfft=self.NFFT, window=self.WINDOW, stride=self.STRIDE)\n",
    "\n",
    "                if SHOW_PLOT:\n",
    "                    plt.figure()\n",
    "                    plt.imshow(tf.math.log(spectrogram).numpy())\n",
    "\n",
    "                if MEL_SPECTRO:\n",
    "                    # # Convert to mel-spectrogram\n",
    "                    mel_spectrogram = tfio.audio.melscale(\n",
    "                        spectrogram, rate=self.SAMPLE_RATE, mels=self.MELS, fmin=self.FMIN, fmax=self.FMAX)\n",
    "\n",
    "                    if SHOW_PLOT:\n",
    "                        plt.figure()\n",
    "                        plt.imshow(tf.math.log(mel_spectrogram).numpy())\n",
    "\n",
    "                    if TORCH_EXPORT:\n",
    "                        torch.save(mel_spectrogram, str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_raw_mel_spectrogram.pt')\n",
    "                    if TFIO_EXPORT:\n",
    "                        tf.io.write_file(str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_raw_mel_spectrogram.pt', tf.io.serialize_tensor(mel_spectrogram))\n",
    "\n",
    "                    if FREQ_MASK and \"TEST\" not in dir and \"VALIDATION\" not in dir:\n",
    "                        freq_mask = tfio.audio.freq_mask(mel_spectrogram, param=10)\n",
    "                        if TORCH_EXPORT:\n",
    "                            torch.save(freq_mask, str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_freq_mask_mel_spectrogram.pt')\n",
    "                        if TFIO_EXPORT:\n",
    "                            tf.io.write_file(str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_freq_mask_mel_spectrogram.pt', tf.io.serialize_tensor(mel_spectrogram))\n",
    "                    \n",
    "                    if TIME_MASK and \"TEST\" not in dir and \"VALIDATION\" not in dir:\n",
    "                        time_mask = tfio.audio.time_mask(mel_spectrogram, param=10)\n",
    "                        if TORCH_EXPORT:\n",
    "                            torch.save(time_mask, str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_time_mask_mel_spectrogram.pt')\n",
    "                        if TFIO_EXPORT:\n",
    "                            tf.io.write_file(str(os.path.join(self.TENSOR_OUTPUT_DIR, tmp_set, tmp_label, os.path.split(dir)[-1].split('.')[0])) + '_time_mask_mel_spectrogram.pt', tf.io.serialize_tensor(mel_spectrogram))\n",
    "\n",
    "        print(\"\\nTensors complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning tensor directory\n",
      "Finding all pre-processed files\n",
      "\n",
      "Found the following directories {'TRAIN': 'D:\\\\Data\\\\bc_out\\\\TRAIN_raw_flac', 'TEST': 'D:\\\\Data\\\\bc_out\\\\TEST_raw_flac', 'VALIDATION': 'D:\\\\Data\\\\bc_out\\\\VALIDATION_raw_flac'}\n",
      "\n",
      "FOUND: brant~TRAIN -> 759\n",
      "FOUND: jabwar~TRAIN -> 763\n",
      "FOUND: sheowl~TRAIN -> 915\n",
      "FOUND: spodov~TRAIN -> 610\n",
      "FOUND: wiltur~TRAIN -> 1081\n",
      "FOUND: brant~TEST -> 9\n",
      "FOUND: jabwar~TEST -> 13\n",
      "FOUND: sheowl~TEST -> 8\n",
      "FOUND: spodov~TEST -> 23\n",
      "FOUND: wiltur~TEST -> 1\n",
      "FOUND: brant~VALIDATION -> 228\n",
      "FOUND: jabwar~VALIDATION -> 105\n",
      "FOUND: sheowl~VALIDATION -> 123\n",
      "FOUND: spodov~VALIDATION -> 185\n",
      "FOUND: wiltur~VALIDATION -> 176\n",
      "\n",
      "This process will take approx 5-10 mins to complete depending on the power of your PC\n",
      "\n",
      "Generating tensors... \n",
      "\n",
      "\n",
      "Tensors complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_spectro_pipeline = mel_spectrogram_pipeline(data_preprocessing_pipeline._SET_OUTPUT_DIR)\n",
    "\n",
    "data_spectro_pipeline.clean_dirs()\n",
    "data_spectro_pipeline.get_preprocessed_files()\n",
    "\n",
    "print('\\nThis process will take approx 5-10 mins to complete depending on the power of your PC')\n",
    "data_spectro_pipeline.generate_mel_spectrograms(MEL_SPECTRO=True, SHOW_PLOT=False, FREQ_MASK=True, TIME_MASK=True, TORCH_EXPORT=False, TFIO_EXPORT=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to load data into memory for model training\n",
    "\n",
    "This pipeline will load all the tensors into a train, test, and validation data structure and prepare it for inputs into a model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_test_vali_pipeline():\n",
    "    def __init__(self, output_directory) -> None:\n",
    "        self.DATASET_PATH  = output_directory\n",
    "        self.TENSOR_OUTPUT_DIR  = None\n",
    "\n",
    "        if '/' in self.DATASET_PATH:\n",
    "            self.TENSOR_OUTPUT_DIR = os.path.join(self.DATASET_PATH, 'OUTPUT_tensors')\n",
    "        elif '\\\\' in self.DATASET_PATH:\n",
    "            self.TENSOR_OUTPUT_DIR = os.path.join(self.DATASET_PATH, 'OUTPUT_tensors')\n",
    "\n",
    "        self.VALID_FILES = False\n",
    "\n",
    "        self.PATHS = []\n",
    "        self.train_data = self.test_data = self.vali_data = pd.DataFrame(columns=['Label', 'Tensor'])\n",
    "\n",
    "    \n",
    "    def check_valid_dirs(self) -> None:\n",
    "        def contains_test(arr):\n",
    "            if any(\"TEST\" in item for item in arr): return True\n",
    "            return False\n",
    "        def contains_train(arr):\n",
    "            if any(\"TRAIN\" in item for item in arr): return True\n",
    "            return False\n",
    "        def contains_vali(arr):\n",
    "            if any(\"VALIDATION\" in item for item in arr): return True\n",
    "            return False\n",
    "\n",
    "        print('Checking to find train, test and vali directories inside tensors folder...')\n",
    "        for root, dir, files in os.walk(self.TENSOR_OUTPUT_DIR):\n",
    "            for i in dir:\n",
    "                self.PATHS.append(os.path.join(root, i))\n",
    "            if contains_test(dir) and contains_train(dir) and contains_vali(dir):\n",
    "                self.VALID_FILES = True\n",
    "                print('PASS')\n",
    "            else:\n",
    "                raise ValueError('Cannot find folders from previous pipline which include train, test and validation directories')\n",
    "            break\n",
    "\n",
    "    def load_data(self, LOAD_RAW:bool = False, \n",
    "                        LOAD_FREQ:bool = False, \n",
    "                        LOAD_TIME:bool = False, \n",
    "                        TO_CSV:bool = False, \n",
    "                        TORCH_LOAD:bool = False, \n",
    "                        TFIO_LOAD:bool = False) -> None:\n",
    "\n",
    "        for path in self.PATHS:\n",
    "            if 'TEST' in str(path) or 'TRAIN' in str(path) or 'VALIDATION' in str(path):\n",
    "                for root, dir, file in os.walk(path):\n",
    "                    for tmp_label in dir:  \n",
    "                        for file in [f for f in listdir(os.path.join(root, tmp_label)) if isfile(join(os.path.join(root, tmp_label), f))]:\n",
    "                            if '_raw_mel' in str(file) and LOAD_RAW:\n",
    "                                if TORCH_LOAD:\n",
    "                                    tmp_data = torch.load(os.path.join(path, tmp_label, file))\n",
    "                                if TFIO_LOAD:\n",
    "                                    tmp_data = tf.io.parse_tensor(tf.io.read_file(os.path.join(path, tmp_label, file)), tf.float32)\n",
    "                                    \n",
    "                                new_row = pd.Series({\"Label\": tmp_label,\n",
    "                                                    \"Tensor\": tmp_data})\n",
    "                                if \"TRAIN\" in path:\n",
    "                                    self.train_data = pd.concat([self.train_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"TEST\" in path:\n",
    "                                    self.test_data = pd.concat([self.test_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"VALIDATION\" in path:\n",
    "                                    self.vali_data = pd.concat([self.vali_data, new_row.to_frame().T], ignore_index=True)\n",
    "                            elif '_freq_mask' in str(file) and LOAD_FREQ:\n",
    "                                if TORCH_LOAD:\n",
    "                                    tmp_data = torch.load(os.path.join(path, tmp_label, file))\n",
    "                                if TFIO_LOAD:\n",
    "                                    tmp_data = tf.io.parse_tensor(tf.io.read_file(os.path.join(path, tmp_label, file)), tf.float32)\n",
    "\n",
    "                                new_row = pd.Series({\"Label\": tmp_label,\n",
    "                                                    \"Tensor\": tmp_data})\n",
    "                                if \"TRAIN\" in path:\n",
    "                                    self.train_data = pd.concat([self.train_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"TEST\" in path:\n",
    "                                    self.test_data = pd.concat([self.test_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"VALIDATION\" in path:\n",
    "                                    self.vali_data = pd.concat([self.vali_data, new_row.to_frame().T], ignore_index=True)\n",
    "                            elif '_time_mask' in str(file) and LOAD_TIME:\n",
    "                                if TORCH_LOAD:\n",
    "                                    tmp_data = torch.load(os.path.join(path, tmp_label, file))\n",
    "                                if TFIO_LOAD:\n",
    "                                    tmp_data = tf.io.parse_tensor(tf.io.read_file(os.path.join(path, tmp_label, file)), tf.float32)\n",
    "                                    \n",
    "                                new_row = pd.Series({\"Label\": tmp_label,\n",
    "                                                    \"Tensor\": tmp_data})\n",
    "                                if \"TRAIN\" in path:\n",
    "                                    self.train_data = pd.concat([self.train_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"TEST\" in path:\n",
    "                                    self.test_data = pd.concat([self.test_data, new_row.to_frame().T], ignore_index=True)\n",
    "                                elif \"VALIDATION\" in path:\n",
    "                                    self.vali_data = pd.concat([self.vali_data, new_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "        if TO_CSV:\n",
    "            self.train_data.to_csv(os.path.join(self.DATASET_PATH, 'train_df.csv'))\n",
    "            self.test_data.to_csv(os.path.join(self.DATASET_PATH, 'test_df.csv'))\n",
    "            self.vali_data.to_csv(os.path.join(self.DATASET_PATH, 'vali_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking to find train, test and vali directories inside tensors folder...\n",
      "PASS\n",
      "This process will take approx 1 minute\n",
      "Finished\n",
      "Train Shape: (12384, 2), Test Shape: (54, 2), Validation Shape: (817, 2)\n"
     ]
    }
   ],
   "source": [
    "model_data_pipeline = train_test_vali_pipeline(data_preprocessing_pipeline._SET_OUTPUT_DIR)\n",
    "\n",
    "model_data_pipeline.check_valid_dirs()\n",
    "\n",
    "print('This process will take approx 1 minute')\n",
    "model_data_pipeline.load_data(LOAD_RAW=True, LOAD_FREQ=True, LOAD_TIME=True, TO_CSV=False, TORCH_LOAD=False, TFIO_LOAD=True)\n",
    "\n",
    "print(\"Finished\")\n",
    "print(f'Train Shape: {model_data_pipeline.train_data.shape}, Test Shape: {model_data_pipeline.test_data.shape}, Validation Shape: {model_data_pipeline.vali_data.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "2848388c1d7df64c5912f8c74b12cb2f63a5fbb869f66edadd9f1eda580b6df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
