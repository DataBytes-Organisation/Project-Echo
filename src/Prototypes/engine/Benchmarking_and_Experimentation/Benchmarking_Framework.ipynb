{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Echo - Experiment Benchmarking Framework\n",
    "\n",
    "This notebook provides an interactive interface to the benchmarking framework. It allows you to run various experiments with different model architectures and augmentation strategies, and compare their performance.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The benchmarking framework is designed to systematically evaluate different combinations of:\n",
    "- Model architectures (EfficientNet, MobileNet, ResNet, etc.)\n",
    "- Audio augmentation strategies\n",
    "- Image/spectrogram augmentation strategies\n",
    "\n",
    "Results are collected and visualized to help identify the best performing configurations for bat sound classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Install Required Libraries\n",
    "The following cell is to install required libraries if you are running this notebook remotly, such as on an instance from Vast.ai or google colab.\n",
    "Ensure you have a clean python 3.9.21 kernal to start.\n",
    "Details on how to set this up are contained within the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "from IPython.display import display\n",
    "slider = IntSlider()\n",
    "display(slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import re\n",
    "import importlib\n",
    "\n",
    "'''# Add the current directory to path for imports\n",
    "module_path = os.getcwd() # Gets the current working directory of the notebook\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)'''\n",
    "\n",
    "# --- MODIFICATION FOR DOCKERIZED KERNEL ---\n",
    "# This path MUST match the location of your 'Benchmarking_and_Experimentation'\n",
    "# directory INSIDE THE DOCKER CONTAINER, based on your volume mount.\n",
    "\n",
    "# Assuming your local 'c:\\Users\\deanf\\OneDrive\\Coding\\GitHub\\Project-Echo'\n",
    "# is mounted to '/workspace/Project-Echo' inside the container:\n",
    "actual_module_path_inside_container =\"D://Users//Moiz//Documents//Git25//Project-Echo/src/Prototypes/engine/Benchmarking_and_Experimentation\"\n",
    "# This is the directory containing your 'config' and 'utils' Python packages.\n",
    "\n",
    "if not os.path.isdir(actual_module_path_inside_container):\n",
    "    print(f\"ERROR: The path '{actual_module_path_inside_container}' does NOT exist or is not a directory INSIDE THE CONTAINER.\")\n",
    "    print(f\"Current CWD inside container (from kernel's perspective) is: {os.getcwd()}\")\n",
    "    # For debugging, you can list directories from the root of your mounted project:\n",
    "    # mounted_project_root_in_container = \"/workspace/Project-Echo\" # Adjust if your mount is different\n",
    "    # if os.path.exists(mounted_project_root_in_container):\n",
    "    #     print(f\"Contents of '{mounted_project_root_in_container}': {os.listdir(mounted_project_root_in_container)}\")\n",
    "else:\n",
    "    if actual_module_path_inside_container not in sys.path:\n",
    "        sys.path.insert(0, actual_module_path_inside_container) # Insert at the beginning for higher precedence\n",
    "    print(f\"Successfully added to sys.path: {actual_module_path_inside_container}\")\n",
    "    # You can verify the contents if needed:\n",
    "    # print(f\"Contents of '{actual_module_path_inside_container}': {os.listdir(actual_module_path_inside_container)}\")\n",
    "    # print(f\"Checking for config dir: {os.path.exists(os.path.join(actual_module_path_inside_container, 'config'))}\")\n",
    "    # print(f\"Checking for __init__.py in config: {os.path.exists(os.path.join(actual_module_path_inside_container, 'config', '__init__.py'))}\")\n",
    "\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "# Import framework components\n",
    "from config.experiment_configs import EXPERIMENTS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import re\n",
    "import importlib\n",
    "\n",
    "# Add the current directory to path for imports\n",
    "module_path = os.getcwd() # Gets the current working directory of the notebook\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import framework components\n",
    "from config.experiment_configs import EXPERIMENTS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Available Experiments\n",
    "\n",
    "Here you can view and select experiments to run. Each experiment represents a combination of model architecture and augmentation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up the directories and options for benchmarking. \n",
    "\n",
    "Ensure to update these in the system_config.py file in the config folder.\n",
    "\n",
    "The default directories are as follows:\n",
    "\n",
    "DATA_DIR = \"D:\\Echo\\Audio_data\"  # Directory containing audio data\n",
    "\n",
    "CACHE_DIR = \"D:\\Echo\\Training_cache\"  # Directory for caching pipeline results\n",
    "\n",
    "OUTPUT_DIR = \"D:\\Echo\\results\"  # Directory to save experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import directories from system_config\n",
    "from config.system_config import SC\n",
    "\n",
    "# Get directory paths from system config\n",
    "DATA_DIR = SC['AUDIO_DATA_DIRECTORY']\n",
    "CACHE_DIR = SC['CACHE_DIRECTORY']\n",
    "OUTPUT_DIR = SC['OUTPUT_DIRECTORY']\n",
    "\n",
    "print(f\"Using directories from system_config:\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Cache Directory: {CACHE_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "print(\"Physical GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU name:\", tf.test.gpu_device_name())\n",
    "\n",
    "\n",
    "# Configure GPU memory if available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"GPU support enabled: {len(gpus)} GPU(s) found\")\n",
    "else:\n",
    "    print(\"No GPU support found, running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available experiments\n",
    "experiment_data = []\n",
    "for exp in EXPERIMENTS:\n",
    "    experiment_data.append({\n",
    "        \"name\": exp[\"name\"],\n",
    "        \"model\": exp[\"model\"],\n",
    "        \"audio_augmentation\": exp[\"audio_augmentation\"],\n",
    "        \"image_augmentation\": exp[\"image_augmentation\"],\n",
    "        \"epochs\": exp[\"epochs\"],\n",
    "        \"batch_size\": exp[\"batch_size\"]\n",
    "    })\n",
    "\n",
    "experiments_df = pd.DataFrame(experiment_data)\n",
    "display(experiments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Experiment Selection\n",
    "\n",
    "Use the widgets below to select experiments and set directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create widgets for directory selection\n",
    "data_dir_widget = widgets.Text(\n",
    "    value=DATA_DIR,\n",
    "    description='Data Directory:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "cache_dir_widget = widgets.Text(\n",
    "    value=CACHE_DIR,\n",
    "    description='Cache Directory:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "output_dir_widget = widgets.Text(\n",
    "    value=OUTPUT_DIR,\n",
    "    description='Output Directory:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Group directory widgets\n",
    "dir_widgets_box = widgets.VBox([data_dir_widget, cache_dir_widget, output_dir_widget])\n",
    "\n",
    "# Create widget for experiment selection\n",
    "experiment_options = [(exp[\"name\"], exp[\"name\"]) for exp in EXPERIMENTS]\n",
    "experiment_widget = widgets.SelectMultiple(\n",
    "    options=experiment_options,\n",
    "    description='Select Experiments:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%', height='200px')\n",
    ")\n",
    "\n",
    "# Buttons for actions\n",
    "run_selected_button = widgets.Button(\n",
    "    description='Run Selected Experiments',\n",
    "    button_style='primary',\n",
    "    tooltip='Run the selected experiments'\n",
    ")\n",
    "\n",
    "run_all_button = widgets.Button(\n",
    "    description='Run All Experiments',\n",
    "    tooltip='Run all experiments'\n",
    ")\n",
    "\n",
    "generate_report_button = widgets.Button(\n",
    "    description='Generate Report Only',\n",
    "    button_style='info',\n",
    "    tooltip='Generate a report from existing results'\n",
    ")\n",
    "\n",
    "# Group buttons\n",
    "buttons_box = widgets.HBox([run_selected_button, run_all_button, generate_report_button])\n",
    "\n",
    "# Output area for logs\n",
    "output_area = widgets.Output(layout={'border': '1px solid black', 'width': '90%', 'height': '300px'}) # Adjusted width and added height\n",
    "\n",
    "# Main container for all control widgets\n",
    "controls_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Directory Configuration:</h3>\"), # Optional title\n",
    "    dir_widgets_box,\n",
    "    widgets.HTML(\"<hr><h3>Experiment Selection:</h3>\"), # Optional separator and title\n",
    "    experiment_widget,\n",
    "    widgets.HTML(\"<hr><h3>Actions:</h3>\"), # Optional separator and title\n",
    "    buttons_box\n",
    "])\n",
    "\n",
    "# Display main controls container and then the output area\n",
    "display(controls_box)\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Runner Functions\n",
    "\n",
    "These functions handle the execution of experiments and report generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.optimised_engine_pipeline import train_model\n",
    "\n",
    "\n",
    "def run_selected_experiments(b):\n",
    "\n",
    "    from IPython.display import clear_output # Moved import here for clarity\n",
    "    # clear_output(wait=True) # Clear previous output first\n",
    "    output_area.clear_output(wait=True) \n",
    "    with output_area:\n",
    "        print(\"Starting experiment run...\")\n",
    "        # Get new directory paths from widgets\n",
    "        new_data_dir = data_dir_widget.value\n",
    "        new_cache_dir = cache_dir_widget.value\n",
    "        \n",
    "        # Define path to system_config.py (relative to notebook directory)\n",
    "        # Assumes 'config' is a subdirectory of the notebook's directory\n",
    "        config_file_path = os.path.join('config', 'system_config.py')\n",
    "        \n",
    "        try:\n",
    "            print(f\"Attempting to update {config_file_path}...\")\n",
    "            with open(config_file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            new_lines = []\n",
    "            config_updated = False\n",
    "            for line in lines:\n",
    "                if \"'AUDIO_DATA_DIRECTORY':\" in line:\n",
    "                    # Use r-string for replacement to handle backslashes in path correctly\n",
    "                    new_line = re.sub(r\"('AUDIO_DATA_DIRECTORY':\\s*r\\\")[^\\\"]*(\\\")\", rf'\\1{new_data_dir}\\2', line)\n",
    "                    if new_line != line:\n",
    "                        config_updated = True\n",
    "                    new_lines.append(new_line)\n",
    "                elif \"'CACHE_DIRECTORY':\" in line: \n",
    "                    new_line = re.sub(r\"('CACHE_DIRECTORY':\\s*r\\\")[^\\\"]*(\\\")\", rf'\\1{new_cache_dir}\\2', line)\n",
    "                    if new_line != line:\n",
    "                        config_updated = True\n",
    "                    new_lines.append(new_line)\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "            \n",
    "            if config_updated:\n",
    "                with open(config_file_path, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "                print(f\"Successfully updated {config_file_path} with new directory paths.\")\n",
    "            else:\n",
    "                print(f\"{config_file_path} already up-to-date or keys not found.\")\n",
    "            \n",
    "            # Reload the system_config module to apply changes\n",
    "            importlib.reload(config.system_config)\n",
    "            # Re-import SC if it's used directly in this notebook, or ensure train_model gets the fresh one.\n",
    "            # from config.system_config import SC \n",
    "            print(\"System configuration reloaded.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating or reloading system_config.py: {e}\")\n",
    "            print(\"Proceeding with potentially outdated configuration.\")\n",
    "            # Decide if you want to return or proceed if config update fails\n",
    "            # return \n",
    "\n",
    "        selected_experiments = list(experiment_widget.value)\n",
    "        if not selected_experiments:\n",
    "            print(\"No experiment selected. Please select at least one experiment.\")\n",
    "            return\n",
    "        \n",
    "        for exp_name in selected_experiments:\n",
    "            exp_config = next((exp for exp in EXPERIMENTS if exp[\"name\"] == exp_name), None)\n",
    "            if exp_config is None:\n",
    "                print(f\"Experiment {exp_name} not found in EXPERIMENTS.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Running experiment: {exp_config['name']}\")\n",
    "            # Pass configuration values to the train_model function.\n",
    "            # train_model will use the reloaded system_config.SC for DATA_DIR and CACHE_DIR\n",
    "            model, history = train_model(\n",
    "                model_name=exp_config['model'],\n",
    "                epochs=exp_config.get('epochs'),\n",
    "                batch_size=exp_config.get('batch_size')\n",
    "            )\n",
    "            print(f\"Training completed for experiment: {exp_config['name']}\")\n",
    "            if model:\n",
    "                 model.summary(print_fn=lambda x: print(x)) # Ensure summary prints to output_area\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "run_selected_button.on_click(run_selected_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Previous Results (From here down, notebook is under development)\n",
    "\n",
    "If you've already run experiments, you can view and analyse the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under development\n",
    "def load_results(output_dir=OUTPUT_DIR):\n",
    "    # Check if results directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Results directory does not exist: {output_dir}\")\n",
    "        return None\n",
    "    \n",
    "    # Look for comparison report CSV\n",
    "    csv_files = [f for f in os.listdir(output_dir) if f.startswith(\"comparison_results_\") and f.endswith(\".csv\")]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No comparison results found. Run experiments or generate a report first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load the latest CSV file\n",
    "    latest_csv = max(csv_files)\n",
    "    csv_path = os.path.join(output_dir, latest_csv)\n",
    "    results_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"Loaded results from: {csv_path}\")\n",
    "    return results_df\n",
    "\n",
    "# Load and display results if available\n",
    "results_df = load_results()\n",
    "if results_df is not None:\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualise Results\n",
    "\n",
    "Create various visualisations to compare experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under Development\n",
    "# Taken from previously developed notebooks in Machine Learing course\n",
    "\n",
    "def visualize_results(results_df):\n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\"No results available to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set the figure size for better visibility\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create accuracy comparison bar chart\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Experiment', y='Test Accuracy', data=results_df)\n",
    "    plt.title('Test Accuracy by Experiment')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create F1 score comparison bar chart\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Experiment', y='F1 Score', data=results_df)\n",
    "    plt.title('F1 Score by Experiment')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Training time comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Experiment', y='Training Time (min)', data=results_df)\n",
    "    plt.title('Training Time by Experiment (minutes)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Model comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    model_comparison = results_df.groupby('Model')['Test Accuracy'].mean().reset_index()\n",
    "    sns.barplot(x='Model', y='Test Accuracy', data=model_comparison)\n",
    "    plt.title('Average Accuracy by Model')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a separate visualization for augmentation impact\n",
    "    plt.figure(figsize=(12, 6))\n",
    "       \n",
    "    aug_df = pd.DataFrame(aug_data)\n",
    "    sns.barplot(x='Augmentation', y='Accuracy', hue='Augmentation Type', data=aug_df)\n",
    "    plt.title('Accuracy by Augmentation Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results if available\n",
    "if results_df is not None:\n",
    "    visualize_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment Analysis and Conclusions\n",
    "(if required)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tianc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tianc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "ℹ️ 发现已有音频数据： C:\\Users\\tianc\\Downloads\\echo_data\n",
      "\n",
      "=== Running mobilenetv3small_baseline (MobileNetV3Small_224) ===\n",
      "Building datasets for MobileNetV3Small_224...\n",
      "Train dataset size (batches): 1\n",
      "Validation dataset size (batches): 0\n",
      "Test dataset size (batches): 1\n",
      "Datasets built successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21s/step - accuracy: 0.0000e+00 - loss: 1.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tianc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running efficientnetlite0_baseline (EfficientNetLite0_224) ===\n",
      "Building datasets for EfficientNetLite0_224...\n",
      "Train dataset size (batches): 1\n",
      "Validation dataset size (batches): 0\n",
      "Test dataset size (batches): 1\n",
      "Datasets built successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19s/step - accuracy: 1.0000 - loss: 0.1827\n",
      "\n",
      "✅ Saved metrics to: C:\\Users\\tianc\\Downloads\\作业\\374\\echo\\1\\src\\Prototypes\\engine\\Benchmarking_and_Experimentation\\edge_leaderboard.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Experiment': 'mobilenetv3small_baseline',\n",
       "  'Model': 'MobileNetV3Small_224',\n",
       "  'HubURL': None,\n",
       "  'ACC': 0.0,\n",
       "  'F1': None,\n",
       "  'mAP': None},\n",
       " {'Experiment': 'efficientnetlite0_baseline',\n",
       "  'Model': 'EfficientNetLite0_224',\n",
       "  'HubURL': 'https://tfhub.dev/tensorflow/efficientnet/lite0/classification/2',\n",
       "  'ACC': 1.0,\n",
       "  'F1': None,\n",
       "  'mAP': None}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 准备：导入 & 假数据保障 ===\n",
    "from pathlib import Path\n",
    "import os, numpy as np, soundfile as sf\n",
    "\n",
    "from config.system_config import SC\n",
    "from config.experiment_configs import EXPERIMENTS\n",
    "from config.model_configs import MODELS\n",
    "from utils.optimised_engine_pipeline import train_model\n",
    "\n",
    "# 若数据目录为空/不存在，就自动写入两类简易正弦波音频，保证可跑通\n",
    "def ensure_dummy_audio(base: Path, n_per_class: int = 2, sr: int = 16000, dur_s: float = 1.0):\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    classes = [(\"class_a\", 440), (\"class_b\", 660)]\n",
    "    wrote = False\n",
    "    for cls, freq in classes:\n",
    "        d = base / cls\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        # 如果该类下没有 wav，就创建 n_per_class 个\n",
    "        if not any(p.suffix.lower()==\".wav\" for p in d.glob(\"*.wav\")):\n",
    "            t = np.linspace(0, dur_s, int(sr*dur_s), endpoint=False)\n",
    "            for i in range(n_per_class):\n",
    "                x = (0.2*np.sin(2*np.pi*freq*t)).astype(np.float32)\n",
    "                sf.write(d / f\"sample_{i+1}.wav\", x, sr)\n",
    "            wrote = True\n",
    "    if wrote:\n",
    "        print(\"✅ 已生成最小可跑通的假数据：\", base)\n",
    "    else:\n",
    "        print(\"ℹ️ 发现已有音频数据：\", base)\n",
    "\n",
    "# 读取配置中的数据目录并确保有数据\n",
    "audio_dir = Path(SC[\"AUDIO_DATA_DIRECTORY\"])\n",
    "ensure_dummy_audio(audio_dir)\n",
    "\n",
    "# === 评估指标辅助 ===\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "save_csv = Path(\"edge_leaderboard.csv\")   # 结果会写到当前目录\n",
    "epochs_override = 1                       # 想更快可设为 1；有真实数据再调大\n",
    "\n",
    "def pick_metric(history, names):\n",
    "    if history is None:\n",
    "        return None\n",
    "    for k in names:\n",
    "        if k in history.history and history.history[k]:\n",
    "            return float(history.history[k][-1])\n",
    "    return None\n",
    "\n",
    "# === 训练循环 ===\n",
    "rows = []\n",
    "for exp in EXPERIMENTS:\n",
    "    name = exp[\"name\"]\n",
    "    model_key = exp[\"model\"]\n",
    "    print(f\"\\n=== Running {name} ({model_key}) ===\")\n",
    "\n",
    "    # 训练\n",
    "    model, history = train_model(\n",
    "        model_name=model_key,\n",
    "        epochs=epochs_override if epochs_override is not None else exp.get(\"epochs\", 2),\n",
    "        batch_size=exp.get(\"batch_size\", 16),\n",
    "        # 若你的 train_model 支持下列参数可继续传入；不支持就删掉\n",
    "        # l2_regularization=0.0,\n",
    "        # l2_coefficient=0.0,\n",
    "    )\n",
    "\n",
    "    # 取指标（history 里具体键名可能不同，可按需改）\n",
    "    acc = pick_metric(history, [\"val_accuracy\", \"accuracy\"])\n",
    "    f1  = pick_metric(history, [\"val_f1_macro\", \"f1_macro\", \"val_f1\", \"f1\"])\n",
    "    mAP = pick_metric(history, [\"val_map\", \"map\", \"val_mAP\", \"mAP\"])\n",
    "\n",
    "    hub_url = MODELS.get(model_key, {}).get(\"hub_url\")\n",
    "    rows.append({\n",
    "        \"Experiment\": name,\n",
    "        \"Model\":      model_key,\n",
    "        \"HubURL\":     hub_url,\n",
    "        \"ACC\":        acc,\n",
    "        \"F1\":         f1,\n",
    "        \"mAP\":        mAP,\n",
    "    })\n",
    "\n",
    "# === 写入 CSV ===\n",
    "write_header = not save_csv.exists()\n",
    "with save_csv.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Experiment\",\"Model\",\"HubURL\",\"ACC\",\"F1\",\"mAP\"])\n",
    "    if write_header:\n",
    "        w.writeheader()\n",
    "    for r in rows:\n",
    "        w.writerow(r)\n",
    "\n",
    "print(f\"\\n✅ Saved metrics to: {save_csv.resolve()}\")\n",
    "rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'edge_leaderboard.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m acc_path = Path(\u001b[33m\"\u001b[39m\u001b[33medge_leaderboard.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m lat_path = Path(\u001b[33m\"\u001b[39m\u001b[33medge_latency.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df_acc = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43macc_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df_lat = pd.read_csv(lat_path)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 兼容不同列名，将 Model 作为键合并\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tianc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tianc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tianc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tianc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tianc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'edge_leaderboard.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "acc_path = Path(\"edge_leaderboard.csv\")\n",
    "lat_path = Path(\"edge_latency.csv\")\n",
    "df_acc = pd.read_csv(acc_path)\n",
    "df_lat = pd.read_csv(lat_path)\n",
    "\n",
    "# 兼容不同列名，将 Model 作为键合并\n",
    "df = pd.merge(df_acc, df_lat, how=\"left\", left_on=\"Model\", right_on=\"Model\")\n",
    "out = Path(\"edge_results_combined.csv\")\n",
    "df.to_csv(out, index=False)\n",
    "out, df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
