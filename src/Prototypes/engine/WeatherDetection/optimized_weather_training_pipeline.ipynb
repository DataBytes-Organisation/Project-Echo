{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17d4f29310028ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Optimized Weather Event Classification Training Pipeline\n",
    "\n",
    "This training pipeline is designed for processing audio data and training a convolutional neural network (CNN) model for audio classification. It specifically deals with weather-related audio data, transforming raw audio files into mel-spectrogram representations that are then used as input for the CNN. Below are the key components and steps involved in this pipeline:\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Audio Data Loading and Preprocessing**:\n",
    "   - **Loading Audio**: Audio files are loaded with a defined sample rate.\n",
    "   - **Padding or Trimming**: Each audio is either padded or trimmed to a uniform length based on the specified duration to maintain consistency.\n",
    "   - **Mel-Spectrogram Conversion**: Converts audio into mel-spectrograms using parameters like FFT window size and hop length.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - **Spectrogram Resizing**: Spectrograms are resized to fit the model's input dimensions.\n",
    "   - **Label Encoding**: Converts categorical labels into a one-hot encoding format suitable for classification.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - **CNN Architecture**: The model consists of several convolutional layers followed by max pooling and dropout layers to prevent overfitting. It ends with a global average pooling layer and a fully connected layer.\n",
    "   - **Callbacks**: Includes model checkpointing, reducing learning rate on plateau, and early stopping to improve training efficiency and prevent overfitting.\n",
    "\n",
    "4. **Data Augmentation**:\n",
    "   - Applies transformations like rotation, width and height shift, zoom, etc., to artificially expand the training dataset, which helps improve model generalization.\n",
    "\n",
    "5. **Training Execution**:\n",
    "   - Uses an image data generator to feed data into the model in batches, facilitating efficient training.\n",
    "\n",
    "6. **Model Evaluation and Saving**:\n",
    "   - The training and validation loss and accuracy are plotted to monitor the training process.\n",
    "   - The best-performing model is saved for later use in practical applications.\n",
    "\n",
    "## Model Deployment\n",
    "\n",
    "After training, the model is saved both in TensorFlow's SavedModel format and as an H5 file, ensuring it can be easily loaded for future predictions or evaluation.\n",
    "\n",
    "This pipeline is built to handle specifically formatted audio data and is optimized for high performance in audio classification tasks related to weather phenomena.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1dee0dca83d85f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "#import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603bc8f176bd22b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Configuration Dictionary for Audio Processing and Model Input\n",
    "\n",
    "The `SC` dictionary contains configuration settings that are essential for the audio preprocessing and the CNN model input preparation. Below are the details of each configuration:\n",
    "\n",
    "## Audio Processing Parameters:\n",
    "- **AUDIO_CLIP_DURATION**: The duration of the audio clip in seconds. Each audio file is processed to have this fixed length (2 seconds).\n",
    "- **AUDIO_NFFT**: The number of FFT (Fast Fourier Transform) points used to calculate the mel-spectrogram (2048 points).\n",
    "- **AUDIO_WINDOW**: The windowing function used in the FFT. It is set to `None`, meaning librosa's default will be used.\n",
    "- **AUDIO_STRIDE**: The stride (hop length) between successive FFTs during the spectrogram calculation (200 samples).\n",
    "- **AUDIO_SAMPLE_RATE**: The sampling rate used for audio files (16000 Hz).\n",
    "- **AUDIO_MELS**: The number of Mel bands used in the mel-spectrogram (260 bands).\n",
    "- **AUDIO_FMIN**: The lowest frequency to include when generating the mel-spectrogram (20 Hz).\n",
    "- **AUDIO_FMAX**: The highest frequency to include when generating the mel-spectrogram (13000 Hz).\n",
    "- **AUDIO_TOP_DB**: The threshold for the top decibels used in the dynamic range compression during log mel-spectrogram generation (80 dB).\n",
    "\n",
    "## Model Input Specifications:\n",
    "- **MODEL_INPUT_IMAGE_WIDTH**: The width of the input images to the CNN model (260 pixels).\n",
    "- **MODEL_INPUT_IMAGE_HEIGHT**: The height of the input images to the CNN model (260 pixels).\n",
    "- **MODEL_INPUT_IMAGE_CHANNELS**: The number of channels in the input images to the CNN model, corresponding to RGB channels (3 channels).\n",
    "\n",
    "These parameters are critical in ensuring that the audio data is uniformly processed and prepared in a format suitable for the CNN model training, helping in achieving consistent results and effective learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e98ee9fd2333ffe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SC = {\n",
    "    'AUDIO_CLIP_DURATION': 2,\n",
    "    'AUDIO_NFFT': 2048,\n",
    "    'AUDIO_WINDOW': None,\n",
    "    'AUDIO_STRIDE': 200,\n",
    "    'AUDIO_SAMPLE_RATE': 16000,\n",
    "    'AUDIO_MELS': 260,\n",
    "    'AUDIO_FMIN': 20,\n",
    "    'AUDIO_FMAX': 13000,\n",
    "    'AUDIO_TOP_DB': 80,\n",
    "\n",
    "    'MODEL_INPUT_IMAGE_WIDTH': 260,\n",
    "    'MODEL_INPUT_IMAGE_HEIGHT': 260,\n",
    "    'MODEL_INPUT_IMAGE_CHANNELS': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070f9649872e475",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Function: load_and_pad_audio\n",
    "\n",
    "The `load_and_pad_audio` function is designed to load an audio file, ensure it has a consistent duration, and handle discrepancies in length by either padding or trimming the audio data. Below are the steps and mechanisms involved:\n",
    "\n",
    "## Function Details:\n",
    "- **Parameters**:\n",
    "  - `file_path`: The path to the audio file.\n",
    "  - `duration`: The target duration of the audio in seconds. Default is 2 seconds.\n",
    "  - `sr`: The sample rate to use when loading the audio. Default is 16000 Hz.\n",
    "\n",
    "- **Process**:\n",
    "  1. **Loading**: The function attempts to load an audio file using `librosa.load` with the specified sample rate (`sr`).\n",
    "  2. **Duration Adjustment**:\n",
    "     - **Short Audio**: If the loaded audio is shorter than the required duration, it calculates the necessary padding on both ends to make the audio meet the specified duration.\n",
    "     - **Long Audio**: If the audio is longer than required, it trims the audio starting from the middle to the required length.\n",
    "  3. **Error Handling**: If the audio file cannot be loaded, an error message is printed, and the function returns `None` for both the audio and sample rate.\n",
    "\n",
    "- **Output**:\n",
    "  - Returns the adjusted audio array and the sample rate if the audio file is successfully processed. If there is an error, returns `None` for both.\n",
    "\n",
    "This function is crucial for preparing audio data with consistent length, which is essential for downstream processing like feature extraction and machine learning model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4b860f025ca6e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_and_pad_audio(file_path, duration=2, sr=16000):\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=sr)\n",
    "        required_samples = sr * duration\n",
    "        audio_length = len(audio)\n",
    "\n",
    "        if audio_length < required_samples:\n",
    "            pad_length = (required_samples - audio_length) // 2\n",
    "            audio = np.pad(audio, (pad_length, required_samples - audio_length - pad_length), \"constant\")\n",
    "        elif audio_length > required_samples:\n",
    "            start = (audio_length - required_samples) // 2\n",
    "            audio = audio[start:start + required_samples]\n",
    "        return audio, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31130759929a1b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Function: load_audio_files\n",
    "\n",
    "The `load_audio_files` function is responsible for loading and processing audio files from a specified directory, converting them into mel-spectrograms, and organizing them along with their labels for further use in machine learning training. Below is a detailed breakdown of its functionality:\n",
    "\n",
    "## Function Details:\n",
    "- **Parameter**:\n",
    "  - `path`: The directory path where audio files are organized by class labels in subdirectories.\n",
    "\n",
    "- **Process**:\n",
    "  1. **Directory Reading**: Identifies all subdirectories within the specified path, each representing a class label.\n",
    "  2. **Audio Processing**:\n",
    "     - For each class label, it iterates through the audio files in the corresponding subdirectory.\n",
    "     - Each audio file is loaded and processed to ensure it has a uniform duration using the `load_and_pad_audio` function.\n",
    "     - Converts the audio into a mel-spectrogram using librosa's `melspectrogram` function with predefined settings (FFT points, hop length, number of mel bands, etc.).\n",
    "     - Converts the mel-spectrogram to a decibel scale using `librosa.power_to_db`, which helps in normalizing the dynamic range.\n",
    "  3. **Data Collection**:\n",
    "     - The processed mel-spectrogram and its corresponding label are stored.\n",
    "     - Continues this process for all audio files, skipping any that cannot be loaded or processed.\n",
    "  4. **Output Compilation**:\n",
    "     - Compiles all successfully processed audio data into a list of mel-spectrograms and their corresponding labels.\n",
    "     - Also returns a list of class labels identified from the directory structure.\n",
    "\n",
    "- **Output**:\n",
    "  - Returns three items:\n",
    "    - `audios`: A list of mel-spectrogram arrays.\n",
    "    - `labels`: A list of labels corresponding to the mel-spectrograms.\n",
    "    - `class_labels`: A list of unique class labels derived from the directory names.\n",
    "\n",
    "This function plays a critical role in the preprocessing pipeline, ensuring that all audio files are consistently formatted and labeled correctly for effective model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea84bb6132d330",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_audio_files(path):\n",
    "    audios, labels = [], []\n",
    "    class_labels = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    for label in class_labels:\n",
    "        class_path = os.path.join(path, label)\n",
    "        for file in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, file)\n",
    "            audio, sr = load_and_pad_audio(file_path, duration=SC['AUDIO_CLIP_DURATION'], sr=SC['AUDIO_SAMPLE_RATE'])\n",
    "            if audio is None:\n",
    "                continue\n",
    "            mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr,\n",
    "                n_fft=SC['AUDIO_NFFT'],\n",
    "                hop_length=SC['AUDIO_STRIDE'],\n",
    "                n_mels=SC['AUDIO_MELS'],\n",
    "                fmin=SC['AUDIO_FMIN'],\n",
    "                fmax=SC['AUDIO_FMAX'])\n",
    "            log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, top_db=SC['AUDIO_TOP_DB'])\n",
    "            audios.append(log_mel_spectrogram)\n",
    "            labels.append(label)\n",
    "    return audios, labels, class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc594d40706cc659",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preparing Spectrograms and Data Splitting for Training\n",
    "\n",
    "This section of the code is responsible for resizing spectrograms to a uniform target size, encoding labels for classification, padding spectrograms for uniformity, and splitting the data into training and testing sets. Here is a detailed breakdown of each part:\n",
    "\n",
    "## Functions and Processes:\n",
    "- **prepare_spectrograms**:\n",
    "  - **Purpose**: Resizes each spectrogram to a specified target size and adjusts the channel dimension to match the input requirements of the CNN model.\n",
    "  - **Process**:\n",
    "    - Converts each spectrogram to a 3-channel image to mimic RGB data, which is typical input for pre-trained CNN models.\n",
    "    - Uses bilinear interpolation for resizing which is a standard method for image data.\n",
    "\n",
    "- **Label Encoding**:\n",
    "  - Utilizes `LabelEncoder` to transform textual class labels into unique integers.\n",
    "  - Transforms these integer labels into one-hot encoded format using `to_categorical`, making them suitable for model training.\n",
    "\n",
    "- **Spectrogram Padding**:\n",
    "  - Ensures all spectrograms have the same dimensions by padding them with zeros where necessary. This is crucial for batching in neural network training.\n",
    "\n",
    "- **Data Splitting**:\n",
    "  - Divides the data into training and testing sets with a split ratio of 80% training and 20% testing. This helps in evaluating the model's performance on unseen data.\n",
    "\n",
    "## Code Execution:\n",
    "- Loads audio files from the specified path and processes them into mel-spectrograms.\n",
    "- Encodes labels and prepares spectrograms for neural network input.\n",
    "- Splits the prepared spectrogram data and corresponding labels into training and testing datasets.\n",
    "\n",
    "## Outputs:\n",
    "- **X_train, X_test**: Training and testing sets of spectrograms.\n",
    "- **y_train, y_test**: Corresponding training and testing sets of labels.\n",
    "\n",
    "This code segment is crucial for preparing the input data in a way that aligns with the requirements of sophisticated machine learning models, ensuring consistency and usability across different computational processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9adaafcf8bbc99d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\project-echo\\Lib\\site-packages\\librosa\\feature\\spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def prepare_spectrograms(spectrograms, target_size=(260, 260)):\n",
    "    resized_spectrograms = np.array([tf.image.resize(spect[:, :, np.newaxis], target_size, method='bilinear').numpy() for spect in spectrograms])\n",
    "    resized_spectrograms = np.repeat(resized_spectrograms, 3, axis=3)\n",
    "    return resized_spectrograms\n",
    "\n",
    "path = r\"D:\\Deakin\\Project Echo\\Weather_Sounds\\Weather_Sounds_train_test\"\n",
    "audios, labels, class_labels = load_audio_files(path)\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "max_length = max(audio.shape[1] for audio in audios)\n",
    "max_height = max(audio.shape[0] for audio in audios)\n",
    "audios_padded = np.array([\n",
    "    np.pad(audio, ((0, max_height - audio.shape[0]), (0, max_length - audio.shape[1])), 'constant')\n",
    "    for audio in audios\n",
    "])\n",
    "\n",
    "prepared_spectrograms = prepare_spectrograms(audios_padded)\n",
    "X_train, X_test, y_train, y_test = train_test_split(prepared_spectrograms, labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a1522233eac8f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Function: build_model\n",
    "\n",
    "The `build_model` function constructs a convolutional neural network (CNN) tailored for image-based classification, designed to handle preprocessed audio data represented as spectrograms. Here's how the function is structured and what each part accomplishes:\n",
    "\n",
    "## Parameters:\n",
    "- **input_shape**: A tuple defining the shape of the input data, including height, width, and number of channels.\n",
    "- **num_classes**: The number of unique labels or classes in the dataset, which determines the output layer's size.\n",
    "\n",
    "## Model Architecture:\n",
    "1. **Initial Convolution Layer**:\n",
    "   - Applies a 32-filter convolutional layer with a kernel size of (3x3), using 'same' padding and 'relu' activation. This layer is designed to extract initial features from the input data without reducing its dimensionality.\n",
    "2. **Batch Normalization and Pooling**:\n",
    "   - Follows each convolution layer with batch normalization, which normalizes the activations of the previous layer, helping in faster convergence and more stable training.\n",
    "   - Uses max pooling with a (2x2) window to reduce the spatial dimensions of the feature maps, effectively summarizing the features.\n",
    "3. **Additional Convolution Layers**:\n",
    "   - Stacks two more convolution layers with increasing number of filters (64 and 128), each followed by batch normalization, max pooling, and dropout layers. These layers increase the model's capacity to learn more complex features and include dropout to prevent overfitting.\n",
    "4. **Global Average Pooling**:\n",
    "   - Applies global average pooling to reduce each feature map to a single value, reducing the total number of parameters and decreasing the risk of overfitting.\n",
    "5. **Fully Connected Layers**:\n",
    "   - A dense layer with 256 units and 'relu' activation processes the pooled features, followed by a dropout layer.\n",
    "   - The final output layer with a number of units equal to `num_classes`, using a 'softmax' activation to output probabilities for each class.\n",
    "\n",
    "## Compilation:\n",
    "- The model is compiled with the 'adam' optimizer and 'categorical_crossentropy' loss function, which are suitable for multi-class classification tasks.\n",
    "- It also tracks 'accuracy' as a metric to evaluate the model's performance during training.\n",
    "\n",
    "## Output:\n",
    "- Returns the fully constructed and compiled CNN model, ready to be trained on spectrogram data for audio classification.\n",
    "\n",
    "This function encapsulates a robust architecture suitable for handling complex patterns in spectrogram images, making it ideal for tasks like audio classification where distinguishing features might be subtle yet crucial for accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd9af6da999233b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\project-echo\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (SC['MODEL_INPUT_IMAGE_WIDTH'], SC['MODEL_INPUT_IMAGE_HEIGHT'], SC['MODEL_INPUT_IMAGE_CHANNELS'])\n",
    "model = build_model(input_shape, num_classes=len(class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a5de4e5146424",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model Training Setup and Callbacks\n",
    "\n",
    "This section of the code defines the training process, including data augmentation, training execution, and the use of callbacks to enhance model training. Each component is crafted to optimize the model's performance and handle the training process efficiently.\n",
    "\n",
    "## Training Callbacks:\n",
    "- **ModelCheckpoint**:\n",
    "  - Saves the model after every epoch if it shows improvement over the previous best model, based on validation loss (`val_loss`).\n",
    "  - `save_best_only=True` ensures that only the top-performing model is saved, optimizing storage and focusing on the best results.\n",
    "  - `mode='min'` specifies that the `val_loss` should minimize for improvement.\n",
    "- **ReduceLROnPlateau**:\n",
    "  - Reduces the learning rate when the validation loss has stopped improving.\n",
    "  - `factor=0.1` reduces the learning rate to 10% of its current value.\n",
    "  - `patience=10` waits for 10 epochs without improvement before reducing the learning rate.\n",
    "  - `min_lr=0.00001` sets a lower bound on the learning rate to prevent it from decreasing too much.\n",
    "  - `verbose=1` ensures messages about learning rate reduction are printed.\n",
    "- **EarlyStopping**:\n",
    "  - Stops training when the validation loss has not improved for a given number of epochs (`patience=20`).\n",
    "  - `restore_best_weights=True` ensures that the model's weights are reverted to the best encountered during training upon early stop.\n",
    "\n",
    "## Data Augmentation:\n",
    "- Uses `ImageDataGenerator` to artificially enhance the size and diversity of the training dataset by applying random transformations that include:\n",
    "  - `rotation_range=20`: Rotates images by up to 20 degrees.\n",
    "  - `width_shift_range=0.2` and `height_shift_range=0.2`: Shifts images along the width and height by up to 20% of their dimensions.\n",
    "  - `shear_range=0.2`: Applies shear transformations.\n",
    "  - `zoom_range=0.2`: Zooms into images by up to 20%.\n",
    "  - `horizontal_flip=True`: Flips images horizontally (not typically useful for audio data but included for completeness).\n",
    "  - `fill_mode='nearest'`: Uses the nearest pixels to fill in new pixels when applying transformations.\n",
    "\n",
    "## Training Execution:\n",
    "- The model is trained using the `fit` method on batches of data provided by `train_generator`.\n",
    "- `epochs=100` sets the number of passes over the complete dataset.\n",
    "- The training and validation datasets are specified, and the aforementioned callbacks are utilized to monitor and optimize the training process.\n",
    "\n",
    "## Outputs:\n",
    "- `history`: Captures the training history, including metrics such as loss and accuracy for both training and validation phases.\n",
    "\n",
    "This training configuration leverages modern techniques in machine learning to ensure that the model learns effectively from the augmented data, adapts to new challenges dynamically through callbacks, and stops at the optimal time to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe394be6f08b671",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\project-echo\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/294\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4:40\u001b[0m 2s/step - accuracy: 0.8325 - loss: 0.4333"
     ]
    },
    {
     "ename": "AbortedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall/gradient_tape/sequential_1/conv2d_2_1/convolution/Conv2DBackpropFilter defined at (most recent call last):\n<stack traces unavailable>\nOperation received an exception:Status: 1, message: could not create a primitive, in file tensorflow/core/kernels/mkl/mkl_conv_grad_filter_ops.cc:685\n\t [[{{node StatefulPartitionedCall/gradient_tape/sequential_1/conv2d_2_1/convolution/Conv2DBackpropFilter}}]] [Op:__inference_one_step_on_iterator_74118]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m      7\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m      8\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      9\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\project-echo\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\project-echo\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAbortedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall/gradient_tape/sequential_1/conv2d_2_1/convolution/Conv2DBackpropFilter defined at (most recent call last):\n<stack traces unavailable>\nOperation received an exception:Status: 1, message: could not create a primitive, in file tensorflow/core/kernels/mkl/mkl_conv_grad_filter_ops.cc:685\n\t [[{{node StatefulPartitionedCall/gradient_tape/sequential_1/conv2d_2_1/convolution/Conv2DBackpropFilter}}]] [Op:__inference_one_step_on_iterator_74118]"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint('models/best_model.weights.h5', save_best_only=True, monitor='val_loss', mode='min',save_weights_only=True)\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "history = model.fit(x=train_generator, epochs=100, validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpoint_callback, reduce_lr_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ebfe4f35b4f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plotting Training and Validation Loss\n",
    "\n",
    "This code segment is used to visualize the training and validation loss over epochs, providing a graphical representation of the model's learning progress. Here's an overview of how it is structured:\n",
    "\n",
    "## Code Explanation:\n",
    "- `plt.plot(history.history['loss'], label='Training loss')`: Plots the training loss values stored in `history.history['loss']` across all epochs. This line adds a label \"Training loss\" to distinguish it in the plot.\n",
    "- `plt.plot(history.history['val_loss'], label='Validation loss')`: Similarly, this line plots the validation loss values from `history.history['val_loss']`, labeled as \"Validation loss\".\n",
    "- `plt.title('Training and Validation Loss')`: Sets the title of the plot to \"Training and Validation Loss\" to indicate what the graph represents.\n",
    "- `plt.legend()`: Adds a legend to the plot, which helps in distinguishing between the training and validation loss lines.\n",
    "- `plt.show()`: Displays the plot. This function call ensures that the plot is rendered and shown to the user.\n",
    "\n",
    "## Purpose:\n",
    "- **Visualization**: This plot is crucial for understanding how well the model is learning and generalizing over time. It helps in identifying patterns such as overfitting or underfitting based on the divergence or convergence of these two lines.\n",
    "- **Monitoring**: Allows developers and researchers to monitor the training process visually, making it easier to decide whether further training is necessary or if adjustments need to be made to the training process.\n",
    "\n",
    "## Output:\n",
    "- A line graph showing the loss on the vertical axis and the number of epochs on the horizontal axis. The training loss is typically shown in one color and the validation loss in another to clearly illustrate the differences and trends during the training phase.\n",
    "\n",
    "This visualization is an essential part of machine learning model training, as it provides immediate visual feedback on the effectiveness and progression of the training regime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f693083714a9ba4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a46a3e6b80565",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Saving the Trained Model\n",
    "\n",
    "This section of the code is dedicated to saving the trained model for later use, ensuring that all the training effort can be utilized effectively in practical applications. The model is saved in two different formats to provide flexibility in deployment and further usage.\n",
    "\n",
    "## Code Explanation:\n",
    "- **Directory Setup**:\n",
    "  - `model_dir = \"weather_audio_detection_model\"`: Defines a string variable that specifies the directory name where the TensorFlow SavedModel will be stored.\n",
    "- **Save as TensorFlow SavedModel**:\n",
    "  - `tf.saved_model.save(model, model_dir)`: Saves the entire model in TensorFlow's SavedModel format, which includes the architecture, weights, and the training configuration. This format is ideal for serving via TensorFlow Serving and can be useful for further fine-tuning or transfer learning.\n",
    "- **Save as HDF5 File**:\n",
    "  - `model.save('WeatherAudioDetectionModel.h5')`: Saves the model as an HDF5 file, a versatile storage format that is widely used in data-intensive environments, which is particularly useful for loading the model in other Python environments and for integration with other Python-based tools.\n",
    "\n",
    "## Purpose:\n",
    "- **Versatility and Compatibility**: Saving the model in these two formats ensures that it can be easily integrated and deployed across different platforms and applications.\n",
    "- **Preservation**: These methods ensure the preservation of the model's state after training, allowing for easy replication of results and further analysis at any time.\n",
    "\n",
    "## Output:\n",
    "- Two files are generated and saved:\n",
    "  - A directory named `weather_audio_detection_model` containing the SavedModel.\n",
    "  - An HDF5 file named `WeatherAudioDetectionModel.h5`.\n",
    "\n",
    "These saving methods are critical for the lifecycle of machine learning projects, providing the means to utilize trained models beyond the immediate environment in which they were trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c8899b5fbe95dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_dir = \"weather_audio_detection_model\"\n",
    "# tf.saved_model.save(model, model_dir)\n",
    "model.save('WeatherAudioDetectionModel.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
