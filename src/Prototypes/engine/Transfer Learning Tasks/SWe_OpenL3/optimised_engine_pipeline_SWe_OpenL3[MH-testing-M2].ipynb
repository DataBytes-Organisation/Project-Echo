{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimised Engine Pipeline\n",
    "\n",
    "The notebook provide an optimised version of the generic engine pipeline.\n",
    "\n",
    "This notebook aims to enhance the engine pipeline through:\n",
    "\n",
    "(1) Be compatible with Lambda Stack (https://lambdalabs.com/lambda-stack-deep-learning-software) to enable containisation, cloud and cross-platform development\n",
    "\n",
    "(2) Upgrade tensorflow from 2.10 to 2.11. Version 2.13.0 is also compatible with Keras version 2.12.0.\n",
    "\n",
    "(3) Enable execution on linux and Windows 11 WSL2 environments\n",
    "\n",
    "(3) Minimise library dependencies\n",
    "\n",
    "(4) Solve the parallel pipeline execution issues inherent in the generic_engine_pipeline for faster execution of the pipeline\n",
    "\n",
    "Author: akudilczak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version           :  3.10.12\n",
      "TensorFlow Version       :  2.13.0\n",
      "Keras Version            :  2.13.1\n",
      "Librosa Version          :  0.10.1\n",
      "Audiomentations Version  :  0.33.0\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# library imports\n",
    "########################################################################################\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# environment settings\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# generic libraries\n",
    "from platform import python_version\n",
    "import functools\n",
    "from functools import lru_cache\n",
    "import diskcache as dc\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensor flow / keras related libraries\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa \n",
    "from keras.utils import dataset_utils\n",
    "\n",
    "# image processing related libraries\n",
    "import librosa\n",
    "\n",
    "# audio processing libraries\n",
    "import audiomentations\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "# print system information\n",
    "print('Python Version           : ', python_version())\n",
    "print('TensorFlow Version       : ', tf.__version__)\n",
    "print('Keras Version            : ', keras.__version__)\n",
    "\n",
    "print('Librosa Version          : ', librosa.__version__)\n",
    "print('Audiomentations Version  : ', audiomentations.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Configuration\n",
    "\n",
    "The following code sets up the pipeline with configuration options.\n",
    "\n",
    "The key is to set the audio data directory to the root directory containing the folders with raw audio files. \n",
    "\n",
    "This expects the folders names to be the species names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# system constants\n",
    "########################################################################################\n",
    "\n",
    "SC = {\n",
    "    'AUDIO_DATA_DIRECTORY': \"/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound\",\n",
    "    'CACHE_DIRETORY': \"/Volumes/UGREEN Samsung SSD/project echo audio/test data/Temp Cache\",\n",
    "\n",
    "    'AUDIO_CLIP_DURATION': 2, # seconds\n",
    "    'AUDIO_SAMPLE_RATE': 48000,\n",
    "\n",
    "    'USE_DISK_CACHE': True,\n",
    "    'SAMPLE_VARIANTS': 20,\n",
    "    'CLASSIFIER_BATCH_SIZE': 16,\n",
    "    'MAX_EPOCHS': 5000,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_memory_limit(mem_mb):\n",
    "  # enforce memory limit on GPU\n",
    "\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if gpus:\n",
    "    try:\n",
    "      tf.config.experimental.set_virtual_device_configuration(\n",
    "          gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=mem_mb)])\n",
    "      print(f\"vram limit set to {mem_mb}MB\")\n",
    "    except RuntimeError as e:\n",
    "      print(e)\n",
    "      \n",
    "# enforce max 5GB memory on GPU for this notebook if you have a small GPU\n",
    "# enforce_memory_limit(5120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk Caching\n",
    "\n",
    "The following code creates a disk cache.  You will need lots of space (20 GB+) if you create large melspectrograms.\n",
    "\n",
    "The caching works by serialising a function call signature and hashing it into a key.  This key is used to store the result of the function call.\n",
    "\n",
    "This allows the a result from the cache to be utilised instead of calling the function, which means the entire data processing pipeline can be cached if used correctly.\n",
    "\n",
    "This works best when the function being cached is idempotent.  There may be circumstances where it doesn't matter.  Be careful with using this cache as you may get unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create a DiskCache instance\n",
    "# This cache will allow us store intermediate function results to speed up the \n",
    "# data processing pipeline\n",
    "########################################################################################\n",
    "if SC['USE_DISK_CACHE']:\n",
    "    cache = dc.Cache(SC['CACHE_DIRETORY'], cull_limit=0, size_limit=10**9) \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# a helper function to create a hash key from a function signature and arguments\n",
    "########################################################################################\n",
    "def create_function_key(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    func_name = partial_func.func.__name__\n",
    "    func_module = partial_func.func.__module__\n",
    "    args_repr = repr(partial_func.args)\n",
    "    kwargs_repr = repr(sorted(partial_func.keywords.items()))\n",
    "\n",
    "    key = f\"{func_module}.{func_name}:{args_repr}:{kwargs_repr}\"\n",
    "    # Use hashlib to create a hash of the key for shorter and consistent length\n",
    "    key_hash = hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "    return key, key_hash, partial_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the files into a Tensorflow dataset structure for model training\n",
    "\n",
    "This initial code loads only the filenames.  The filenames are then split into train, validation and test datasets.  This is designed deliberately this way to conserve runtime memory.\n",
    "\n",
    "Subsequent downstream loading of the file content occurs as part of the data pipeline transformation 'map' function.  See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# these helper functions load the audio data into a 'dataset' using only paths\n",
    "# just dealing with paths at this early stage means the entire dataset can be shuffled in\n",
    "# memory and split before loading the actual audio data into memory\n",
    "########################################################################################\n",
    "def paths_and_labels_to_dataset(audio_paths, labels, num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    zipped_path_ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    return zipped_path_ds\n",
    "\n",
    "def create_datasets(audio_files, train_split=0.7, val_split=0.2):\n",
    "    file_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            audio_files,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.ogg','.mp3','.wav','.flac'),\n",
    "            class_names=None,\n",
    "            shuffle=True,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        audio_paths=file_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    # Calculate the size of the dataset\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # Calculate the number of elements for each dataset split\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size).take(val_size)\n",
    "    test_ds = dataset.skip(train_size + val_size).take(test_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2474 files belonging to 105 classes.\n",
      "Class names:  ['Acanthagenys rufogularis', 'Acanthiza apicalis', 'Acanthiza lineata', 'Acanthiza pusilla', 'Acanthorhynchus tenuirostris', 'Accipiter novaehollandiae', 'Acridotheres tristis', 'Acrocephalus australis', 'Aegotheles cristatus', 'Alauda arvensis', 'Amaurornis moluccana', 'Anas castanea', 'Anthochaera chrysoptera', 'Aprosmictus erythropterus', 'Atrichornis clamosus', 'Bubulcus ibis', 'Cacomantis flabelliformis', 'Calamanthus fuliginosus', 'Caligavis chrysops', 'Caprimulgus macrurus', 'Charadrius ruficapillus', 'Chenonetta jubata', 'Colluricincla harmonica', 'Conopophila albogularis', 'Cormobates leucophaea', 'Corvus coronoides', 'Corvus orru', 'Cracticus nigrogularis', 'Cracticus torquatus', 'Dacelo novaeguineae', 'Dasyornis longirostris', 'Dicaeum hirundinaceum', 'Dicrurus bracteatus', 'Elanus axillaris', 'Entomyzon cyanotis albipennis', 'Eurostopodus mystacalis', 'Falco cenchroides', 'Falcunculus frontatus', 'Gallinula tenebrosa', 'Geopelia humeralis', 'Gerygone palpebrosa', 'Grallina cyanoleuca', 'Gymnorhina tibicen [tibicen Group]', 'Haliastur sphenurus', 'Hylacola pyrrhopygia', 'Lathamus discolor', 'Lewinia pectoralis', 'Lichenostomus cratitius', 'Lichmera indistincta', 'Malurus cyaneus', 'Malurus splendens', 'Manorina melanophrys', 'Megapodius reinwardt', 'Meliphaga lewinii', 'Melithreptus albogularis', 'Melithreptus brevirostris', 'Menura novaehollandiae', 'Microeca flavigaster [flavigaster Group]', 'Myiagra alecto', 'Myiagra inquieta', 'Myiagra rubecula', 'Myzomela erythrocephala', 'Ninox strenua', 'Orthonyx temminckii', 'Pachycephala fuliginosa', 'Pachycephala inornata', 'Pachycephala melanura', 'Pachycephala rufiventris', 'Pachycephala simplex simplex', 'Pardalotus punctatus', 'Pardalotus striatus', 'Pardalotus striatus [melanocephalus Group]', 'Parvipsitta pusilla', 'Petrochelidon nigricans', 'Petroica goodenovii', 'Phaps elegans', 'Philemon buceroides', 'Philemon buceroides [buceroides Group]', 'Philemon citreogularis', 'Phylidonyris novaehollandiae', 'Phylidonyris pyrrhopterus', 'Pomatostomus temporalis', 'Psophodes cristatus', 'Psophodes nigrogularis', 'Psophodes nigrogularis nigrogularis', 'Ptilinopus regina', 'Ptilotula penicillata', 'Pyrrholaemus brunneus', 'Rhipidura albiscapa', 'Rhipidura rufiventris isura', 'Sericornis frontalis', 'Sphecotheres vieilloti', 'Spilopelia chinensis', 'Sterna hirundo', 'Stipiturus malachurus', 'Stizoptera bichenovii', 'Stomiopera unicolor', 'Tachybaptus novaehollandiae', 'Trichoglossus chlorolepidotus', 'Trichoglossus moluccanus', 'Turnix maculosus', 'Vanellus miles', 'Zapornia tabuensis', 'Zosterops lateralis', 'Zosterops luteus']\n",
      "Training   dataset length: 1979\n",
      "Validation dataset length: 470\n",
      "Test       dataset length: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 15:24:18.054556: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2023-09-16 15:24:18.054578: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2023-09-16 15:24:18.054585: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2023-09-16 15:24:18.054612: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-09-16 15:24:18.054627: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "train_ds, val_ds, test_ds, class_names = create_datasets(SC['AUDIO_DATA_DIRECTORY'],train_split=0.8, val_split=0.19)\n",
    "print(\"Class names: \", class_names)\n",
    "print(f\"Training   dataset length: {len(train_ds)}\")\n",
    "print(f\"Validation dataset length: {len(val_ds)}\")\n",
    "print(f\"Test       dataset length: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Malurus splendens/region_137.200-139.200.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Stomiopera unicolor/region_60.450-62.200.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Malurus splendens/region_193.200-195.200.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Phylidonyris pyrrhopterus/region_11.300-13.300.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Psophodes nigrogularis/region_32.800-34.800.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Menura novaehollandiae/region_63.150-64.300.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Phylidonyris novaehollandiae/region_28.000-30.000.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Acanthiza apicalis/region_170.500-171.350.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Zosterops lateralis/region_26.000-28.000.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'/Volumes/UGREEN Samsung SSD/project echo audio/test data/eBird Testing Subset Sorted sound/Malurus splendens/region_173.900-175.900.wav'>, <tf.Tensor: shape=(105,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for item in train_ds.take(10):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "# enable this cache if using large audio files, and lots of available RAM\n",
    "def python_load_and_decode_file(sample, label, variant, cache_key, cache_found):\n",
    "    if sample.decode('utf-8').endswith('.mp3'):\n",
    "        audio = AudioSegment.from_mp3(sample.decode('utf-8'))\n",
    "        sample = sample.decode('utf-8').replace('.mp3', '.wav')\n",
    "        audio.export(sample, format=\"wav\")\n",
    "\n",
    "    if cache_found == np.int32(0):\n",
    "        \n",
    "        tmp_audio_t = None\n",
    "        \n",
    "        with open(sample, 'rb') as file:\n",
    "\n",
    "            # Load the audio data with librosa\n",
    "            tmp_audio_t, _ = librosa.load(file, sr=SC['AUDIO_SAMPLE_RATE'])\n",
    "            \n",
    "            # cast and keep right channel only\n",
    "            if tmp_audio_t.ndim == 2 and tmp_audio_t.shape[0] == 2:\n",
    "                tmp_audio_t = tmp_audio_t[1, :]\n",
    "            \n",
    "            # cast and keep right channel only\n",
    "            tmp_audio_t = tmp_audio_t.astype(np.float32)\n",
    "                    \n",
    "            assert(tmp_audio_t is not None)\n",
    "            assert(isinstance(tmp_audio_t, np.ndarray))\n",
    "        \n",
    "        sample = tmp_audio_t\n",
    "        \n",
    "    else:\n",
    "        sample = cache[cache_key.decode('utf-8')]\n",
    "    \n",
    "    return sample, label, variant, cache_key, cache_found\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling the audio files\n",
    "\n",
    "The function loads and decodes an audio file from the given path, calculates the audio file's duration in seconds, and then extracts a random subsection of the specified duration (in seconds) from the audio. If the audio duration is shorter than the specified duration, the function pads the subsection with silence to meet the required length. The resulting subsection is returned as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_load_random_subsection(sample, label, variant, cache_key, cache_found):\n",
    "    \n",
    "    if cache_found == np.int32(0):\n",
    "        duration_secs = SC['AUDIO_CLIP_DURATION']\n",
    "        \n",
    "        # Determine the audio file's duration in seconds\n",
    "        audio_duration_secs = tf.shape(sample)[0] / SC['AUDIO_SAMPLE_RATE']\n",
    "        \n",
    "        if audio_duration_secs>duration_secs:\n",
    "        \n",
    "            # Calculate the starting point of the 5-second subsection\n",
    "            max_start = tf.cast(audio_duration_secs - duration_secs, tf.float32)\n",
    "            start_time_secs = tf.random.uniform((), 0.0, max_start, dtype=tf.float32)\n",
    "            \n",
    "            start_index = tf.cast(start_time_secs * SC['AUDIO_SAMPLE_RATE'], dtype=tf.int32)\n",
    "    \n",
    "            # Load the 5-second subsection\n",
    "            end_index = tf.cast(start_index + tf.cast(duration_secs, tf.int32) * SC['AUDIO_SAMPLE_RATE'], tf.int32)\n",
    "            \n",
    "            subsection = sample[start_index : end_index]\n",
    "        \n",
    "        else:\n",
    "            # Pad the subsection with silence if it's shorter than 5 seconds\n",
    "            padding_length = duration_secs * SC['AUDIO_SAMPLE_RATE'] - tf.shape(sample)[0]\n",
    "            padding = tf.zeros([padding_length], dtype=sample.dtype)\n",
    "            subsection = tf.concat([sample, padding], axis=0)\n",
    "\n",
    "        sample = subsection\n",
    "\n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio augmentations\n",
    "\n",
    "The following code applies a sequence of augmentations to the audio signal.\n",
    "\n",
    "A probability of applying the augmentation is used to ensure the augmentation isn't applied every sample.\n",
    "\n",
    "This means there will be some samples that go straight through with no augmentations and a small probability that in fact all augmentations will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio augmentation pipeline\n",
    "def python_audio_augmentations(sample, label, variant, cache_key, cache_found):\n",
    "    \n",
    "    if cache_found == np.int32(0):\n",
    "        # See https://github.com/iver56/audiomentations for more options\n",
    "        augmentations = Compose([\n",
    "            # Add Gaussian noise with a random amplitude to the audio\n",
    "            # This can help the model generalize to real-world scenarios where noise is present\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.2),\n",
    "\n",
    "            # Time-stretch the audio without changing its pitch\n",
    "            # This can help the model become invariant to small changes in the speed of the audio\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.2),\n",
    "\n",
    "            # Shift the pitch of the audio within a range of semitones\n",
    "            # This can help the model generalize to variations in pitch that may occur in real-world scenarios\n",
    "            PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n",
    "\n",
    "            # Shift the audio in time by a random fraction\n",
    "            # This can help the model become invariant to the position of important features in the audio\n",
    "            Shift(min_fraction=-0.5, max_fraction=0.5, p=0.2),\n",
    "        ])\n",
    "        \n",
    "        # apply audio augmentation to the clip\n",
    "        # note: this augmentation is NOT applied in the test and validation pipelines\n",
    "        sample = augmentations(samples=sample, sample_rate=SC['AUDIO_SAMPLE_RATE'])\n",
    "    \n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Augmentations\n",
    "\n",
    "After the melspectrogram pipeline is executed, a 2D image is created of the signal energy at frequence/time points.\n",
    "\n",
    "This image is augmented by applying the function below.  In this example a random rotation of between -2 and 2 degrees is applied every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image augmentation pipeline\n",
    "#def tensorflow_image_augmentations(sample, label, variant, cache_key, cache_found):\n",
    "\n",
    "   # if cache_found == np.int32(0):\n",
    "        # random rotation -2 deg to 2 deg\n",
    "     #   degrees = tf.random.uniform(shape=(1,), minval=-2, maxval=2)\n",
    "        \n",
    "        # convert the angle in degree to radians\n",
    "    #    radians = degrees * 0.017453292519943295  \n",
    "\n",
    "        # rotate the image\n",
    "     #   sample = tfa.image.rotate(sample, radians, interpolation='bilinear')\n",
    "    \n",
    "   # return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openl3_feature_extraction_pipeline(sample, label, variant, cache_key, cache_found):\n",
    "    if cache_found == np.int32(0):\n",
    "        # Adjust the length of the sample to match the expected input shape of the model\n",
    "        sample = sample[:48000]  # Take the first 48000 samples\n",
    "        sample = np.pad(sample, (0, 48000 - len(sample)), 'constant')  # Pad with zeros if less than 48000\n",
    "        sample = sample.reshape(1, -1)  # Reshape to match the expected input shape\n",
    "        \n",
    "        # Call the OpenL3 model to extract the features from the audio data\n",
    "        features = openl3_model.predict(sample[np.newaxis, ...])\n",
    "        \n",
    "        # OpenL3 model will return the features as a numpy array, \n",
    "        sample = features[0]\n",
    "    \n",
    "    return sample, label, variant, cache_key, cache_found\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual dataset pipelines\n",
    "\n",
    "The dataset is split into different pipeline function as that augmentations would be applied to the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_add_variant_and_cache(path, label):\n",
    "    variant     = tf.random.uniform(shape=(), minval=0, maxval=SC['SAMPLE_VARIANTS'], dtype=tf.int32)\n",
    "    sample      = path\n",
    "    cache_key   = b'no key'\n",
    "    cache_found = np.int32(0)\n",
    "    return sample, label, variant, cache_key, cache_found\n",
    "\n",
    "def tensorflow_drop_variant_and_cache(sample, label, variant, cache_key, cache_found):\n",
    "    return sample, label\n",
    "\n",
    "def tensorflow_output_shape_setter(sample, label, variant, cache_key, cache_found):\n",
    "    # Set the output shape based on the output shape of the OpenL3 model\n",
    "    sample.set_shape([6144])  \n",
    "    label.set_shape([len(class_names),]) \n",
    "    return sample, label, variant, cache_key, cache_found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_fuction_wrapper(pipeline_fn, out_types, sample, label, variant, cache_key, cache_found):\n",
    "\n",
    "    # Use a lambda function to pass two arguments to the function\n",
    "    sample, label, variant, cache_key, cache_found = tf.numpy_function(\n",
    "        func=lambda v1,v2,v3,v4,v5: pipeline_fn(v1,v2,v3,v4,v5),\n",
    "        inp=(sample, label, variant, cache_key, cache_found),\n",
    "        Tout=out_types)\n",
    "\n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_disk_cache_start(sample, label, variant, cache_key, cache_found):\n",
    "\n",
    "    cache_key   = b'no key'\n",
    "    cache_found = np.int32(0)\n",
    "    \n",
    "    if SC['USE_DISK_CACHE']:\n",
    "        _,cache_key,_ = create_function_key(python_disk_cache_start, sample, label, variant)\n",
    "        if cache_key in cache:\n",
    "            #print(f'found {cache_key} in cache')\n",
    "            cache_found = np.int32(1)\n",
    "        else:\n",
    "            pass\n",
    "            #print(f'{cache_key} not found in cache')\n",
    "            \n",
    "    return sample, label, variant, cache_key, cache_found\n",
    "\n",
    "def python_disk_cache_end(sample, label, variant, cache_key, cache_found):\n",
    "    cache_key = cache_key.decode('utf-8')\n",
    "    if SC['USE_DISK_CACHE']:\n",
    "        # if it was not found in the cache at the start, then populate with what we built\n",
    "        # during the pipeline execution\n",
    "        if cache_found == np.int32(0):\n",
    "            #print(f'adding {cache_key} to cache')\n",
    "            cache[cache_key] = sample\n",
    "        #else:\n",
    "        #    sample = cache[cache_key]\n",
    "            \n",
    "    return sample, label, variant, cache_key, cache_found\n",
    "\n",
    "def python_openl3_feature_extraction(sample, label, variant, cache_key, cache_found):\n",
    "    \n",
    "    if cache_found == np.int32(0):\n",
    "        # Adjust the length of the sample to match the expected input shape of the model\n",
    "        sample = sample[:48000]  # Take the first 48000 samples\n",
    "        sample = np.pad(sample, (0, 48000 - len(sample)), 'constant')  # Pad with zeros if less than 48000\n",
    "        sample = sample.reshape(1, -1)  # Reshape to match the expected input shape\n",
    "        \n",
    "        # The OpenL3 model expects the input shape to be (batch, time, channel), \n",
    "        # so we need to expand the dimensions of the sample to match this.\n",
    "        sample_expanded = np.expand_dims(sample, axis=0)\n",
    "        \n",
    "        # Use the OpenL3 model to extract the features\n",
    "        features = openl3_model.predict(sample_expanded)\n",
    "        \n",
    "        # We then squeeze the output to remove the batch dimension\n",
    "        sample = np.squeeze(features, axis=0)\n",
    "    \n",
    "    return sample, label, variant, cache_key, cache_found\n",
    "\n",
    "\n",
    "import openl3\n",
    "openl3_model = openl3.models.load_audio_embedding_model(input_repr=\"mel256\", content_type=\"music\", embedding_size=6144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create the datasets necessary for training a classification model\n",
    "# Note: python and tensorflow functions are treated differently in the tensorflow\n",
    "# pipeline.  Each python function needs to be wrapped. \n",
    "# this is why each pipeline function starts with python_ or tensorflow_ to make it clear \n",
    "########################################################################################\n",
    "\n",
    "# Get the length of the training dataset\n",
    "len_train_ds = len(train_ds)\n",
    "parallel_calls = tf.data.AUTOTUNE\n",
    "cache_output_types = (tf.string,tf.float32,tf.int32,tf.string,tf.int32)\n",
    "procs_output_types = (tf.float32,tf.float32,tf.int32,tf.string,tf.int32)\n",
    "\n",
    "# Create the training dataset pipeline\n",
    "train_dataset = (train_ds\n",
    "                 .shuffle(len_train_ds)\n",
    "                 .map(tensorflow_add_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_disk_cache_start, cache_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_load_and_decode_file, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_load_random_subsection, num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_audio_augmentations, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, openl3_feature_extraction_pipeline, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_openl3_feature_extraction, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_disk_cache_end, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_output_shape_setter, num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_drop_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                 .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                 .prefetch(parallel_calls)\n",
    "                 .repeat(count=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation dataset pipeline\n",
    "validation_dataset = (val_ds\n",
    "                    .map(tensorflow_add_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                    .map(functools.partial(python_fuction_wrapper, python_disk_cache_start, cache_output_types), num_parallel_calls=parallel_calls)\n",
    "                    .map(functools.partial(python_fuction_wrapper, python_load_and_decode_file, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                    .map(tensorflow_load_random_subsection, num_parallel_calls=parallel_calls)\n",
    "                    .map(functools.partial(python_fuction_wrapper, openl3_feature_extraction_pipeline, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                    .map(functools.partial(python_fuction_wrapper, python_openl3_feature_extraction, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                    .map(tensorflow_output_shape_setter, num_parallel_calls=parallel_calls)\n",
    "                    .map(functools.partial(python_fuction_wrapper, python_disk_cache_end, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                    .map(tensorflow_output_shape_setter, num_parallel_calls=parallel_calls)\n",
    "                    .map(tensorflow_drop_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                    .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                    .prefetch(parallel_calls)\n",
    "                    .repeat(count=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset pipeline\n",
    "test_dataset = (test_ds\n",
    "                 .map(tensorflow_add_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_disk_cache_start, cache_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_load_and_decode_file, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_load_random_subsection, num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, openl3_feature_extraction_pipeline, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_openl3_feature_extraction, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_output_shape_setter, num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_fuction_wrapper, python_disk_cache_end, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_output_shape_setter, num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_drop_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                 .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                 .prefetch(parallel_calls)\n",
    "                 .repeat(count=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline Inspection\n",
    "\n",
    "The following shows 1 batch of training data to provide a visual guide as to whether the data pipeline is functioning as expected.\n",
    "\n",
    "It is important to observe in the ground truth labels that the classes are random - indicating that the pipeline shuffle function is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 15:24:21.027573: W tensorflow/core/framework/op_kernel.cc:1816] INVALID_ARGUMENT: TypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/__autograph_generated_filei5btv_0o.py\", line 10, in <lambda>\n",
      "    (sample, label, variant, cache_key, cache_found) = ag__.converted_call(ag__.ld(tf).numpy_function, (), dict(func=ag__.autograph_artifact(lambda v1, v2, v3, v4, v5: ag__.converted_call(ag__.ld(pipeline_fn), (ag__.ld(v1), ag__.ld(v2), ag__.ld(v3), ag__.ld(v4), ag__.ld(v5)), None, fscope)), inp=(ag__.ld(sample), ag__.ld(label), ag__.ld(variant), ag__.ld(cache_key), ag__.ld(cache_found)), Tout=ag__.ld(out_types)), fscope)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options, False)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n",
      "    return f(*args)\n",
      "\n",
      "  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/ipykernel_20114/42468250.py\", line 21, in python_audio_augmentations\n",
      "    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.2),\n",
      "\n",
      "TypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\n",
      "\n",
      "\n",
      "2023-09-16 15:24:21.043070: W tensorflow/core/framework/op_kernel.cc:1816] INVALID_ARGUMENT: TypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/__autograph_generated_filei5btv_0o.py\", line 10, in <lambda>\n",
      "    (sample, label, variant, cache_key, cache_found) = ag__.converted_call(ag__.ld(tf).numpy_function, (), dict(func=ag__.autograph_artifact(lambda v1, v2, v3, v4, v5: ag__.converted_call(ag__.ld(pipeline_fn), (ag__.ld(v1), ag__.ld(v2), ag__.ld(v3), ag__.ld(v4), ag__.ld(v5)), None, fscope)), inp=(ag__.ld(sample), ag__.ld(label), ag__.ld(variant), ag__.ld(cache_key), ag__.ld(cache_found)), Tout=ag__.ld(out_types)), fscope)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options, False)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n",
      "    return f(*args)\n",
      "\n",
      "  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/ipykernel_20114/42468250.py\", line 21, in python_audio_augmentations\n",
      "    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.2),\n",
      "\n",
      "TypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} TypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\nTraceback (most recent call last):\n\n  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    ret = func(*args)\n\n  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/__autograph_generated_filei5btv_0o.py\", line 10, in <lambda>\n    (sample, label, variant, cache_key, cache_found) = ag__.converted_call(ag__.ld(tf).numpy_function, (), dict(func=ag__.autograph_artifact(lambda v1, v2, v3, v4, v5: ag__.converted_call(ag__.ld(pipeline_fn), (ag__.ld(v1), ag__.ld(v2), ag__.ld(v3), ag__.ld(v4), ag__.ld(v5)), None, fscope)), inp=(ag__.ld(sample), ag__.ld(label), ag__.ld(variant), ag__.ld(cache_key), ag__.ld(cache_found)), Tout=ag__.ld(out_types)), fscope)\n\n  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n    return _call_unconverted(f, args, kwargs, options, False)\n\n  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n    return f(*args)\n\n  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/ipykernel_20114/42468250.py\", line 21, in python_audio_augmentations\n    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.2),\n\nTypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/mhallid2/Library/CloudStorage/Dropbox/Deakin_Uni/Bach_AI/GitHub/Project-Echo/Transfer learning/SWe_OpenL3/optimised_engine_pipeline_SWe_OpenL3[MH-testing-M2].ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mhallid2/Library/CloudStorage/Dropbox/Deakin_Uni/Bach_AI/GitHub/Project-Echo/Transfer%20learning/SWe_OpenL3/optimised_engine_pipeline_SWe_OpenL3%5BMH-testing-M2%5D.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# show what the pipeline looks like at this stage\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mhallid2/Library/CloudStorage/Dropbox/Deakin_Uni/Bach_AI/GitHub/Project-Echo/Transfer%20learning/SWe_OpenL3/optimised_engine_pipeline_SWe_OpenL3%5BMH-testing-M2%5D.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m features, label \u001b[39min\u001b[39;00m train_dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mhallid2/Library/CloudStorage/Dropbox/Deakin_Uni/Bach_AI/GitHub/Project-Echo/Transfer%20learning/SWe_OpenL3/optimised_engine_pipeline_SWe_OpenL3%5BMH-testing-M2%5D.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSample info: \u001b[39m\u001b[39m{\u001b[39;00mfeatures\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mLabel info: \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mhallid2/Library/CloudStorage/Dropbox/Deakin_Uni/Bach_AI/GitHub/Project-Echo/Transfer%20learning/SWe_OpenL3/optimised_engine_pipeline_SWe_OpenL3%5BMH-testing-M2%5D.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:814\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    813\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    815\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    816\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:777\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 777\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    778\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    779\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    780\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    782\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3028\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3027\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 3028\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   3029\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   3030\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} TypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\nTraceback (most recent call last):\n\n  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    ret = func(*args)\n\n  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/__autograph_generated_filei5btv_0o.py\", line 10, in <lambda>\n    (sample, label, variant, cache_key, cache_found) = ag__.converted_call(ag__.ld(tf).numpy_function, (), dict(func=ag__.autograph_artifact(lambda v1, v2, v3, v4, v5: ag__.converted_call(ag__.ld(pipeline_fn), (ag__.ld(v1), ag__.ld(v2), ag__.ld(v3), ag__.ld(v4), ag__.ld(v5)), None, fscope)), inp=(ag__.ld(sample), ag__.ld(label), ag__.ld(variant), ag__.ld(cache_key), ag__.ld(cache_found)), Tout=ag__.ld(out_types)), fscope)\n\n  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n    return _call_unconverted(f, args, kwargs, options, False)\n\n  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n    return f(*args)\n\n  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/ipykernel_20114/42468250.py\", line 21, in python_audio_augmentations\n    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.2),\n\nTypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 15:24:24.365926: W tensorflow/core/framework/op_kernel.cc:1816] INVALID_ARGUMENT: TypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/__autograph_generated_filei5btv_0o.py\", line 10, in <lambda>\n",
      "    (sample, label, variant, cache_key, cache_found) = ag__.converted_call(ag__.ld(tf).numpy_function, (), dict(func=ag__.autograph_artifact(lambda v1, v2, v3, v4, v5: ag__.converted_call(ag__.ld(pipeline_fn), (ag__.ld(v1), ag__.ld(v2), ag__.ld(v3), ag__.ld(v4), ag__.ld(v5)), None, fscope)), inp=(ag__.ld(sample), ag__.ld(label), ag__.ld(variant), ag__.ld(cache_key), ag__.ld(cache_found)), Tout=ag__.ld(out_types)), fscope)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options, False)\n",
      "\n",
      "  File \"/Applications/anaconda3/envs/TF_projectecho_test/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n",
      "    return f(*args)\n",
      "\n",
      "  File \"/var/folders/nr/hwp3lz852qn_89d92mtrsg640000gp/T/ipykernel_20114/42468250.py\", line 21, in python_audio_augmentations\n",
      "    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.2),\n",
      "\n",
      "TypeError: Shift.__init__() got an unexpected keyword argument 'min_fraction'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for features, label in train_dataset.take(1):\n",
    "    print(f'Sample info: {features.shape}, \\nLabel info: {label.shape} \\n{label}')\n",
    "    for example in range(features.shape[0]):\n",
    "        print(f'Example {example} features: {features[example]}')\n",
    "        print(f'Example {example} label: {label[example]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model\n",
    "\n",
    "In order to test whether the pipeline is working a CNN based image classification model is constructed below.  This model leverages pre-trained model weights for the EfficientNetV2 feature model which generates a vector representation of 1000 floats for each input image.  Note that on first run this model tensorflow hub library will check if model weights are available, and if not, will automatically download them to your computer.  This may take a few minutes the first time this is run.\n",
    "\n",
    "The output from the EfficientNetV2 model is then fed into 2 fully connected layers to perform the classification function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openl3\n",
    "\n",
    "def build_model():\n",
    "    # Input shape, change to match the shape of the feature vectors\n",
    "    input_shape = (6144,)\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Model layers\n",
    "    x = tf.keras.layers.BatchNormalization()(input_layer)\n",
    "    x = tf.keras.layers.Dense(len(class_names) * 8, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(len(class_names) * 4, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.50)(x)\n",
    "    output_layer = tf.keras.layers.Dense(len(class_names), activation=None)(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compile model (you can adjust the optimizer and loss function as necessary)\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "The following sets up callbacks to track model training and performs the model fit to train the model.\n",
    "\n",
    "The callbacks will ensure the best model weights (as defined by lowest validation loss) will be written to disk during training.  This is important as the model training could take several hours (12+ hours) to complete and any errors may cause the training loop to exit - so having the checkpoints acts as a backup.  The model will stop training when it sees no further improvement to the validation loss, after which it will restore the best weights found.  This allows the training to discover when it is overfit and stop further training.  This is why the number of epics is 10000.  It is expected the model training will end significantly earlier than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models/'):\n",
    "    os.mkdir('models/')\n",
    "if not os.path.exists('models/1'):\n",
    "    os.mkdir('models/1')    \n",
    "    \n",
    "# Allow all the weights to be trained\n",
    "model = build_model()\n",
    "\n",
    "# Tensorboard for visualisation of results\n",
    "log_dir = \"tensorboard_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Reduce learning rate to avoid overshooting local minima\n",
    "lr_reduce_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                         factor=0.75,\n",
    "                                                         patience=8, \n",
    "                                                         verbose=1,\n",
    "                                                         mode='min',\n",
    "                                                         cooldown=0, \n",
    "                                                         min_lr=1e-7)\n",
    "\n",
    "# End the training if no improvement for 16 epochs in a row, then restore best model weights\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                  min_delta=0,\n",
    "                                                  patience=16,\n",
    "                                                  verbose=0,\n",
    "                                                  mode=\"min\",\n",
    "                                                  baseline=None,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "# Save the best model as it trains\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('models/checkpoint_generic_model.hdf5', \n",
    "                                               save_best_only=True, \n",
    "                                               monitor='val_loss', \n",
    "                                               mode='min')\n",
    "\n",
    "# Clear the cache (make sure cache is defined correctly)\n",
    "cache.clear()\n",
    "\n",
    "# Fit the model to the training set\n",
    "# This may take a long time depending on your machine and data size\n",
    "model.fit(train_dataset, \n",
    "          validation_data=validation_dataset,\n",
    "          callbacks=[lr_reduce_plateau, early_stopping, tensorboard_callback, mcp_save],\n",
    "          epochs=SC['MAX_EPOCHS']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictions on test data\n",
    "\n",
    "The following provides an example of how to use the trained model to perform inference (predictions).  It calculates the best class and the probability for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict class and probability given a prediction\n",
    "def predict_class(predictions):\n",
    "    # Get the index of the class with the highest predicted probability\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    # Get the class name using the predicted index\n",
    "    predicted_class = class_names[predicted_index]\n",
    "    # Calculate the predicted probability for the selected class\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[predicted_index].numpy()\n",
    "    # Round the probability to 2 decimal places\n",
    "    predicted_probability = str(round(predicted_probability, 2))\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "# Display class names and run prediction on test entries\n",
    "print(f'Class names: {class_names}')\n",
    "for features, labels in test_dataset:\n",
    "    # Generate predictions for the given features\n",
    "    predictions = model.predict(features, verbose=0)\n",
    "\n",
    "    # Iterate over each item in the batch\n",
    "    for batch_idx in range(predictions.shape[0]):\n",
    "        # Get the index of the true class\n",
    "        true_index = int(tf.argmax(tf.squeeze(labels[batch_idx])).numpy())\n",
    "        # Get the true class name using the true index\n",
    "        true_class = class_names[true_index]\n",
    "\n",
    "        # Predict class and probability using the prediction function\n",
    "        predicted_class, predicted_probability = predict_class(predictions[batch_idx])\n",
    "\n",
    "        print(f'True class      : {true_class}')\n",
    "        print(f'Predicted class : {predicted_class}')\n",
    "        print(f'Predicted probability : {predicted_probability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full model for use with tensorflow serving\n",
    "model.save('models/echo_model/1/', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
