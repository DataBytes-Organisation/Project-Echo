{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version        :  3.8.16\n",
      "TensorFlow Version    :  2.10.1\n",
      "TensorFlow IO Version :  0.27.0\n",
      "Librosa Version       :  0.10.0\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# library imports\n",
    "########################################################################################\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# generic libraries\n",
    "from platform import python_version\n",
    "import functools\n",
    "import diskcache as dc\n",
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "# tensor flow / keras related libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from keras.utils import dataset_utils\n",
    "\n",
    "# image processing related libraries\n",
    "import librosa \n",
    "import imageio\n",
    "import PIL\n",
    "\n",
    "# print system information\n",
    "print('Python Version        : ', python_version())\n",
    "print('TensorFlow Version    : ', tf.__version__)\n",
    "print('TensorFlow IO Version : ', tfio.__version__)\n",
    "print('Librosa Version       : ', librosa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# system constants\n",
    "########################################################################################\n",
    "AUDIO_DATA_DIRECTORY = \"d:\\\\data\\\\bc\"\n",
    "CACHE_DIRETORY       = \"d:\\\\pipeline_cache\"\n",
    "\n",
    "AUDIO_NFFT = 512\n",
    "AUDIO_WINDOW = 512\n",
    "AUDIO_STRIDE = 512\n",
    "AUDIO_SAMPLE_RATE = int(44100/2)\n",
    "AUDIO_MELS = 128\n",
    "AUDIO_FMIN = 0\n",
    "AUDIO_FMAX = int(AUDIO_SAMPLE_RATE)/2\n",
    "AUDIO_TOP_DB = 80\n",
    "        \n",
    "MODEL_INPUT_IMAGE_WIDTH = 256\n",
    "MODEL_INPUT_IMAGE_HEIGHT = 256\n",
    "MODEL_INPUT_IMAGE_CHANNELS = 3\n",
    "\n",
    "CLASSIFIER_BATCH_SIZE=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create a DiskCache instance\n",
    "# This cache will allow us store intermediate function results to speed up the \n",
    "# data processing pipeline\n",
    "########################################################################################\n",
    "cache = dc.Cache(CACHE_DIRETORY, cull_limit=0, size_limit=10**9) \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# a helper function to create a hash key from a function signature and arguments\n",
    "########################################################################################\n",
    "def create_function_key(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    func_name = partial_func.func.__name__\n",
    "    func_module = partial_func.func.__module__\n",
    "    args_repr = repr(partial_func.args)\n",
    "    kwargs_repr = repr(sorted(partial_func.keywords.items()))\n",
    "\n",
    "    key = f\"{func_module}.{func_name}:{args_repr}:{kwargs_repr}\"\n",
    "    # Use hashlib to create a hash of the key for shorter and consistent length\n",
    "    key_hash = hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "    return key, key_hash, partial_func\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Execute a function and cache the result\n",
    "# If already executed, retrieve function output from the cache instead\n",
    "########################################################################################\n",
    "def execute_cached_function(func, *args, **kwargs):\n",
    "    key_string,key,partial_func = create_function_key(func, *args, **kwargs)\n",
    "    #print(f'key: {key_string} {key}')\n",
    "    # Check if the result is in the cache\n",
    "    if key in cache:\n",
    "        result = cache[key]\n",
    "        print(f\"Result loaded from cache: {result}\")\n",
    "    else:\n",
    "        # If not in cache, call the slow operation and store the result in cache\n",
    "        result = partial_func()\n",
    "        cache[key] = result\n",
    "        print(f\"Result calculated and stored in cache: {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# these helper functions load the audio data into a 'dataset' using only paths\n",
    "# just dealing with paths at this early stage means the entire dataset can be shuffled in\n",
    "# memory and split before loading the actual audio data into memory\n",
    "########################################################################################\n",
    "def paths_and_labels_to_dataset(image_paths, labels, num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    zipped_path_ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    return zipped_path_ds\n",
    "\n",
    "def create_datasets(audio_files, train_split=0.7, val_split=0.2):\n",
    "    file_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            audio_files,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.ogg','.mp3','.wav','.flac'),\n",
    "            class_names=None,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=file_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    # Calculate the size of the dataset\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # Calculate the number of elements for each dataset split\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_size, seed=42)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size).take(val_size)\n",
    "    test_ds = dataset.skip(train_size + val_size).take(test_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 524 files belonging to 5 classes.\n",
      "Class names:  ['brant', 'jabwar', 'sheowl', 'spodov', 'wiltur']\n",
      "Training   dataset length: 419\n",
      "Validation dataset length: 99\n",
      "Test       dataset length: 6\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "train_ds, val_ds, test_ds, class_names = create_datasets(AUDIO_DATA_DIRECTORY,train_split=0.8, val_split=0.19)\n",
    "print(\"Class names: \", class_names)\n",
    "print(f\"Training   dataset length: {len(train_ds)}\")\n",
    "print(f\"Validation dataset length: {len(val_ds)}\")\n",
    "print(f\"Test       dataset length: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC666501.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\wiltur\\\\XC317966.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 1.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\wiltur\\\\XC618595.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 1.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\brant\\\\XC540354.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 0., 0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC295378.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC443310.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC607790.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC443311.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC491970.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC124409.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for item in train_ds.take(10):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-8.7266113e-07 -1.0969178e-07]\n",
      " [ 1.8670749e-05  2.4067995e-05]\n",
      " [ 1.0681602e-05  1.3049542e-05]\n",
      " ...\n",
      " [ 1.3494791e-03  1.9154038e-03]\n",
      " [ 2.3285199e-04  5.0018949e-04]\n",
      " [ 4.7588945e-04  4.7343917e-04]], shape=(275712, 2), dtype=float32)\n",
      "(275712,)\n",
      "duration 8.615963718820861 5.0\n",
      "start_time_secs 0.8278378844261169 max_start_time 3.6159636974334717\n",
      "start_index 18253\n",
      "end_index 18253\n",
      "subsection (110250,)\n"
     ]
    }
   ],
   "source": [
    "def load_random_subsection(path, duration_secs):\n",
    "\n",
    "    # read the file data\n",
    "    file_contents=tf.io.read_file(path)\n",
    "\n",
    "    try:\n",
    "        tmp_audio_t = tfio.audio.decode_flac(input=file_contents)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        tmp_audio_t = tfio.audio.decode_vorbis(input=file_contents)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(tmp_audio_t)\n",
    "\n",
    "    # cast and keep left channel only\n",
    "    tmp_audio_t = tf.cast(tmp_audio_t, tf.float32)[:,-1]\n",
    "    \n",
    "    print(tmp_audio_t.shape)\n",
    "\n",
    "    # resample the sample rate\n",
    "    tmp_audio_t = tfio.audio.resample(tmp_audio_t, tfio.audio.AudioIOTensor(path)._rate.numpy(), AUDIO_SAMPLE_RATE)\n",
    "\n",
    "    # Determine the audio file's duration in seconds\n",
    "    audio_duration_secs = tf.shape(tmp_audio_t)[0] / AUDIO_SAMPLE_RATE\n",
    "    \n",
    "    if audio_duration_secs>duration_secs:\n",
    "    \n",
    "        print(f'duration {audio_duration_secs} {duration_secs}')\n",
    "\n",
    "        # Calculate the starting point of the 5-second subsection\n",
    "        max_start = tf.cast(audio_duration_secs - duration_secs, tf.float32)\n",
    "        start_time_secs = tf.random.uniform((), 0.0, max_start, dtype=tf.float32)\n",
    "        \n",
    "        print(f'start_time_secs {start_time_secs} max_start_time {max_start}')\n",
    "\n",
    "        start_index = tf.cast(start_time_secs * AUDIO_SAMPLE_RATE, dtype=tf.int32)\n",
    "        print(f'start_index {start_index}')\n",
    "\n",
    "        # Load the 5-second subsection\n",
    "        end_index = tf.cast(start_index + tf.cast(duration_secs, tf.int32) * AUDIO_SAMPLE_RATE, tf.int32)\n",
    "        \n",
    "        print(f'end_index {start_index}')\n",
    "        \n",
    "        subsection = tmp_audio_t[start_index : end_index]\n",
    "    \n",
    "    else:\n",
    "        print(f' padding it ')\n",
    "        # Pad the subsection with silence if it's shorter than 5 seconds\n",
    "        padding_length = duration_secs * AUDIO_SAMPLE_RATE - tf.shape(tmp_audio_t)[0]\n",
    "        padding = tf.zeros([padding_length], dtype=tmp_audio_t.dtype)\n",
    "        subsection = tf.concat([tmp_audio_t, padding], axis=0)\n",
    "\n",
    "    print(f'subsection {subsection.shape}')\n",
    "\n",
    "    return subsection\n",
    "\n",
    "clip = load_random_subsection('d:\\\\data\\\\bc\\\\spodov\\\\XC441823.ogg', duration_secs=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_example_pipeline(path, label):\n",
    "    \n",
    "    print(f'path {path}')\n",
    "    \n",
    "    tmp_audio_t = load_random_subsection(path, duration_secs=5)\n",
    "    \n",
    "    print(f'tmp_audio_t shape {tmp_audio_t}')\n",
    "        \n",
    "    # Convert to spectrogram\n",
    "    image = tfio.audio.spectrogram(\n",
    "        tmp_audio_t,\n",
    "        nfft=AUDIO_NFFT, \n",
    "        window=AUDIO_WINDOW, \n",
    "        stride=AUDIO_STRIDE)\n",
    "    \n",
    "    # Convert to melspectrogram\n",
    "    image = tfio.audio.melscale(\n",
    "        image, \n",
    "        rate=AUDIO_SAMPLE_RATE, \n",
    "        mels=AUDIO_MELS, \n",
    "        fmin=AUDIO_FMIN, \n",
    "        fmax=AUDIO_FMAX)\n",
    "    \n",
    "    print(f'image shape {image.shape}')\n",
    "\n",
    "    # reshape into standard 3 channels to add the color channel\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    \n",
    "    # most pre-trained model expect 3 color channels\n",
    "    image = tf.repeat(image, MODEL_INPUT_IMAGE_CHANNELS, axis=2)\n",
    "    \n",
    "    print(f'image shape {image.shape}')\n",
    "    \n",
    "    image = tf.ensure_shape(image, [216, 128, MODEL_INPUT_IMAGE_CHANNELS])\n",
    "    image = tf.image.resize(image, (MODEL_INPUT_IMAGE_WIDTH,MODEL_INPUT_IMAGE_HEIGHT), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "    \n",
    "    # for some reason the melspecs seem rotated by 90 degrees. This corrects that.\n",
    "    image = tf.image.rot90(image, k=1)\n",
    "    \n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.00001)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will allow python execution\n",
    "def dataset_example_pipeline_wrapper(path, label):\n",
    "    # Use a lambda function to pass two arguments to the dataset_example_pipeline function\n",
    "    return tf.py_function(func=lambda x, y: dataset_example_pipeline(x, y), inp=(path, label), Tout=(tf.float32, label.dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# create the datasets useful for training a classification model\n",
    "########################################################################################\n",
    "train_dataset = (train_ds\n",
    "                 .map(dataset_example_pipeline_wrapper)\n",
    "                 .batch(CLASSIFIER_BATCH_SIZE)          \n",
    ")\n",
    "\n",
    "# validation_dataset = (val_ds\n",
    "#                       .map(dataset_example_pipeline_wrapper)\n",
    "#                       .batch(CLASSIFIER_BATCH_SIZE)\n",
    "# )\n",
    "\n",
    "# test_dataset = (test_ds\n",
    "#                 .map(dataset_example_pipeline_wrapper)\n",
    "#                 .batch(CLASSIFIER_BATCH_SIZE)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path b'd:\\\\data\\\\bc\\\\jabwar\\\\XC191196.ogg'\n",
      "tf.Tensor(\n",
      "[[ 1.4516487e-05  1.4623534e-05]\n",
      " [-7.2555854e-06 -1.4011867e-05]\n",
      " [ 1.6997699e-05  1.3320816e-05]\n",
      " ...\n",
      " [-3.3115927e-04  8.5126521e-05]\n",
      " [-5.6644093e-04 -2.4861511e-04]\n",
      " [-2.0296275e-04 -3.6039841e-05]], shape=(3046087, 2), dtype=float32)\n",
      "(3046087,)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000021CAF08EDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "duration 95.19020408163266 5\n",
      "start_time_secs 54.84065628051758 max_start_time 90.19020080566406\n",
      "start_index 1209236\n",
      "end_index 1209236\n",
      "subsection (110250,)\n",
      "tmp_audio_t shape [ 0.00392026  0.0064552   0.00502223 ...  0.00150487  0.00057983\n",
      " -0.00167692]\n",
      "image shape (216, 128)\n",
      "image shape (216, 128, 3)\n",
      "path b'd:\\\\data\\\\bc\\\\sheowl\\\\XC431869.ogg'\n",
      "tf.Tensor(\n",
      "[[-5.3238455e-06 -8.2403221e-06]\n",
      " [ 2.3792973e-05  1.2055902e-05]\n",
      " [-1.3287732e-05 -8.7958533e-06]\n",
      " ...\n",
      " [-1.9378622e-06 -5.5549265e-08]\n",
      " [ 2.5502404e-05  2.9752951e-05]\n",
      " [ 3.5742160e-06  7.2412176e-06]], shape=(4482816, 2), dtype=float32)\n",
      "(4482816,)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000021C8D148A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "duration 140.08798185941043 5\n",
      "start_time_secs 112.33496856689453 max_start_time 135.08798217773438\n",
      "start_index 2476986\n",
      "end_index 2476986\n",
      "subsection (110250,)\n",
      "tmp_audio_t shape [4.5620636e-05 6.6329005e-05 1.0314810e-04 ... 4.3762520e-05 8.5251435e-05\n",
      " 3.3864017e-05]\n",
      "image shape (216, 128)\n",
      "image shape (216, 128, 3)\n",
      "path b'd:\\\\data\\\\bc\\\\wiltur\\\\XC517737.ogg'\n",
      " sample info: (2, 256, 256, 3), [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "tf.Tensor(\n",
      "[[-1.9873594e-06  1.4371022e-05]\n",
      " [-1.4003531e-05  4.2140750e-06]\n",
      " [ 1.1570224e-05  2.7020305e-05]\n",
      " ...\n",
      " [ 1.0678863e-05  3.7612153e-07]\n",
      " [-1.4680620e-05 -5.3815442e-05]\n",
      " [ 3.0794417e-05 -6.5996692e-06]], shape=(1934315, 2), dtype=float32)\n",
      "(1934315,)\n",
      "duration 60.44730158730159 5\n",
      "start_time_secs 40.06000518798828 max_start_time 55.44729995727539\n",
      "start_index 883323\n",
      "end_index 883323\n",
      "subsection (110250,)\n",
      "tmp_audio_t shape [-0.00333559 -0.00408912  0.02098476 ...  0.02505711  0.002168\n",
      "  0.02290211]\n",
      "image shape (216, 128)\n",
      "image shape (216, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for melspectrogram,label in train_dataset.take(1):\n",
    "    print(f' sample info: {melspectrogram.shape}, {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the cache works\n",
    "class ArrayProcessor:\n",
    "    def sum_plus_five(self, arr, v2):\n",
    "        array_sum = np.sum(arr)\n",
    "        return array_sum + 5.0 + v2\n",
    "\n",
    "# Usage example\n",
    "processor = ArrayProcessor()\n",
    "\n",
    "# Create a 2D NumPy array\n",
    "arr = np.random.rand(1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 processor.sum_plus_five(arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result calculated and stored in cache: 523760.31744759623\n",
      "4.94 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 execute_cached_function(processor.sum_plus_five, arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result loaded from cache: 523760.31744759623\n",
      "736 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 execute_cached_function(processor.sum_plus_five,arr, 18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
