{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version        :  3.8.16\n",
      "TensorFlow Version    :  2.10.1\n",
      "TensorFlow IO Version :  0.27.0\n",
      "Librosa Version       :  0.10.0\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# library imports\n",
    "########################################################################################\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# generic libraries\n",
    "from platform import python_version\n",
    "import functools\n",
    "import diskcache as dc\n",
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "# tensor flow / keras related libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from keras.utils import dataset_utils\n",
    "\n",
    "# image processing related libraries\n",
    "import librosa \n",
    "import imageio\n",
    "import PIL\n",
    "\n",
    "# from platform import python_version\n",
    "# import tensorflow as tf \n",
    "# import tensorflow_io as tfio\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import datetime\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# from keras.utils import dataset_utils\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from IPython import display\n",
    "# import librosa\n",
    "# import glob\n",
    "# import imageio\n",
    "# import PIL\n",
    "# import tensorflow as tf\n",
    "# import time\n",
    "\n",
    "# print system information\n",
    "print('Python Version        : ', python_version())\n",
    "print('TensorFlow Version    : ', tf.__version__)\n",
    "print('TensorFlow IO Version : ', tfio.__version__)\n",
    "print('Librosa Version       : ', librosa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# system constants\n",
    "########################################################################################\n",
    "AUDIO_DATA_DIRECTORY = \"d:/data/bc\"\n",
    "CACHE_DIRETORY       = \"d:/pipeline_cache\"\n",
    "\n",
    "MODEL_INPUT_IMAGE_WIDTH = 256\n",
    "MODEL_INPUT_IMAGE_HEIGHT = 256\n",
    "MODEL_INPUT_IMAGE_CHANNELS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create a DiskCache instance\n",
    "# This cache will allow us store intermediate function results to speed up the \n",
    "# data processing pipeline\n",
    "########################################################################################\n",
    "cache = dc.Cache(CACHE_DIRETORY, cull_limit=0, size_limit=10**9) \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# a helper function to create a hash key from a function signature and arguments\n",
    "########################################################################################\n",
    "def create_function_key(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    func_name = partial_func.func.__name__\n",
    "    func_module = partial_func.func.__module__\n",
    "    args_repr = repr(partial_func.args)\n",
    "    kwargs_repr = repr(sorted(partial_func.keywords.items()))\n",
    "\n",
    "    key = f\"{func_module}.{func_name}:{args_repr}:{kwargs_repr}\"\n",
    "    # Use hashlib to create a hash of the key for shorter and consistent length\n",
    "    key_hash = hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "    return key, key_hash, partial_func\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Execute a function and cache the result\n",
    "# If already executed, retrieve function output from the cache instead\n",
    "########################################################################################\n",
    "def execute_cached_function(func, *args, **kwargs):\n",
    "    key_string,key,partial_func = create_function_key(func, *args, **kwargs)\n",
    "    #print(f'key: {key_string} {key}')\n",
    "    # Check if the result is in the cache\n",
    "    if key in cache:\n",
    "        result = cache[key]\n",
    "        print(f\"Result loaded from cache: {result}\")\n",
    "    else:\n",
    "        # If not in cache, call the slow operation and store the result in cache\n",
    "        result = partial_func()\n",
    "        cache[key] = result\n",
    "        print(f\"Result calculated and stored in cache: {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# these helper functions load the audio data into a 'dataset' \n",
    "########################################################################################\n",
    "def paths_and_labels_to_dataset(image_paths, labels, num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    img_ds = path_ds.map(\n",
    "        lambda path: tf.io.read_file(path), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    img_ds = tf.data.Dataset.zip((img_ds, label_ds))\n",
    "    return img_ds\n",
    "\n",
    "def create_dataset(audio_files):\n",
    "    image_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            audio_files,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.ogg','.mp3','.wav','.flac'),\n",
    "            class_names=None,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    return dataset, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 524 files belonging to 5 classes.\n",
      "class names:  ['brant', 'jabwar', 'sheowl', 'spodov', 'wiltur']\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "complete_dataset, class_names = create_dataset(AUDIO_DATA_DIRECTORY)\n",
    "print(\"class names: \", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the cache works\n",
    "class ArrayProcessor:\n",
    "    def sum_plus_five(self, arr, v2):\n",
    "        array_sum = np.sum(arr)\n",
    "        return array_sum + 5.0 + v2\n",
    "\n",
    "# Usage example\n",
    "processor = ArrayProcessor()\n",
    "\n",
    "# Create a 2D NumPy array\n",
    "arr = np.random.rand(1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 processor.sum_plus_five(arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result calculated and stored in cache: 524025.7365223917\n",
      "6.75 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 execute_cached_function(processor.sum_plus_five, arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result loaded from cache: 524025.7365223917\n",
      "955 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 execute_cached_function(processor.sum_plus_five,arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transforms(image, label):\n",
    "    # reshape into standard 3 channels\n",
    "    image = tf.io.parse_tensor(image, tf.float32)\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    \n",
    "    # most pre-trained model expect 3 color channels\n",
    "    image = tf.repeat(image, MODEL_INPUT_IMAGE_CHANNELS, axis=2)\n",
    "    \n",
    "    image = tf.ensure_shape(image, [216, 128, MODEL_INPUT_IMAGE_CHANNELS])\n",
    "    image = tf.image.resize(image, (MODEL_INPUT_IMAGE_WIDTH,MODEL_INPUT_IMAGE_HEIGHT), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "    \n",
    "    # for some reason the melspecs seem rotated by 90 degrees. This corrects that.\n",
    "    image = tf.image.rot90(image, k=1)\n",
    "    \n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.00001)\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataset_b \u001b[39m=\u001b[39m ( \n\u001b[0;32m      2\u001b[0m                   train_dataset       \n\u001b[0;32m      3\u001b[0m                   \u001b[39m.\u001b[39mshuffle(\u001b[39m20000\u001b[39m)\n\u001b[0;32m      4\u001b[0m                   \u001b[39m.\u001b[39mmap(dataset_transforms)\n\u001b[1;32m----> 5\u001b[0m                   \u001b[39m.\u001b[39mbatch(baseline_config\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m      6\u001b[0m                   \u001b[39m.\u001b[39mcache()           \n\u001b[0;32m      7\u001b[0m                 )\n\u001b[0;32m      9\u001b[0m validation_dataset_b \u001b[39m=\u001b[39m ( \n\u001b[0;32m     10\u001b[0m                   validation_dataset\n\u001b[0;32m     11\u001b[0m                   \u001b[39m.\u001b[39mmap(dataset_transforms)\n\u001b[0;32m     12\u001b[0m                   \u001b[39m.\u001b[39mbatch(baseline_config\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m     13\u001b[0m                   \u001b[39m.\u001b[39mcache()\n\u001b[0;32m     14\u001b[0m                 )\n\u001b[0;32m     16\u001b[0m test_dataset_b \u001b[39m=\u001b[39m ( \n\u001b[0;32m     17\u001b[0m                   test_dataset\n\u001b[0;32m     18\u001b[0m                   \u001b[39m.\u001b[39mmap(dataset_transforms)\n\u001b[0;32m     19\u001b[0m                   \u001b[39m.\u001b[39mbatch(baseline_config\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m     20\u001b[0m                   \u001b[39m.\u001b[39mcache()\n\u001b[0;32m     21\u001b[0m                 )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baseline_config' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset_b = ( \n",
    "                  train_dataset       \n",
    "                  .shuffle(20000)\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()           \n",
    "                )\n",
    "\n",
    "validation_dataset_b = ( \n",
    "                  validation_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                )\n",
    "\n",
    "test_dataset_b = ( \n",
    "                  test_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                )\n",
    "\n",
    "train_dataset, class_names = create_dataset('TRAIN/')\n",
    "test_dataset, _            = create_dataset('TEST/')\n",
    "validation_dataset, _      = create_dataset('VALIDATION/')\n",
    "print(\"class names: \", class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
