{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version        :  3.8.16\n",
      "TensorFlow Version    :  2.10.1\n",
      "TensorFlow IO Version :  0.27.0\n",
      "Librosa Version       :  0.10.0\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# library imports\n",
    "########################################################################################\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# generic libraries\n",
    "from platform import python_version\n",
    "import functools\n",
    "import diskcache as dc\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# tensor flow / keras related libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils import dataset_utils\n",
    "\n",
    "# image processing related libraries\n",
    "import librosa \n",
    "\n",
    "# print system information\n",
    "print('Python Version        : ', python_version())\n",
    "print('TensorFlow Version    : ', tf.__version__)\n",
    "print('TensorFlow IO Version : ', tfio.__version__)\n",
    "print('Librosa Version       : ', librosa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# system constants\n",
    "########################################################################################\n",
    "AUDIO_DATA_DIRECTORY = \"d:\\\\data\\\\bc\"\n",
    "CACHE_DIRETORY       = \"d:\\\\pipeline_cache\"\n",
    "\n",
    "AUDIO_NFFT = 512\n",
    "AUDIO_WINDOW = 512\n",
    "AUDIO_STRIDE = 512\n",
    "AUDIO_SAMPLE_RATE = int(44100/2)\n",
    "AUDIO_MELS = 128\n",
    "AUDIO_FMIN = 0\n",
    "AUDIO_FMAX = int(AUDIO_SAMPLE_RATE)/2\n",
    "AUDIO_TOP_DB = 80\n",
    "        \n",
    "MODEL_INPUT_IMAGE_WIDTH = 256\n",
    "MODEL_INPUT_IMAGE_HEIGHT = 256\n",
    "MODEL_INPUT_IMAGE_CHANNELS = 3\n",
    "\n",
    "CLASSIFIER_BATCH_SIZE=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create a DiskCache instance\n",
    "# This cache will allow us store intermediate function results to speed up the \n",
    "# data processing pipeline\n",
    "########################################################################################\n",
    "cache = dc.Cache(CACHE_DIRETORY, cull_limit=0, size_limit=10**9) \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# a helper function to create a hash key from a function signature and arguments\n",
    "########################################################################################\n",
    "def create_function_key(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    func_name = partial_func.func.__name__\n",
    "    func_module = partial_func.func.__module__\n",
    "    args_repr = repr(partial_func.args)\n",
    "    kwargs_repr = repr(sorted(partial_func.keywords.items()))\n",
    "\n",
    "    key = f\"{func_module}.{func_name}:{args_repr}:{kwargs_repr}\"\n",
    "    # Use hashlib to create a hash of the key for shorter and consistent length\n",
    "    key_hash = hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "    return key, key_hash, partial_func\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Execute a function and cache the result\n",
    "# If already executed, retrieve function output from the cache instead\n",
    "########################################################################################\n",
    "def execute_cached_function(func, *args, **kwargs):\n",
    "    key_string,key,partial_func = create_function_key(func, *args, **kwargs)\n",
    "    #print(f'key: {key_string} {key}')\n",
    "    # Check if the result is in the cache\n",
    "    if key in cache:\n",
    "        result = cache[key]\n",
    "        print(f\"Result loaded from cache key: {key}\")\n",
    "    else:\n",
    "        # If not in cache, call the slow operation and store the result in cache\n",
    "        result = partial_func()\n",
    "        cache[key] = result\n",
    "        print(f\"Result calculated and stored in cache key: {key}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# these helper functions load the audio data into a 'dataset' using only paths\n",
    "# just dealing with paths at this early stage means the entire dataset can be shuffled in\n",
    "# memory and split before loading the actual audio data into memory\n",
    "########################################################################################\n",
    "def paths_and_labels_to_dataset(image_paths, labels, num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    zipped_path_ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    return zipped_path_ds\n",
    "\n",
    "def create_datasets(audio_files, train_split=0.7, val_split=0.2):\n",
    "    file_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            audio_files,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.ogg','.mp3','.wav','.flac'),\n",
    "            class_names=None,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=file_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    # Calculate the size of the dataset\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # Calculate the number of elements for each dataset split\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_size, seed=42)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size).take(val_size)\n",
    "    test_ds = dataset.skip(train_size + val_size).take(test_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 524 files belonging to 5 classes.\n",
      "Class names:  ['brant', 'jabwar', 'sheowl', 'spodov', 'wiltur']\n",
      "Training   dataset length: 419\n",
      "Validation dataset length: 99\n",
      "Test       dataset length: 6\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "train_ds, val_ds, test_ds, class_names = create_datasets(AUDIO_DATA_DIRECTORY,train_split=0.8, val_split=0.19)\n",
    "print(\"Class names: \", class_names)\n",
    "print(f\"Training   dataset length: {len(train_ds)}\")\n",
    "print(f\"Validation dataset length: {len(val_ds)}\")\n",
    "print(f\"Test       dataset length: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC666501.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\wiltur\\\\XC317966.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 1.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\wiltur\\\\XC618595.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 1.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\brant\\\\XC540354.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 0., 0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC295378.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC443310.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC607790.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC443311.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC491970.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC124409.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for item in train_ds.take(10):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration 8.615963718820861 5.0\n",
      "subsection (110250,)\n"
     ]
    }
   ],
   "source": [
    "def load_random_subsection(path, duration_secs):\n",
    "\n",
    "    # read the file data\n",
    "    file_contents=tf.io.read_file(path)\n",
    "\n",
    "    try:\n",
    "        tmp_audio_t = tfio.audio.decode_flac(input=file_contents)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        tmp_audio_t = tfio.audio.decode_vorbis(input=file_contents)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #print(tmp_audio_t)\n",
    "\n",
    "    # cast and keep left channel only\n",
    "    tmp_audio_t = tf.cast(tmp_audio_t, tf.float32)[:,-1]\n",
    "    \n",
    "    #print(tmp_audio_t.shape)\n",
    "\n",
    "    # resample the sample rate\n",
    "    tmp_audio_t = tfio.audio.resample(tmp_audio_t, tfio.audio.AudioIOTensor(path)._rate.numpy(), AUDIO_SAMPLE_RATE)\n",
    "\n",
    "    # Determine the audio file's duration in seconds\n",
    "    audio_duration_secs = tf.shape(tmp_audio_t)[0] / AUDIO_SAMPLE_RATE\n",
    "    \n",
    "    if audio_duration_secs>duration_secs:\n",
    "    \n",
    "        print(f'duration {audio_duration_secs} {duration_secs}')\n",
    "\n",
    "        # Calculate the starting point of the 5-second subsection\n",
    "        max_start = tf.cast(audio_duration_secs - duration_secs, tf.float32)\n",
    "        start_time_secs = tf.random.uniform((), 0.0, max_start, dtype=tf.float32)\n",
    "        \n",
    "        #print(f'start_time_secs {start_time_secs} max_start_time {max_start}')\n",
    "\n",
    "        start_index = tf.cast(start_time_secs * AUDIO_SAMPLE_RATE, dtype=tf.int32)\n",
    "        #print(f'start_index {start_index}')\n",
    "\n",
    "        # Load the 5-second subsection\n",
    "        end_index = tf.cast(start_index + tf.cast(duration_secs, tf.int32) * AUDIO_SAMPLE_RATE, tf.int32)\n",
    "        \n",
    "        #print(f'end_index {start_index}')\n",
    "        \n",
    "        subsection = tmp_audio_t[start_index : end_index]\n",
    "    \n",
    "    else:\n",
    "        print(f' padding it ')\n",
    "        # Pad the subsection with silence if it's shorter than 5 seconds\n",
    "        padding_length = duration_secs * AUDIO_SAMPLE_RATE - tf.shape(tmp_audio_t)[0]\n",
    "        padding = tf.zeros([padding_length], dtype=tmp_audio_t.dtype)\n",
    "        subsection = tf.concat([tmp_audio_t, padding], axis=0)\n",
    "\n",
    "    print(f'subsection {subsection.shape}')\n",
    "\n",
    "    return subsection\n",
    "\n",
    "clip = load_random_subsection('d:\\\\data\\\\bc\\\\spodov\\\\XC441823.ogg', duration_secs=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_example_implementation(path, label):\n",
    "    \n",
    "    print(f'processing path {path}')\n",
    "    \n",
    "    tmp_audio_t = load_random_subsection(path, duration_secs=5)\n",
    "    \n",
    "    # print(f'tmp_audio_t shape {tmp_audio_t}')\n",
    "        \n",
    "    # Convert to spectrogram\n",
    "    image = tfio.audio.spectrogram(\n",
    "        tmp_audio_t,\n",
    "        nfft=AUDIO_NFFT, \n",
    "        window=AUDIO_WINDOW, \n",
    "        stride=AUDIO_STRIDE)\n",
    "    \n",
    "    # Convert to melspectrogram\n",
    "    image = tfio.audio.melscale(\n",
    "        image, \n",
    "        rate=AUDIO_SAMPLE_RATE, \n",
    "        mels=AUDIO_MELS, \n",
    "        fmin=AUDIO_FMIN, \n",
    "        fmax=AUDIO_FMAX)\n",
    "    \n",
    "    # print(f'image shape {image.shape}')\n",
    "\n",
    "    # reshape into standard 3 channels to add the color channel\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    \n",
    "    # most pre-trained model expect 3 color channels\n",
    "    image = tf.repeat(image, MODEL_INPUT_IMAGE_CHANNELS, axis=2)\n",
    "    \n",
    "    # print(f'image shape {image.shape}')\n",
    "    \n",
    "    image = tf.ensure_shape(image, [216, 128, MODEL_INPUT_IMAGE_CHANNELS])\n",
    "    image = tf.image.resize(image, (MODEL_INPUT_IMAGE_WIDTH,MODEL_INPUT_IMAGE_HEIGHT), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "\n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.00001)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def dataset_example_pipeline(path, label):\n",
    "    return dataset_example_implementation(path, label)\n",
    "    # return execute_cached_function(dataset_example_implementation,path,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will allow python execution\n",
    "def dataset_example_pipeline_wrapper(path, label):\n",
    "    # Use a lambda function to pass two arguments to the dataset_example_pipeline function\n",
    "    return tf.py_function(func=lambda x, y: dataset_example_pipeline(x, y), inp=(path, label), Tout=(tf.float32, label.dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# create the datasets useful for training a classification model\n",
    "########################################################################################\n",
    "train_dataset = (train_ds\n",
    "                 .map(dataset_example_pipeline_wrapper)\n",
    "                 .batch(CLASSIFIER_BATCH_SIZE)          \n",
    ")\n",
    "\n",
    "validation_dataset = (val_ds\n",
    "                      .map(dataset_example_pipeline_wrapper)\n",
    "                      .batch(CLASSIFIER_BATCH_SIZE)\n",
    ")\n",
    "\n",
    "test_dataset = (test_ds\n",
    "                .map(dataset_example_pipeline_wrapper)\n",
    "                .batch(CLASSIFIER_BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing path b'd:\\\\data\\\\bc\\\\sheowl\\\\XC431869.ogg'\n",
      "duration 140.08798185941043 5\n",
      "subsection (110250,)\n",
      "processing path b'd:\\\\data\\\\bc\\\\jabwar\\\\XC282183.ogg'\n",
      "duration 24.058775510204082 5\n",
      "subsection (110250,)\n",
      " sample info: (2, 256, 256, 3), [[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "processing path b'd:\\\\data\\\\bc\\\\wiltur\\\\XC138164.ogg'\n",
      "duration 66.37714285714286 5\n",
      "subsection (110250,)\n"
     ]
    }
   ],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for melspectrogram,label in train_dataset.take(1):\n",
    "    print(f' sample info: {melspectrogram.shape}, {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the cache works\n",
    "class ArrayProcessor:\n",
    "    def sum_plus_five(self, arr, v2):\n",
    "        array_sum = np.sum(arr)\n",
    "        return array_sum + 5.0 + v2\n",
    "\n",
    "# Usage example\n",
    "processor = ArrayProcessor()\n",
    "\n",
    "# Create a 2D NumPy array\n",
    "arr = np.random.rand(1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 processor.sum_plus_five(arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result calculated and stored in cache key: 2c914e161706b493235d0ad436a561825aabc9c4d70965b2213d894303253745\n",
      "5.99 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 execute_cached_function(processor.sum_plus_five, arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result loaded from cache key: 2c914e161706b493235d0ad436a561825aabc9c4d70965b2213d894303253745\n",
      "843 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 execute_cached_function(processor.sum_plus_five,arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trainable):\n",
    "    # build a really simple classification model using a pre-training Efficientnet V2\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # input layer\n",
    "            tf.keras.layers.InputLayer(input_shape=(MODEL_INPUT_IMAGE_HEIGHT, MODEL_INPUT_IMAGE_WIDTH,3)),\n",
    "  \n",
    "            # use the model as a feature generator only\n",
    "            # use pre-trained mobilenet v2 as the feature layer (less parameters, more accessible)\n",
    "            # hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", trainable),  \n",
    "            \n",
    "            hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\", trainable), \n",
    "            \n",
    "            # much larger model to see if more parameters matters...\n",
    "            #hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_152/feature_vector/5\", trainable),  \n",
    "                        \n",
    "            # add the classification layer here       \n",
    "            tf.keras.layers.Flatten(), \n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            \n",
    "            tf.keras.layers.Dense(len(class_names)*3, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(1e-2)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            \n",
    "            tf.keras.layers.Dense(len(class_names)*2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(1e-2)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            \n",
    "            tf.keras.layers.Dropout(0.50),\n",
    "            tf.keras.layers.Dense(len(class_names)*1, activation=None),\n",
    "        ]\n",
    "    )\n",
    "    # need to tell the model what the input shape is\n",
    "    model.build([None, \n",
    "                 MODEL_INPUT_IMAGE_HEIGHT,\n",
    "                 MODEL_INPUT_IMAGE_WIDTH, \n",
    "                 MODEL_INPUT_IMAGE_CHANNELS])\n",
    "\n",
    "    # show the model\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              6931124   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1280)             5120      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                19215     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 15)               60        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                160       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 10)               40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,955,774\n",
      "Trainable params: 6,882,092\n",
      "Non-trainable params: 73,682\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 652, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 46\u001b[0m\n\u001b[0;32m     40\u001b[0m mcp_save \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\u001b[39m'\u001b[39m\u001b[39mmodels/checkpoint_cnn_model.hdf5\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     41\u001b[0m                                            save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[0;32m     42\u001b[0m                                            monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     43\u001b[0m                                            mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[39m# fit the model to the training set\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataset, \n\u001b[0;32m     47\u001b[0m           validation_data\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[0;32m     48\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[lr_reduce_plateau, early_stopping, tensorboard_callback, mcp_save],\n\u001b[0;32m     49\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)      \n",
      "File \u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filen46fe97p.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 652, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('models/'):\n",
    "    os.mkdir('models/')\n",
    "    \n",
    "# allow all the weights to be trained\n",
    "model = build_model(True)\n",
    "\n",
    "# the form_logits means the loss function has the 'softmax' buillt in.  This approach is numerically more stable\n",
    "# than including the softmax activation on the last layer of the classifier\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3), \n",
    "              metrics=[\"accuracy\"],\n",
    "              )\n",
    "\n",
    "# tensorboard for visualisation of results\n",
    "log_dir = \"tensorboard_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=1)\n",
    "\n",
    "# reduce learning rate to avoid overshooting local minima\n",
    "lr_reduce_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                      factor=0.5,\n",
    "                                                      patience=4, \n",
    "                                                      verbose=1,\n",
    "                                                      mode='min',\n",
    "                                                      cooldown=0, \n",
    "                                                      min_lr=1e-8)\n",
    "\n",
    "# end the training if no improvement for 16 epochs in a row, then restore best model weights\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=8,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# save the best model as it trains..\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('models/checkpoint_cnn_model.hdf5', \n",
    "                                           save_best_only=True, \n",
    "                                           monitor='val_loss', \n",
    "                                           mode='min')\n",
    "\n",
    "# fit the model to the training set\n",
    "model.fit(train_dataset, \n",
    "          # validation_data=validation_dataset,\n",
    "          callbacks=[lr_reduce_plateau, early_stopping, tensorboard_callback, mcp_save],\n",
    "          epochs=1000)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model weights\n",
    "model.save_weights('models/baseline_cnn_model.hdf5', save_format='h5')\n",
    "\n",
    "# test load the model for inference\n",
    "test_model = build_model(False)\n",
    "test_model.build([None, \n",
    "                MODEL_INPUT_IMAGE_HEIGHT,\n",
    "                MODEL_INPUT_IMAGE_WIDTH, \n",
    "                MODEL_INPUT_IMAGE_CHANNELS])\n",
    "test_model.load_weights('models/baseline_cnn_model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
