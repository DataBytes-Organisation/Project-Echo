{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version        :  3.8.16\n",
      "TensorFlow Version    :  2.10.1\n",
      "TensorFlow IO Version :  0.27.0\n",
      "Librosa Version       :  0.10.0\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# library imports\n",
    "########################################################################################\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# generic libraries\n",
    "from platform import python_version\n",
    "import functools\n",
    "import diskcache as dc\n",
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "# tensor flow / keras related libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from keras.utils import dataset_utils\n",
    "\n",
    "# image processing related libraries\n",
    "import librosa \n",
    "import imageio\n",
    "import PIL\n",
    "\n",
    "# print system information\n",
    "print('Python Version        : ', python_version())\n",
    "print('TensorFlow Version    : ', tf.__version__)\n",
    "print('TensorFlow IO Version : ', tfio.__version__)\n",
    "print('Librosa Version       : ', librosa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# system constants\n",
    "########################################################################################\n",
    "AUDIO_DATA_DIRECTORY = \"d:\\\\data\\\\bc\"\n",
    "CACHE_DIRETORY       = \"d:\\\\pipeline_cache\"\n",
    "\n",
    "AUDIO_NFFT = 512\n",
    "AUDIO_WINDOW = 512\n",
    "AUDIO_STRIDE = 512\n",
    "AUDIO_SAMPLE_RATE = int(44100/2)\n",
    "AUDIO_MELS = 128\n",
    "AUDIO_FMIN = 0\n",
    "AUDIO_FMAX = int(AUDIO_SAMPLE_RATE)/2\n",
    "AUDIO_TOP_DB = 80\n",
    "        \n",
    "MODEL_INPUT_IMAGE_WIDTH = 256\n",
    "MODEL_INPUT_IMAGE_HEIGHT = 256\n",
    "MODEL_INPUT_IMAGE_CHANNELS = 3\n",
    "\n",
    "CLASSIFIER_BATCH_SIZE=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create a DiskCache instance\n",
    "# This cache will allow us store intermediate function results to speed up the \n",
    "# data processing pipeline\n",
    "########################################################################################\n",
    "cache = dc.Cache(CACHE_DIRETORY, cull_limit=0, size_limit=10**9) \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# a helper function to create a hash key from a function signature and arguments\n",
    "########################################################################################\n",
    "def create_function_key(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    func_name = partial_func.func.__name__\n",
    "    func_module = partial_func.func.__module__\n",
    "    args_repr = repr(partial_func.args)\n",
    "    kwargs_repr = repr(sorted(partial_func.keywords.items()))\n",
    "\n",
    "    key = f\"{func_module}.{func_name}:{args_repr}:{kwargs_repr}\"\n",
    "    # Use hashlib to create a hash of the key for shorter and consistent length\n",
    "    key_hash = hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "    return key, key_hash, partial_func\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Execute a function and cache the result\n",
    "# If already executed, retrieve function output from the cache instead\n",
    "########################################################################################\n",
    "def execute_cached_function(func, *args, **kwargs):\n",
    "    key_string,key,partial_func = create_function_key(func, *args, **kwargs)\n",
    "    #print(f'key: {key_string} {key}')\n",
    "    # Check if the result is in the cache\n",
    "    if key in cache:\n",
    "        result = cache[key]\n",
    "        print(f\"Result loaded from cache: {result}\")\n",
    "    else:\n",
    "        # If not in cache, call the slow operation and store the result in cache\n",
    "        result = partial_func()\n",
    "        cache[key] = result\n",
    "        print(f\"Result calculated and stored in cache: {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# these helper functions load the audio data into a 'dataset' using only paths\n",
    "# just dealing with paths at this early stage means the entire dataset can be shuffled in\n",
    "# memory and split before loading the actual audio data into memory\n",
    "########################################################################################\n",
    "def paths_and_labels_to_dataset(image_paths, labels, num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    zipped_path_ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    return zipped_path_ds\n",
    "\n",
    "def create_datasets(audio_files, train_split=0.7, val_split=0.2):\n",
    "    file_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            audio_files,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.ogg','.mp3','.wav','.flac'),\n",
    "            class_names=None,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=file_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    # Calculate the size of the dataset\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # Calculate the number of elements for each dataset split\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_size, seed=42)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size).take(val_size)\n",
    "    test_ds = dataset.skip(train_size + val_size).take(test_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 524 files belonging to 5 classes.\n",
      "Class names:  ['brant', 'jabwar', 'sheowl', 'spodov', 'wiltur']\n",
      "Training   dataset length: 419\n",
      "Validation dataset length: 99\n",
      "Test       dataset length: 6\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "train_ds, val_ds, test_ds, class_names = create_datasets(AUDIO_DATA_DIRECTORY,train_split=0.8, val_split=0.19)\n",
    "print(\"Class names: \", class_names)\n",
    "print(f\"Training   dataset length: {len(train_ds)}\")\n",
    "print(f\"Validation dataset length: {len(val_ds)}\")\n",
    "print(f\"Test       dataset length: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC666501.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\wiltur\\\\XC317966.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 1.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\wiltur\\\\XC618595.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 1.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\brant\\\\XC540354.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 0., 0., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC295378.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC443310.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\sheowl\\\\XC607790.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 1., 0., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC443311.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC491970.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'd:\\\\data\\\\bc\\\\spodov\\\\XC124409.ogg'>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for item in train_ds.take(10):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_example_pipeline(path, label):\n",
    "    \n",
    "    tmp_audio_t = None\n",
    "    file_contents=tf.io.read_file(path)\n",
    "    \n",
    "    print(path)\n",
    "    # print(file_contents)\n",
    "    \n",
    "    # try:\n",
    "    #     if tmp_audio_t == None:\n",
    "    #         tmp_audio_t = tfio.audio.decode_flac(input=file_contents, dtype=tf.int16)\n",
    "    # except:\n",
    "    #     pass\n",
    "    \n",
    "    # try:\n",
    "    #     if tmp_audio_t == None:\n",
    "    #         tmp_audio_t = tfio.audio.decode_flac(input=file_contents, dtype=tf.int32)\n",
    "    # except:\n",
    "    #     pass\n",
    "    \n",
    "    try:\n",
    "        if tmp_audio_t == None:\n",
    "            tmp_audio_t = tfio.audio.decode_vorbis(input=file_contents)\n",
    "    except:\n",
    "        pass    \n",
    "           \n",
    "    tmp_audio_t = tf.cast(tmp_audio_t, tf.float32)\n",
    "    \n",
    "    print(f'tmp_audio_t shape {tmp_audio_t}')\n",
    "        \n",
    "    # tmp_audio_t = tfio.audio.resample(tmp_audio_t, tfio.audio.AudioIOTensor(path)._rate.numpy(), AUDIO_SAMPLE_RATE)\n",
    "\n",
    "    # Convert to spectrogram\n",
    "    image = tfio.audio.spectrogram(\n",
    "        tmp_audio_t[:, 0], # left channel only\n",
    "        nfft=AUDIO_NFFT, \n",
    "        window=AUDIO_WINDOW, \n",
    "        stride=AUDIO_STRIDE)\n",
    "    \n",
    "    # Convert to melspectrogram\n",
    "    image = tfio.audio.melscale(\n",
    "        image, \n",
    "        rate=AUDIO_SAMPLE_RATE, \n",
    "        mels=AUDIO_MELS, \n",
    "        fmin=AUDIO_FMIN, \n",
    "        fmax=AUDIO_FMAX)\n",
    "    \n",
    "    print(f'image shape {image.shape}')\n",
    "\n",
    "    # reshape into standard 3 channels to add the color channel\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    \n",
    "    # most pre-trained model expect 3 color channels\n",
    "    image = tf.repeat(image, MODEL_INPUT_IMAGE_CHANNELS, axis=2)\n",
    "    \n",
    "    print(f'image shape {image.shape}')\n",
    "    \n",
    "    image = tf.ensure_shape(image, [216, 128, MODEL_INPUT_IMAGE_CHANNELS])\n",
    "    image = tf.image.resize(image, (MODEL_INPUT_IMAGE_WIDTH,MODEL_INPUT_IMAGE_HEIGHT), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "    \n",
    "    # for some reason the melspecs seem rotated by 90 degrees. This corrects that.\n",
    "    image = tf.image.rot90(image, k=1)\n",
    "    \n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.00001)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will allow python execution\n",
    "def dataset_example_pipeline_wrapper(path, label):\n",
    "    # Use a lambda function to pass two arguments to the dataset_example_pipeline function\n",
    "    return tf.py_function(func=lambda x, y: dataset_example_pipeline(x, y), inp=(path, label), Tout=(tf.float32, label.dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# create the datasets useful for training a classification model\n",
    "########################################################################################\n",
    "train_dataset = (train_ds\n",
    "                 .map(dataset_example_pipeline_wrapper)\n",
    "                 .batch(CLASSIFIER_BATCH_SIZE)          \n",
    ")\n",
    "\n",
    "# validation_dataset = (val_ds\n",
    "#                       .map(dataset_example_pipeline_wrapper)\n",
    "#                       .batch(CLASSIFIER_BATCH_SIZE)\n",
    "# )\n",
    "\n",
    "# test_dataset = (test_ds\n",
    "#                 .map(dataset_example_pipeline_wrapper)\n",
    "#                 .batch(CLASSIFIER_BATCH_SIZE)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'd:\\\\data\\\\bc\\\\sheowl\\\\XC431869.ogg', shape=(), dtype=string)\n",
      "tmp_audio_t shape [[-5.3238455e-06 -8.2403221e-06]\n",
      " [ 2.3792973e-05  1.2055902e-05]\n",
      " [-1.3287732e-05 -8.7958533e-06]\n",
      " ...\n",
      " [-1.9378622e-06 -5.5549265e-08]\n",
      " [ 2.5502404e-05  2.9752951e-05]\n",
      " [ 3.5742160e-06  7.2412176e-06]]\n",
      "image shape (8756, 128)\n",
      "tf.Tensor(b'd:\\\\data\\\\bc\\\\spodov\\\\XC181467.ogg', shape=(), dtype=string)\n",
      "tmp_audio_t shape [[-5.3643462e-06  1.3893226e-06]\n",
      " [-4.5676023e-07  1.0300678e-05]\n",
      " [ 3.0567719e-06  2.5023275e-05]\n",
      " ...\n",
      " [ 1.1289783e-04  3.7449958e-05]\n",
      " [-4.3799012e-04 -3.3382158e-04]\n",
      " [-1.5377003e-04 -5.7742064e-04]]\n",
      "image shape (482, 128)\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} InvalidArgumentError: {{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shape of tensor input [8756,128,3] is not compatible with expected shape [216,128,3]. [Op:EnsureShape]\nTraceback (most recent call last):\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\Andrew\\AppData\\Local\\Temp\\__autograph_generated_filexkvl894n.py\", line 12, in <lambda>\n    retval_ = ag__.converted_call(ag__.ld(tf).py_function, (), dict(func=ag__.autograph_artifact((lambda x, y: ag__.converted_call(ag__.ld(dataset_example_pipeline), (ag__.ld(x), ag__.ld(y)), None, fscope))), inp=(ag__.ld(path), ag__.ld(label)), Tout=(ag__.ld(tf).float32, ag__.ld(label).dtype)), fscope)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 335, in converted_call\n    return _call_unconverted(f, args, kwargs, options, False)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 459, in _call_unconverted\n    return f(*args)\n\n  File \"C:\\Users\\Andrew\\AppData\\Local\\Temp\\ipykernel_37152\\550811495.py\", line 65, in dataset_example_pipeline\n    image = tf.ensure_shape(image, [216, 128, MODEL_INPUT_IMAGE_CHANNELS])\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 7209, in raise_from_not_ok_status\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shape of tensor input [8756,128,3] is not compatible with expected shape [216,128,3]. [Op:EnsureShape]\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# show what the pipeline looks like at this stage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m train_dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(item)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    765\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 766\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[0;32m    767\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    768\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[1;32m--> 749\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[0;32m    750\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[0;32m    751\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    752\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[0;32m    754\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3014\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3015\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 3016\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   3017\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   3018\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} InvalidArgumentError: {{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shape of tensor input [8756,128,3] is not compatible with expected shape [216,128,3]. [Op:EnsureShape]\nTraceback (most recent call last):\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\Andrew\\AppData\\Local\\Temp\\__autograph_generated_filexkvl894n.py\", line 12, in <lambda>\n    retval_ = ag__.converted_call(ag__.ld(tf).py_function, (), dict(func=ag__.autograph_artifact((lambda x, y: ag__.converted_call(ag__.ld(dataset_example_pipeline), (ag__.ld(x), ag__.ld(y)), None, fscope))), inp=(ag__.ld(path), ag__.ld(label)), Tout=(ag__.ld(tf).float32, ag__.ld(label).dtype)), fscope)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 335, in converted_call\n    return _call_unconverted(f, args, kwargs, options, False)\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 459, in _call_unconverted\n    return f(*args)\n\n  File \"C:\\Users\\Andrew\\AppData\\Local\\Temp\\ipykernel_37152\\550811495.py\", line 65, in dataset_example_pipeline\n    image = tf.ensure_shape(image, [216, 128, MODEL_INPUT_IMAGE_CHANNELS])\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"d:\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 7209, in raise_from_not_ok_status\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shape of tensor input [8756,128,3] is not compatible with expected shape [216,128,3]. [Op:EnsureShape]\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for item in train_dataset.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the cache works\n",
    "class ArrayProcessor:\n",
    "    def sum_plus_five(self, arr, v2):\n",
    "        array_sum = np.sum(arr)\n",
    "        return array_sum + 5.0 + v2\n",
    "\n",
    "# Usage example\n",
    "processor = ArrayProcessor()\n",
    "\n",
    "# Create a 2D NumPy array\n",
    "arr = np.random.rand(1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r1 -n1 processor.sum_plus_five(arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r1 -n1 execute_cached_function(processor.sum_plus_five, arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r1 -n1 execute_cached_function(processor.sum_plus_five,arr, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transforms(image, label):\n",
    "    # reshape into standard 3 channels\n",
    "    image = tf.io.parse_tensor(image, tf.float32)\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    \n",
    "    # most pre-trained model expect 3 color channels\n",
    "    image = tf.repeat(image, MODEL_INPUT_IMAGE_CHANNELS, axis=2)\n",
    "    \n",
    "    image = tf.ensure_shape(image, [216, 128, MODEL_INPUT_IMAGE_CHANNELS])\n",
    "    image = tf.image.resize(image, (MODEL_INPUT_IMAGE_WIDTH,MODEL_INPUT_IMAGE_HEIGHT), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "    \n",
    "    # for some reason the melspecs seem rotated by 90 degrees. This corrects that.\n",
    "    image = tf.image.rot90(image, k=1)\n",
    "    \n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.00001)\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_b = ( \n",
    "                  train_dataset       \n",
    "                  .shuffle(20000)\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()           \n",
    "                )\n",
    "\n",
    "validation_dataset_b = ( \n",
    "                  validation_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                )\n",
    "\n",
    "test_dataset_b = ( \n",
    "                  test_dataset\n",
    "                  .map(dataset_transforms)\n",
    "                  .batch(baseline_config.batch_size)\n",
    "                  .cache()\n",
    "                )\n",
    "\n",
    "train_dataset, class_names = create_dataset('TRAIN/')\n",
    "test_dataset, _            = create_dataset('TEST/')\n",
    "validation_dataset, _      = create_dataset('VALIDATION/')\n",
    "print(\"class names: \", class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
