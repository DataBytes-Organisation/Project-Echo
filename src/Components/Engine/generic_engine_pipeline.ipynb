{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Engine Pipeline\n",
    "\n",
    "The notebook provides a generic end-to-end model training pipeline for experimentation.\n",
    "\n",
    "The notebook works by configuring the input data set and uses a disk caching method to cache function calls to speed up model training.\n",
    "\n",
    "Unlike prior work on Project Echo, no offline pre-processing of the audio files is required (this is encapsulted in the processing pipeline within this notebook).\n",
    "\n",
    "Author: akudilczak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# library imports\n",
    "########################################################################################\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# generic libraries\n",
    "from platform import python_version\n",
    "import functools\n",
    "from functools import lru_cache\n",
    "import diskcache as dc\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensor flow / keras related libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa \n",
    "from keras.utils import dataset_utils\n",
    "\n",
    "# image processing related libraries\n",
    "import librosa\n",
    "\n",
    "# audio processing libraries\n",
    "import audiomentations\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "# print system information\n",
    "print('Python Version           : ', python_version())\n",
    "print('TensorFlow Version       : ', tf.__version__)\n",
    "print('TensorFlow IO Version    : ', tfio.__version__)\n",
    "print('Librosa Version          : ', librosa.__version__)\n",
    "print('Audiomentations Version  : ', audiomentations.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Configuration\n",
    "\n",
    "The following code sets up the pipeline with configuration options.\n",
    "\n",
    "The key is to set the audio data directory to the root directory containing the folders with raw audio files. \n",
    "\n",
    "This expects the folders names to be the species names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# system constants\n",
    "########################################################################################\n",
    "\n",
    "SC = {\n",
    "    'AUDIO_DATA_DIRECTORY': \"d:\\\\data\\\\bc\",\n",
    "    'CACHE_DIRETORY': \"d:\\\\pipeline_cache\",\n",
    "\n",
    "    'AUDIO_CLIP_DURATION': 5, # 5 second clips\n",
    "    'AUDIO_NFFT': 2000,\n",
    "    'AUDIO_WINDOW': 500,\n",
    "    'AUDIO_STRIDE': 200,\n",
    "    'AUDIO_SAMPLE_RATE': 32000,  # int(44100/2)\n",
    "    'AUDIO_MELS': 260,\n",
    "    'AUDIO_FMIN': 10,\n",
    "    'AUDIO_FMAX': int(32000 / 2),\n",
    "    'AUDIO_TOP_DB': 80,\n",
    "\n",
    "    'MODEL_INPUT_IMAGE_WIDTH': 260,\n",
    "    'MODEL_INPUT_IMAGE_HEIGHT': 260,\n",
    "    'MODEL_INPUT_IMAGE_CHANNELS': 3,\n",
    "\n",
    "    'USE_DISK_CACHE': True,\n",
    "    'SAMPLE_VARIANTS': 40,\n",
    "    'CLASSIFIER_BATCH_SIZE': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_memory_limit(mem_mb):\n",
    "  # enforce memory limit on GPU\n",
    "\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if gpus:\n",
    "    try:\n",
    "      tf.config.experimental.set_virtual_device_configuration(\n",
    "          gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=mem_mb)])\n",
    "      print(f\"vram limit set to {mem_mb}MB\")\n",
    "    except RuntimeError as e:\n",
    "      print(e)\n",
    "      \n",
    "# enforce max 5GB memory on GPU for this notebook if you have a small GPU\n",
    "# enforce_memory_limit(5120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk Caching\n",
    "\n",
    "The following code creates a disk cache.  You will need lots of space (20 GB+) if you create large melspectrograms.\n",
    "\n",
    "The caching works by serialising a function call signature and hashing it into a key.  This key is used to store the result of the function call.\n",
    "\n",
    "This allows the a result from the cache to be utilised instead of calling the function, which means the entire data processing pipeline can be cached if used correctly.\n",
    "\n",
    "This works best when the function being cached is idempotent.  There may be circumstances where it doesn't matter.  Be careful with using this cache as you may get unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create a DiskCache instance\n",
    "# This cache will allow us store intermediate function results to speed up the \n",
    "# data processing pipeline\n",
    "########################################################################################\n",
    "cache = dc.Cache(SC['CACHE_DIRETORY'], cull_limit=0, size_limit=10**9) \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# a helper function to create a hash key from a function signature and arguments\n",
    "########################################################################################\n",
    "def create_function_key(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    func_name = partial_func.func.__name__\n",
    "    func_module = partial_func.func.__module__\n",
    "    args_repr = repr(partial_func.args)\n",
    "    kwargs_repr = repr(sorted(partial_func.keywords.items()))\n",
    "\n",
    "    key = f\"{func_module}.{func_name}:{args_repr}:{kwargs_repr}\"\n",
    "    # Use hashlib to create a hash of the key for shorter and consistent length\n",
    "    key_hash = hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "    return key, key_hash, partial_func\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Execute a function and cache the result\n",
    "# If already executed, retrieve function output from the cache instead\n",
    "########################################################################################\n",
    "def execute_cached_function(func, *args, **kwargs):\n",
    "    key_string,key,partial_func = create_function_key(func, *args, **kwargs)\n",
    "    # Check if the result is in the cache\n",
    "    if key in cache:\n",
    "        result = cache[key]\n",
    "        # print(f\"Result loaded from cache key: {key}\")\n",
    "    else:\n",
    "        # If not in cache, call the slow operation and store the result in cache\n",
    "        result = partial_func()\n",
    "        cache[key] = result\n",
    "        #print(f\"New result calculated and stored in cache key: {key}\")\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the files into a Tensorflow dataset structure for model training\n",
    "\n",
    "This initial code loads only the filenames.  The filenames are then split into train, validation and test datasets.  This is designed deliberately this way to conserve runtime memory.\n",
    "\n",
    "Subsequent downstream loading of the file content occurs as part of the data pipeline transformation 'map' function.  See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# these helper functions load the audio data into a 'dataset' using only paths\n",
    "# just dealing with paths at this early stage means the entire dataset can be shuffled in\n",
    "# memory and split before loading the actual audio data into memory\n",
    "########################################################################################\n",
    "def paths_and_labels_to_dataset(image_paths, labels, num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    zipped_path_ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    return zipped_path_ds\n",
    "\n",
    "def create_datasets(audio_files, train_split=0.7, val_split=0.2):\n",
    "    file_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            audio_files,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.ogg','.mp3','.wav','.flac'),\n",
    "            class_names=None,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=file_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    # Calculate the size of the dataset\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # Calculate the number of elements for each dataset split\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_size, seed=42)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size).take(val_size)\n",
    "    test_ds = dataset.skip(train_size + val_size).take(test_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "train_ds, val_ds, test_ds, class_names = create_datasets(SC['AUDIO_DATA_DIRECTORY'],train_split=0.8, val_split=0.19)\n",
    "print(\"Class names: \", class_names)\n",
    "print(f\"Training   dataset length: {len(train_ds)}\")\n",
    "print(f\"Validation dataset length: {len(val_ds)}\")\n",
    "print(f\"Test       dataset length: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for item in train_ds.take(10):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_subsection(path, duration_secs):\n",
    "\n",
    "    # read the file data\n",
    "    file_contents=tf.io.read_file(path)\n",
    "\n",
    "    # attempt to decode each known format\n",
    "    try:\n",
    "        tmp_audio_t = tfio.audio.decode_flac(input=file_contents)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        tmp_audio_t = tfio.audio.decode_vorbis(input=file_contents)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        tmp_audio_t = tfio.audio.decode_mp3(input=file_contents)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        tmp_audio_t = tfio.audio.decode_wav(input=file_contents)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # cast and keep left channel only\n",
    "    tmp_audio_t = tf.cast(tmp_audio_t, tf.float32)[:,-1]\n",
    "    \n",
    "    # resample the sample rate\n",
    "    tmp_audio_t = tfio.audio.resample(tmp_audio_t, tfio.audio.AudioIOTensor(path)._rate.numpy(), SC['AUDIO_SAMPLE_RATE'])\n",
    "\n",
    "    # Determine the audio file's duration in seconds\n",
    "    audio_duration_secs = tf.shape(tmp_audio_t)[0] / SC['AUDIO_SAMPLE_RATE']\n",
    "    \n",
    "    if audio_duration_secs>duration_secs:\n",
    "    \n",
    "        # Calculate the starting point of the 5-second subsection\n",
    "        max_start = tf.cast(audio_duration_secs - duration_secs, tf.float32)\n",
    "        start_time_secs = tf.random.uniform((), 0.0, max_start, dtype=tf.float32)\n",
    "        \n",
    "        start_index = tf.cast(start_time_secs * SC['AUDIO_SAMPLE_RATE'], dtype=tf.int32)\n",
    "  \n",
    "        # Load the 5-second subsection\n",
    "        end_index = tf.cast(start_index + tf.cast(duration_secs, tf.int32) * SC['AUDIO_SAMPLE_RATE'], tf.int32)\n",
    "        \n",
    "        subsection = tmp_audio_t[start_index : end_index]\n",
    "    \n",
    "    else:\n",
    "        # Pad the subsection with silence if it's shorter than 5 seconds\n",
    "        padding_length = duration_secs * SC['AUDIO_SAMPLE_RATE'] - tf.shape(tmp_audio_t)[0]\n",
    "        padding = tf.zeros([padding_length], dtype=tmp_audio_t.dtype)\n",
    "        subsection = tf.concat([tmp_audio_t, padding], axis=0)\n",
    "\n",
    "    return subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio augmentation pipeline\n",
    "# see https://github.com/iver56/audiomentations for more options\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.2),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n",
    "    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image augmentation pipeline\n",
    "def image_augmentations(image, label):\n",
    "\n",
    "    degrees = tf.random.uniform(shape=(1,), minval=-2, maxval=2)\n",
    "    radians = degrees * 0.017453292519943295  # convert the angle in degree to radians\n",
    "\n",
    "    image = tfa.image.rotate(image, radians, interpolation='bilinear')\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_melspectro_pipeline(path, label, variant, config, audio_clip):\n",
    "    \n",
    "    # Convert to spectrogram\n",
    "    image = tfio.audio.spectrogram(\n",
    "        audio_clip,\n",
    "        nfft=config['AUDIO_NFFT'], \n",
    "        window=config['AUDIO_WINDOW'], \n",
    "        stride=config['AUDIO_STRIDE'])\n",
    "\n",
    "    # Convert to melspectrogram\n",
    "    image = tfio.audio.melscale(\n",
    "        image, \n",
    "        rate=config['AUDIO_SAMPLE_RATE'], \n",
    "        mels=config['AUDIO_MELS'], \n",
    "        fmin=config['AUDIO_FMIN'], \n",
    "        fmax=config['AUDIO_FMAX'])\n",
    "    \n",
    "    # reshape into standard 3 channels to add the color channel\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    \n",
    "    # most pre-trained model classifer model expects 3 color channels\n",
    "    image = tf.repeat(image, config['MODEL_INPUT_IMAGE_CHANNELS'], axis=2)\n",
    "    \n",
    "    # calculate the image shape and ensure it is correct\n",
    "    expected_clip_samples = int(config['AUDIO_CLIP_DURATION'] * config['AUDIO_SAMPLE_RATE'] / config['AUDIO_STRIDE'])\n",
    "    image = tf.ensure_shape(image, [expected_clip_samples, config['AUDIO_MELS'], config['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "    \n",
    "    # note here a high quality LANCZOS5 is applied to resize the image to match model image input size\n",
    "    image = tf.image.resize(image, (config['MODEL_INPUT_IMAGE_WIDTH'],config['MODEL_INPUT_IMAGE_HEIGHT']), \n",
    "                            method=tf.image.ResizeMethod.LANCZOS5)\n",
    "\n",
    "    # rescale to range [0,1]\n",
    "    image = image - tf.reduce_min(image) \n",
    "    image = image / (tf.reduce_max(image)+0.0000001)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_validation_pipeline(path, label, variant, config):\n",
    "    # this loads a random 5 second clip of audio from the raw audio file\n",
    "    audio_clip = load_random_subsection(path, duration_secs=config['AUDIO_CLIP_DURATION'])\n",
    "\n",
    "    # Convert to spectrogram\n",
    "    image, label = dataset_melspectro_pipeline(path, label, variant, config, audio_clip)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def dataset_test_pipeline(path, label, variant, config):\n",
    "    # this loads a random 5 second clip of audio from the raw audio file\n",
    "    audio_clip = load_random_subsection(path, duration_secs=config['AUDIO_CLIP_DURATION'])\n",
    "\n",
    "    # Convert to spectrogram\n",
    "    image, label = dataset_melspectro_pipeline(path, label, variant, config, audio_clip)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def dataset_training_pipeline(path, label, variant, config):\n",
    "    # this loads a random 5 second clip of audio from the raw audio file\n",
    "    # note: it also converts the sample rate to AUDIO_SAMPLE_RATE and selects the left channel only\n",
    "    audio_clip = load_random_subsection(path, duration_secs=config['AUDIO_CLIP_DURATION'])\n",
    "   \n",
    "    # apply audio augmentation to the clip\n",
    "    # note: this augmentation is NOT applied in the test and validation pipelines\n",
    "    audio_clip = audio_augmentations(samples=audio_clip.numpy(), sample_rate=config['AUDIO_SAMPLE_RATE'])\n",
    "   \n",
    "    # Convert to spectrogram\n",
    "    image, label = dataset_melspectro_pipeline(path, label, variant, config, audio_clip)\n",
    "    \n",
    "    # apply image based augmentation to the melspectrogram\n",
    "    image, label = image_augmentations(image, label)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_cached_pipeline(pipeline_fn, path, label):\n",
    "    variant = random.randrange(0, SC['SAMPLE_VARIANTS'])\n",
    "    if SC['USE_DISK_CACHE']:\n",
    "        return execute_cached_function(pipeline_fn,path,label,variant,SC)\n",
    "    else:\n",
    "        return pipeline_fn(path,label,variant,SC)\n",
    "    \n",
    "# this will allow python execution within the tensorflow pipeline\n",
    "def dataset_pipeline_wrapper(pipeline_fn, path, label):\n",
    "  \n",
    "    # Use a lambda function to pass two arguments to the dataset_example_pipeline function\n",
    "    image, label = tf.py_function(func=lambda x, y: dataset_cached_pipeline(pipeline_fn, x, y), inp=(path, label), Tout=(tf.float32, label.dtype))\n",
    "\n",
    "    # Set the shape of the output tensors manually\n",
    "    image.set_shape([SC['MODEL_INPUT_IMAGE_WIDTH'], SC['MODEL_INPUT_IMAGE_HEIGHT'], SC['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "    label.set_shape([len(class_names),])  # Set the shape of the label tensor\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# create the datasets useful for training a classification model\n",
    "# note: it seems that using py_function prevents the use of parallel calls which slows down\n",
    "# things siginificantly.  This requires investigation of alternative ways to construct the pipeline\n",
    "########################################################################################\n",
    "\n",
    "len_train_ds = len(train_ds)\n",
    "\n",
    "train_dataset = (train_ds\n",
    "                 .shuffle(len_train_ds)\n",
    "                 .map(functools.partial(dataset_pipeline_wrapper, dataset_training_pipeline), num_parallel_calls=1)\n",
    "                 .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                 .repeat(count=10)         \n",
    ")\n",
    "\n",
    "validation_dataset = (val_ds\n",
    "                      .map(functools.partial(dataset_pipeline_wrapper, dataset_validation_pipeline), num_parallel_calls=1)\n",
    "                      .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                      .repeat(count=4)\n",
    ")\n",
    "\n",
    "test_dataset = (test_ds\n",
    "                .map(functools.partial(dataset_pipeline_wrapper, dataset_test_pipeline), num_parallel_calls=1)\n",
    "                .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                .repeat(count=5)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show what the pipeline looks like at this stage\n",
    "for melspectrogram,label in train_dataset.take(1):\n",
    "    print(f' sample info: {melspectrogram.shape}, {label}')\n",
    "    for example in range(melspectrogram.shape[0]):\n",
    "        plt.imshow(melspectrogram[example,:,:,0].numpy().T, cmap='afmhot', origin='lower')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trainable):\n",
    "    # build a really simple classification model using a pre-training Efficientnet V2\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # input layer\n",
    "            tf.keras.layers.InputLayer(input_shape=(SC['MODEL_INPUT_IMAGE_HEIGHT'], \n",
    "                                                    SC['MODEL_INPUT_IMAGE_WIDTH'], \n",
    "                                                    SC['MODEL_INPUT_IMAGE_CHANNELS'])),\n",
    "  \n",
    "            # use the model as a feature generator only\n",
    "            # needs 384x384x3 images\n",
    "            # hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\", trainable), \n",
    "            \n",
    "            # needs 260x260x3 images\n",
    "            hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\", trainable), \n",
    "                    \n",
    "            # add the classification layer here       \n",
    "            tf.keras.layers.Flatten(), \n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            \n",
    "            tf.keras.layers.Dense(len(class_names)*8, \n",
    "                                  activation=\"relu\", \n",
    "                                  #kernel_regularizer=tf.keras.regularizers.L1(1e-2)\n",
    "                                  ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            \n",
    "            tf.keras.layers.Dense(len(class_names)*4, \n",
    "                                  activation=\"relu\", \n",
    "                                  #kernel_regularizer=tf.keras.regularizers.L1(1e-2)\n",
    "                                  ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            \n",
    "            tf.keras.layers.Dropout(0.50),\n",
    "            tf.keras.layers.Dense(len(class_names), activation=None),\n",
    "        ]\n",
    "    )\n",
    "    # need to tell the model what the input shape is\n",
    "    model.build([None, \n",
    "                 SC['MODEL_INPUT_IMAGE_HEIGHT'],\n",
    "                 SC['MODEL_INPUT_IMAGE_WIDTH'], \n",
    "                 SC['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "\n",
    "    # show the model\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models/'):\n",
    "    os.mkdir('models/')\n",
    "    \n",
    "# allow all the weights to be trained\n",
    "model = build_model(True)\n",
    "\n",
    "# the form_logits means the loss function has the 'softmax' buillt in.  This approach is numerically more stable\n",
    "# than including the softmax activation on the last layer of the classifier\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "              metrics=[\"accuracy\"],\n",
    "              )\n",
    "\n",
    "# tensorboard for visualisation of results\n",
    "log_dir = \"tensorboard_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=1)\n",
    "\n",
    "# reduce learning rate to avoid overshooting local minima\n",
    "lr_reduce_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                      factor=0.75,\n",
    "                                                      patience=12, \n",
    "                                                      verbose=1,\n",
    "                                                      mode='min',\n",
    "                                                      cooldown=0, \n",
    "                                                      min_lr=1e-7)\n",
    "\n",
    "# end the training if no improvement for 16 epochs in a row, then restore best model weights\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=16,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# save the best model as it trains..\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('models/checkpoint_generic_model.hdf5', \n",
    "                                           save_best_only=True, \n",
    "                                           monitor='val_loss', \n",
    "                                           mode='min')\n",
    "\n",
    "# any changes to the source code will generally require the disk cache to be cleared.\n",
    "# So to be safe, the cache is cleared before training the model.  If you are sure\n",
    "# the cache is still valid then comment out this code\n",
    "# the first few epochs of the model training will be slow as the cache is populated with pipeline samples\n",
    "# and will depend on the dataset size and the number of variants included\n",
    "# cache.clear()\n",
    "\n",
    "# fit the model to the training set\n",
    "# this may take 12-24 hours to run to full model convergence depending on your machine\n",
    "model.fit(train_dataset, \n",
    "          validation_data=validation_dataset,\n",
    "          callbacks=[lr_reduce_plateau, early_stopping, tensorboard_callback, mcp_save],\n",
    "          epochs=10000)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs the pipeline and prediction model and returns target and probability\n",
    "def predict_class(predictions):\n",
    "   target_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "   target_class = class_names[target_index]    \n",
    "   target_proba = 100.0*tf.nn.softmax(predictions)[target_index].numpy()\n",
    "   target_proba = str(round(target_proba, 2))\n",
    "   return target_class, target_proba\n",
    "\n",
    "# run prediction on the test entries\n",
    "print(f'class_names:  {class_names}')\n",
    "for feature,label in test_dataset:\n",
    "   \n",
    "    predictions = model.predict(feature, verbose=0)\n",
    "    # print(f'predictions : {predictions}')\n",
    "    \n",
    "    for b in range(predictions.shape[0]):\n",
    "        \n",
    "        true_index = int(tf.argmax(tf.squeeze(label[b])).numpy())\n",
    "        true_class = class_names[true_index] \n",
    "        \n",
    "        pred_class, pred_proba = predict_class(predictions[b])\n",
    "        \n",
    "        print(f'')\n",
    "        print(f'true_class  : {true_class}')    \n",
    "        print(f'pred_class  : {pred_class}')\n",
    "        print(f'pred_proba  : {pred_proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model weights\n",
    "model.save_weights('models/generic_engine_pipeline_model.hdf5', save_format='h5')\n",
    "\n",
    "# test load the model for inference\n",
    "test_model = build_model(False)\n",
    "test_model.build([None, \n",
    "                SC['MODEL_INPUT_IMAGE_HEIGHT'],\n",
    "                SC['MODEL_INPUT_IMAGE_WIDTH'], \n",
    "                SC['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "test_model.load_weights('models/generic_engine_pipeline_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
